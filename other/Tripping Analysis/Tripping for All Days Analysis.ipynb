{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82eb4ab",
   "metadata": {},
   "source": [
    "# IMPORT MODULES & FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47d9527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samha\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pytz #for timezone calculation\n",
    "import math\n",
    "import matplotlib.dates as md\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import seaborn as sns; sns.set()\n",
    "import itertools\n",
    "#import datetime\n",
    "from time import gmtime, strftime\n",
    "from matplotlib import cm\n",
    "from IPython.display import display\n",
    "#%matplotlib qt\n",
    "#%matplotlib inline\n",
    "\n",
    "#SET GLOBAL PARAMETERS\n",
    "# ================== Global parameters for fonts & sizes =================\n",
    "FONT_SIZE = 20\n",
    "rc={'font.size': FONT_SIZE, 'axes.labelsize': FONT_SIZE, 'legend.fontsize': FONT_SIZE, \n",
    "    'axes.titlesize': FONT_SIZE, 'xtick.labelsize': FONT_SIZE, 'ytick.labelsize': FONT_SIZE}\n",
    "plt.rcParams.update(**rc)\n",
    "plt.rc('font', weight='bold')\n",
    " \n",
    "# For label titles\n",
    "fontdict={'fontsize': FONT_SIZE, 'fontweight' : 'bold'}\n",
    "# can add in above dictionary: 'verticalalignment': 'baseline' \n",
    "\n",
    "style = 'ggplot' # choose a style from the above options\n",
    "plt.style.use(style)\n",
    "\n",
    "from solarcurtailment.energy_calculation import *\n",
    "from solarcurtailment.clear_sky_day import *\n",
    "from solarcurtailment.tripping_curt import *\n",
    "from solarcurtailment.vvar_curt import *\n",
    "from solarcurtailment.vwatt_curt import *\n",
    "from solarcurtailment.polyfit import *\n",
    "from solarcurtailment.file_processing import *\n",
    "from solarcurtailment.data_visualization import *\n",
    "\n",
    "#class instantiation\n",
    "file_processing = FileProcessing()\n",
    "clear_sky_day = ClearSkyDay()\n",
    "data_visualization = DataVisualization()\n",
    "energy_calculation = EnergyCalculation()\n",
    "tripping_curt = TrippingCurt()\n",
    "polyfit_f = Polyfit()\n",
    "vvar_curt = VVarCurt()\n",
    "vwatt_curt = VWattCurt()\n",
    "\n",
    "#from FileProcessing \n",
    "def input_monthly_files(file_path, data_date_idx):\n",
    "    \"\"\"Open time-series D-PV data and ghi data of a certain month. Only compatible for SoLA data format.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file location of the data\n",
    "        data_date_idx (str): The month of the files in format 'YYYYMM' eg '201907'\n",
    "\n",
    "    Returns:\n",
    "        data (df): the opened & cleaned time-series D-PV data\n",
    "        ghi (df): the opened & cleaned ghi data\n",
    "        data_ori (df): the opened & unmodified time-series D-PV data\n",
    "        ghi_ori (df): the opened & unmodified ghi data\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = file_path + r\"/processed_unsw_\" + data_date_idx + '_data_raw.csv'\n",
    "    data_ori = pd.read_csv(data_path)\n",
    "    data = data_ori.set_index('utc_tstamp')\n",
    "\n",
    "    # Convert timestamp to local Adelaide time\n",
    "    data.index = pd.to_datetime(data.index) # convert index from object type to datetime\n",
    "    Adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    data.index = data.index.tz_localize(pytz.utc).tz_convert(Adelaide_local_time) # convert utc to local adelaide time\n",
    "    data.index.rename('Timestamp', inplace = True)\n",
    "\n",
    "    # Load GHI data\n",
    "    ghi_date_idx = data_date_idx[0:4] + '_' + data_date_idx[4:]\n",
    "    ghi_path = file_path + r\"/sl_023034_\" + ghi_date_idx +'.txt'\n",
    "    ghi = pd.read_csv (ghi_path) \n",
    "    ghi_ori = ghi.copy()\n",
    "\n",
    "    ghi['timestamp'] = pd.to_datetime(pd.DataFrame ({'year' : ghi['Year Month Day Hours Minutes in YYYY'].values, \n",
    "                                                    'month' : ghi['MM'], \n",
    "                                                    'day' : ghi['DD'], \n",
    "                                                   'hour' : ghi['HH24'], \n",
    "                                                   'minute' : ghi['MI format in Local standard time']}))\n",
    "    ghi.set_index('timestamp', inplace = True)\n",
    "    # Deal with the space characters (ghi is in object/string form at the moment)\n",
    "    ghi['Mean global irradiance (over 1 minute) in W/sq m'] = [float(ghi_t) if ghi_t.count(' ')<= 3 else np.nan for ghi_t in ghi['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    \n",
    "    return data, ghi, data_ori, ghi_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f5fb1",
   "metadata": {},
   "source": [
    "# CHECK ALL DAYS\n",
    "Find list of all dates which data are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be921a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_list(year, month, num_of_days):\n",
    "    \"\"\"Create a list of date as str for certain month and year.\n",
    "\n",
    "    Args:\n",
    "        year (str): year\n",
    "        month (str): month\n",
    "        num_of_days (int): number of days for a certain month and year. \n",
    "\n",
    "    Returns:\n",
    "        date_list (list): list of date as str\n",
    "    \"\"\"\n",
    "    \n",
    "    date_list = []\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        date_list.append(year + '-' + month + '-0'  + str(i))\n",
    "    for i in range(10, num_of_days + 1):\n",
    "        date_list.append(year + '-' + month + '-'  + str(i))\n",
    "    \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538ff25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datelist_201909 = create_date_list('2019', '09', 30)\n",
    "datelist_201910 = create_date_list('2019', '10', 31)\n",
    "datelist_201911 = create_date_list('2019', '11', 30)\n",
    "datelist_201912 = create_date_list('2019', '12', 31)\n",
    "datelist_202001 = create_date_list('2020', '01', 31)\n",
    "datelist_202002 = create_date_list('2020', '02', 29)\n",
    "datelist_202003 = create_date_list('2020', '03', 31)\n",
    "datelist_202004 = create_date_list('2020', '04', 30)\n",
    "\n",
    "\n",
    "date_list = datelist_201909 + datelist_201910 + datelist_201911 + datelist_201912 + datelist_202001 + datelist_202002 + datelist_202003 + datelist_202004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69bb7a",
   "metadata": {},
   "source": [
    "# CLEAR SKY DAYS LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dda359",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_201909 = ['2019-09-12', '2019-09-17', '2019-09-18', '2019-09-26', '2019-09-29', '2019-09-30']\n",
    "csd_201910 = ['2019-10-01', '2019-10-02', '2019-10-03', '2019-10-29']\n",
    "csd_201911 = ['2019-11-18', '2019-11-27']\n",
    "csd_201912 = ['2019-12-16', '2019-12-19', '2019-12-25', '2019-12-28']\n",
    "csd_202001 = ['2020-01-01', '2020-01-02', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-12', '2020-01-13', '2020-01-14', '2020-01-17', '2020-01-29', '2020-01-30']\n",
    "csd_202002 = ['2020-02-04', '2020-02-05', '2020-02-06', '2020-02-10', '2020-02-11', '2020-02-13', '2020-02-22', '2020-02-23']\n",
    "csd_202003 = ['2020-03-03', '2020-03-08', '2020-03-10', '2020-03-14', '2020-03-15', '2020-03-25', '2020-03-26']\n",
    "csd_202004 = ['2020-04-08', '2020-04-09', '2020-04-13', '2020-04-24']\n",
    "\n",
    "csd_list = csd_201909 + csd_201910 + csd_201911 + csd_201912 + csd_202001 + csd_202002 + csd_202003 + csd_202004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2437f",
   "metadata": {},
   "source": [
    "# INPUT ALL MONTHLY DATA & GENERAL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15458787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 201909 ...\n",
      "processing 201910 ...\n",
      "processing 201911 ...\n"
     ]
    }
   ],
   "source": [
    "#file_path = r\"/Users/samhan/Downloads/data\" #for running in Samhan's laptop\n",
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\" #for running in TETB CEEM09 computer\n",
    "\n",
    "month_list = ['201909', '201910', '201911', '201912', '202001', '202002', '202003', '202004']\n",
    "data = {}\n",
    "data_ori = {}\n",
    "ghi = {}\n",
    "ghi_ori = {}\n",
    "for month in month_list:\n",
    "    print('processing {} ...'.format(month))\n",
    "    data[month], ghi[month], data_ori[month], ghi_ori[month] = input_monthly_files(file_path, month)\n",
    "    data[month]['date'] = data[month].index.date\n",
    "    \n",
    "site_details, unique_cids= file_processing.input_general_files(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc841d8",
   "metadata": {},
   "source": [
    "# LOOP OVER ALL SITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b059e2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "\n",
    "for c_id in unique_cids_7['c_id']: #for the whole data testing\n",
    "# for c_id in unique_cids['c_id'][:3]: #for testing\n",
    "# for c_id in [1317822057]: #for testing\n",
    "    last_month_analyzed = 0\n",
    "    count_curtailment = 0\n",
    "    total_energy_generation = 0\n",
    "    total_tripping_curtailment = 0\n",
    "    for date in date_list: \n",
    "        print('Analyzing c_id {} date {} ...'.format(c_id, date))\n",
    "        \n",
    "        #filter D-PV time series data\n",
    "        date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        month_analyzed = date[:4] + date [5:7]\n",
    "        \n",
    "        if not month_analyzed == last_month_analyzed:\n",
    "            site_id_filter = data[month_analyzed]['c_id'] == c_id\n",
    "            data_certain_site = data[month_analyzed].loc[site_id_filter]\n",
    "            last_month_analyzed = month_analyzed\n",
    "            \n",
    "        date_filter = data_certain_site['date'] == date_dt\n",
    "        data_analyzed = data_certain_site.loc[date_filter]\n",
    "\n",
    "        #check tripping curtailment\n",
    "        size_is_ok = file_processing.check_data_size(data_analyzed)\n",
    "        if not size_is_ok:\n",
    "            print('Cannot analyze {} site on {} due to incomplete data.'.format(c_id, date))\n",
    "        else:\n",
    "            data_site, ac_cap, dc_cap, EFF_SYSTEM, inverter = vvar_curt.site_organize(c_id, site_details, data_analyzed, unique_cids)\n",
    "            data_site = file_processing.resample_in_minute(data_site)\n",
    "            \n",
    "            if date in csd_list:\n",
    "                is_clear_sky_day = True\n",
    "            else:\n",
    "                is_clear_sky_day = False\n",
    "            \n",
    "            tripping_response, tripping_curt_energy, estimation_method, data_site = tripping_curt.check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date)    \n",
    "            energy_generated, data_site = energy_calculation.check_energy_generated(data_site, date, is_clear_sky_day, tripping_curt_energy)\n",
    "            \n",
    "            total_energy_generation += energy_generated\n",
    "            \n",
    "            if tripping_curt_energy > 0:\n",
    "                count_curtailment += 1\n",
    "                total_tripping_curtailment += tripping_curt_energy\n",
    "    \n",
    "    #summarize result\n",
    "    c_id_summary = pd.DataFrame({\n",
    "            'c_id' : [c_id],\n",
    "            'total_energy_generation (kWh)' : [total_energy_generation],\n",
    "            'total_tripping_curtailment (kWh)' : [total_tripping_curtailment],\n",
    "            'count curtailment (days)' : [count_curtailment],\n",
    "            'total days (days)' : [len(date_list)]\n",
    "        })\n",
    "    \n",
    "    summary = pd.concat([summary, c_id_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dd676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f84e0d",
   "metadata": {},
   "source": [
    "# SUMMARY PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df387c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['percentage_of_generation_lost'] = summary['total_tripping_curtailment (kWh)'] / summary['total_energy_generation (kWh)'] * 100\n",
    "summary['proportion of days with curtailment'] = summary['count curtailment (days)'] / summary['total days (days)'] * 100\n",
    "summary.sort_values(by = 'percentage_of_generation_lost', ascending = False, inplace = True)\n",
    "summary.reset_index(drop = True, inplace = True)\n",
    "summary['proportion_of_sites'] = summary.index/(len(summary)-1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(file_path + r'/tripping_alldays_7.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6030a7",
   "metadata": {},
   "source": [
    "# VISUALIZATION SIMILAR TO FIGURE 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = r\"/Users/samhan/Downloads/data\" #for running in Samhan's laptop\n",
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\" #for running in TETB CEEM09 computer\n",
    "\n",
    "summary = pd.read_csv(file_path +r'/tripping_alldays_summary_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ce51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(18, 10)\n",
    "\n",
    "ax1.scatter(summary['proportion_of_sites'], summary[\"percentage_of_generation_lost\"], color = 'b', \n",
    "            label = 'Percentage generation lost')\n",
    "\n",
    "ax1.set_xlabel('Proportion of sites', **fontdict)\n",
    "ax1.set_ylabel('Percentage of generation lost per site', **fontdict)\n",
    "\n",
    "ax1.set_xticks([0, 20, 40, 60, 80, 100])\n",
    "ax1.set_xticklabels(['0 %', '20 %', '40 %', '60 %', '80 %', '100 %'], **fontdict)\n",
    "ax1.set_ylim(0, 25);\n",
    "ax1.set_yticks([0, 5, 10, 15, 20, 25])\n",
    "ax1.set_yticklabels(['0 %', '5 %', '10 %', '15 %', '20 %', '25 %'], **fontdict)\n",
    "ax1.legend(loc = 9, prop={'size': 15})\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.scatter(summary['proportion_of_sites'], summary['proportion of days with curtailment'], color='r',\n",
    "           label = 'Proportion of days with some curtailment')\n",
    "ax2.set_ylabel('Proportion of days with some curtailment', **fontdict)\n",
    "ax2.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "ax2.set_yticklabels(['0 %', '20 %', '40 %', '60 %', '80 %', '100 %'], **fontdict)\n",
    "ax2.set_ylim(0, 100);\n",
    "ax2.legend(loc = 0, prop={'size': 15})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5242d",
   "metadata": {},
   "source": [
    "# UPDATING SPREADSHEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_before = pd.read_excel(file_path + r'/SolA Curtailment Summary_Final_tripping.xlsx')\n",
    "#spreadsheet_before.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "\n",
    "new_data = summary[['c_id', 'total_energy_generation (kWh)', 'total_tripping_curtailment (kWh)']].copy()\n",
    "new_data.rename(columns={\n",
    "    \"total_energy_generation (kWh)\": \"total_energy_generation_alldays (kWh)\",\n",
    "    \"total_tripping_curtailment (kWh)\": \"total_tripping_curtailment_alldays (kWh)\"}, inplace = True)\n",
    "\n",
    "spreadsheet_after = pd.merge(spreadsheet_before, new_data, left_on = 'c_id', right_on = 'c_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_after.to_excel(file_path + r'/SolA Curtailment Summary_Final_trippingalldays.xlsx', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca78cf39",
   "metadata": {},
   "source": [
    "# ANALYZE A SITE FOR A CERTAIN DAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e1f29",
   "metadata": {},
   "source": [
    "Just for testing purpose. It is not used in the actual script deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_id = 814682996\n",
    "date = '2020-04-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce810f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Analyzing c_id {} date {} ...'.format(c_id, date))\n",
    "        \n",
    "#filter D-PV time series data\n",
    "date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "month_analyzed = date[:4] + date [5:7]\n",
    "\n",
    "date_filter = data[month_analyzed].date == date_dt\n",
    "site_id_filter = data[month_analyzed]['c_id'] == c_id\n",
    "data_analyzed = data[month_analyzed].loc[date_filter & site_id_filter]\n",
    "\n",
    "#check tripping curtailment\n",
    "size_is_ok = file_processing.check_data_size(data_analyzed)\n",
    "if not size_is_ok:\n",
    "    print('Cannot analyze {} site on {} due to incomplete data.'.format(c_id, date))\n",
    "else:\n",
    "    data_site, ac_cap, dc_cap, EFF_SYSTEM, inverter = vvar_curt.site_organize(c_id, site_details, data_analyzed, unique_cids)\n",
    "    data_site = file_processing.resample_in_minute(data_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a295b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_site['power'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_site, polyfit, is_good_polyfit_quality = polyfit_f.check_polyfit(data_site, ac_cap)\n",
    "if date in csd_list:\n",
    "    is_clear_sky_day = True\n",
    "else:\n",
    "    is_clear_sky_day = False    \n",
    "tripping_response, tripping_curt_energy, estimation_method, data_site = tripping_curt.check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date)    \n",
    "energy_generated, data_site = energy_calculation.check_energy_generated(data_site, date, is_clear_sky_day, tripping_curt_energy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripping_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83023f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripping_curt_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzed['power'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripping_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_details.loc[site_details['c_id'] == 358283166 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzed['power'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c933b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_analyzed['power'].abs() > 300).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyzed.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63825691",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cids.loc[unique_cids['c_id'] == 814682996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a727694",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cids_4 = unique_cids.loc[147:198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cids_5 = unique_cids.loc[198:301]\n",
    "unique_cids_6 = unique_cids.loc[301:401]\n",
    "unique_cids_7 = unique_cids.loc[401:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63849cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_cids_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c0f47",
   "metadata": {},
   "source": [
    "# COMBINE ALL RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7624650",
   "metadata": {},
   "source": [
    "This is to combine some of the results in the previous section that is done in seperate times. Should not be needed if the above section is run only once for all sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa63cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_1 = pd.read_csv(file_path + r'/tripping_alldays_1.csv')\n",
    "summary_2 = pd.read_csv(file_path + r'/tripping_alldays_2.csv')\n",
    "summary_3 = pd.read_csv(file_path + r'/tripping_alldays_3.csv')\n",
    "summary_4 = pd.read_csv(file_path + r'/tripping_alldays_4.csv')\n",
    "summary_5 = pd.read_csv(file_path + r'/tripping_alldays_5.csv')\n",
    "summary_6 = pd.read_csv(file_path + r'/tripping_alldays_6.csv')\n",
    "summary_7 = pd.read_csv(file_path + r'/tripping_alldays_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_all_sites = pd.concat([summary_1, summary_2, summary_3, summary_4, summary_5, summary_6, summary_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_all_sites.sort_values(by = 'percentage_of_generation_lost', ascending = False, inplace = True)\n",
    "summary_all_sites.reset_index(drop = True, inplace = True)\n",
    "summary_all_sites['proportion_of_sites'] = summary_all_sites.index/(len(summary_all_sites)-1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_all_sites.to_csv(file_path + r'/tripping_alldays_summary_all.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c611dba",
   "metadata": {},
   "source": [
    "# MESSY BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105fff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd47330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
