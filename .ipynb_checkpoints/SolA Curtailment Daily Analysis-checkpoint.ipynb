{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c632c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pytz #for timezone calculation\n",
    "import math\n",
    "import matplotlib.dates as md\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "%matplotlib qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8abb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Global parameters for fonts & sizes =================\n",
    "font_size = 10\n",
    "rc={'font.size': font_size, 'axes.labelsize': font_size, 'legend.fontsize': font_size, \n",
    "    'axes.titlesize': font_size, 'xtick.labelsize': font_size, 'ytick.labelsize': font_size}\n",
    "plt.rcParams.update(**rc)\n",
    "plt.rc('font', weight='bold')\n",
    " \n",
    "# For label titles\n",
    "fontdict={'fontsize': font_size, 'fontweight' : 'bold'}\n",
    "# can add in above dictionary: 'verticalalignment': 'baseline' \n",
    "\n",
    "style = 'ggplot' # choose a style from the above options\n",
    "plt.style.use(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713a410",
   "metadata": {},
   "source": [
    "# ENERGY GENERATED CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_energy_generated(data_site, date):\n",
    "    sh_idx = (data_site.index.hour>= 7) & (data_site.index.hour <= 17)\n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    date_idx = data_site.index.date == date_dt\n",
    "    energy_generated = data_site.loc[sh_idx & date_idx, 'power'].resample('h').mean().sum()/1000\n",
    "    return energy_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06afb5",
   "metadata": {},
   "source": [
    "# CHECK CLEAR SKY DAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab69dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clear_sky_day(date):\n",
    "    dateFile = date[:4]+'_'+ date[5:7]\n",
    "    ghi = pd.read_csv(file_path +'/sl_023034_' + dateFile + \".txt\")\n",
    "    timestamp_date_string = Get_timestamp_date_string(dateFile)\n",
    "    separated_ghi_data = Separate_ghi_data(timestamp_date_string, ghi)\n",
    "    ghi_df = separated_ghi_data[date]\n",
    "    res, average_delta_y = Detect_clear_sky_day(ghi_df, 530)\n",
    "\n",
    "    if res:\n",
    "        #clear_sky_days.append(date)\n",
    "        #overall_clear_sky_days_dict[dateFile].append(date)\n",
    "        is_clear_sky_day = True\n",
    "    else:\n",
    "        is_clear_sky_day = False\n",
    "    return is_clear_sky_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba8379",
   "metadata": {},
   "source": [
    "# TRIPPING CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ee0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util #customized module to calculate the first derivative of the power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89428a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting cumulative count of 0 with reset on 1\n",
    "def rcount(a):\n",
    "    without_reset = (a == 0).cumsum()\n",
    "    reset_at = (a == 1)\n",
    "    overcount = np.maximum.accumulate(without_reset * reset_at)\n",
    "    result = without_reset - overcount\n",
    "    return result\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ed6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date):\n",
    "    # In Naomi's script, the first step is to clean the data (remove duplicates, convert to numeric, etc). However, since the inputted data here is already cleaned, we directly use the data without further cleaning process.\n",
    "    # Approx capacity factor value considered to be 'zero', e.g. less than 1% CF is zero.\n",
    "\n",
    "    # List of connection types for filtering\n",
    "    pv_list = ['pv_site_net', 'pv_site', 'pv_inverter_net']\n",
    "    CF_ZERO_APPROX = 0.01\n",
    "\n",
    "    # Approx cf derivative at which considered to be 'ramp'. That is, for at least a 10% change in capacity factor (ABSOLUTE!) expect to be ramping up or down.\n",
    "    # Note, only applied 'next to' zeros. So should not capture shading effects.\n",
    "    FIRST_DERIV_FALL_LIMIT = -0.05\n",
    "    FIRST_DERIV_INCREASE_LIMIT = 0.05\n",
    "    # For missing data check\n",
    "    ALLOWED_MISSING_DATA_PERCENTAGE = 0.05\n",
    "    # Average percentage of capacity at which a system must operate over the course of the day in order to be included in analysis\n",
    "    VERY_LOW_OUTPUT_AVE_PERCENTAGE_CAPACITY = 0.05\n",
    "\n",
    "\n",
    "    # Get data\n",
    "    unaltered_data = pd.DataFrame({\n",
    "        'c_id' : c_id,\n",
    "        'v' : data_site['voltage'],\n",
    "        'd' : data_site['duration'],\n",
    "        'site_id' : unique_cids.loc[unique_cids['c_id'] == c_id, 'site_id'].iloc[0],\n",
    "        'e' : data_site['energy'],\n",
    "        'con_type' : site_details.loc[site_details['c_id'] == c_id, 'con_type'].iloc[0],\n",
    "        'first_ac' : ac_cap/1000, #IDK what is this, but let's assume it is just the same with sum_ac, which is the inverter ac capacity\n",
    "        'power_kW' : data_site['power']/1000,\n",
    "        'reactive_power' : data_site['reactive_power'],\n",
    "        'clean' : 'cleaned',\n",
    "        'manufacturer' : site_details.loc[site_details['c_id'] == c_id, 'inverter_manufacturer'].iloc[0],\n",
    "        'model' : site_details.loc[site_details['c_id'] == c_id, 'inverter_model'].iloc[0],\n",
    "        'sum_ac' : ac_cap/1000,\n",
    "        'time_offset' : float(\"nan\")\n",
    "    }, index = data_site.index)\n",
    "    unaltered_data.index.rename('ts', inplace=True) \n",
    "\n",
    "    # rename energy column\n",
    "    unaltered_data = unaltered_data.rename(columns = {'e' : 'energy', 'd':'duration', 'sum_ac':'ac'})\n",
    "    # filter for clean\n",
    "    unaltered_data = unaltered_data[unaltered_data['clean']=='cleaned']\n",
    "    # Attempt to fix issues by sorting the index at the beginning\n",
    "    unaltered_data = unaltered_data.sort_index()\n",
    "    # Change name to t_stamp\n",
    "    unaltered_data.index.name = 't_stamp'\n",
    "    # Add time by seconds from start of the day\n",
    "    unaltered_data['hrs'] = unaltered_data.index.hour\n",
    "    unaltered_data['min'] = unaltered_data.index.minute\n",
    "    unaltered_data['sec'] = unaltered_data.index.second\n",
    "    unaltered_data['time_in_seconds'] = unaltered_data['hrs'] * 60 * 60 + unaltered_data['min'] * 60 + unaltered_data['sec']\n",
    "\n",
    "    # Get list of time_intervals\n",
    "    time_interval_list = unaltered_data['duration'].drop_duplicates().tolist()\n",
    "\n",
    "    # ********************* Further data cleaning [START] *********************\n",
    "    # Check for missing data issues\n",
    "    # TODO - this may not longer work since the Solar Analytics data can contain a mix of 60s and 5s data\n",
    "    # Flag sites with too much missing data (based on threshold), need to also keep the duration\n",
    "    missing_data_df = pd.DataFrame({'num_data_pts': unaltered_data.groupby('c_id')['energy'].count(), 'duration': unaltered_data.groupby('c_id')['duration'].first()}).reset_index()\n",
    "    # We now have two possible time intervals: 30s or 60s.\n",
    "    # Therefore, we need to run twice?\n",
    "    for time_interval in time_interval_list:\n",
    "        # Expected number of time periods\n",
    "        num_time_periods = 24 * 60 * (60 / time_interval)\n",
    "        # Get the minimum number of data points required in order to have enough data (i.e. not lots of missing data)\n",
    "        missing_data_threshold = num_time_periods * (1 - ALLOWED_MISSING_DATA_PERCENTAGE)\n",
    "        missing_data_df['missing_data_flag'] = np.nan\n",
    "        missing_data_df.loc[(missing_data_df['num_data_pts'] <= missing_data_threshold) & missing_data_df['duration']==time_interval , 'missing_data_flag'] = 1.0\n",
    "    # Merge information about missing data back onto time series df\n",
    "    unaltered_data = unaltered_data.reset_index().merge(missing_data_df, on='c_id', how='left').set_index('t_stamp')\n",
    "    # Filter unaltered data for only sites with sufficient data points\n",
    "    unaltered_data = unaltered_data[unaltered_data['missing_data_flag'] != 1.0]\n",
    "\n",
    "    # Filter for PV only\n",
    "    unaltered_data = unaltered_data[unaltered_data['con_type'].isin(pv_list)]\n",
    "\n",
    "    # First fix duratoin_x / duration_y problem. Not sure where it is coming from\n",
    "    # I don't know why, but there is duration_x and duration_y. Drop one and rename the other\n",
    "    unaltered_data = unaltered_data.drop(['duration_x'], axis=1)\n",
    "    unaltered_data = unaltered_data.rename(columns={'duration_y': 'duration'})\n",
    "\n",
    "    # Open _circuit_details_for_editing.csv file for sunrise/set times\n",
    "    assist_df = pd.DataFrame({\n",
    "        'c_id' : [c_id],\n",
    "        'energy_day' : [float(\"nan\")],\n",
    "        'energy_night' : [float(\"nan\")],\n",
    "        'con_type' : [site_details.loc[site_details['c_id'] == c_id, 'con_type'].iloc[0]],\n",
    "        'sunrise' : ['2/09/2019  6:06:06'], #later need to be edited\n",
    "        'sunset' : ['2/09/2019  19:28:19'], #later need to be edited\n",
    "        'min_power' : [data_site['power'].min()/1000],\n",
    "        'max_power' : [data_site['power'].max()/1000],\n",
    "        'polarity' : [site_details.loc[site_details['c_id'] == c_id, 'polarity'].iloc[0]],\n",
    "        'frac_day' : [float('nan')],\n",
    "        'old_con_type' : [float('nan')],\n",
    "        'con_type_change' : [float('nan')],\n",
    "        'site_id' : [unique_cids.loc[unique_cids['c_id'] == c_id, 'site_id'].iloc[0]]\n",
    "    })\n",
    "\n",
    "    # Check for PV sites with very low output and remove them\n",
    "    get_site_ac_df = unaltered_data[['site_id', 'first_ac', 'ac']]\n",
    "    get_site_ac_df = get_site_ac_df.drop_duplicates(subset='site_id')\n",
    "    # merge keeping only the site_ids in the time series df.\n",
    "    assist_df = assist_df.merge(get_site_ac_df, left_on='site_id', right_on='site_id', how='right')\n",
    "\n",
    "    # Check whether c_ids operated at less than an average of 5% capacity\n",
    "    # Compare using max power output compared with first_ac.\n",
    "    max_p_df = pd.DataFrame({'max_p_kW': unaltered_data.groupby('c_id')['power_kW'].max(), 'first_ac' : unaltered_data.groupby('c_id')['first_ac'].first()})\n",
    "    max_p_df['low_output_flag'] = np.nan\n",
    "    max_p_df.loc[max_p_df['max_p_kW'] < VERY_LOW_OUTPUT_AVE_PERCENTAGE_CAPACITY * max_p_df['first_ac'] , 'low_output_flag'] = 1\n",
    "    # Copy c_ids to a column (from index)\n",
    "    max_p_df['c_id'] = max_p_df.index\n",
    "    # Get list of c_ids to be excluded\n",
    "    c_ids_to_WITHOUT_low_output = max_p_df[max_p_df['low_output_flag'] != 1]\n",
    "    c_ids_to_WITHOUT_low_output = c_ids_to_WITHOUT_low_output['c_id'].tolist()\n",
    "\n",
    "    # Only keep sites that have enough output\n",
    "    unaltered_data = unaltered_data[unaltered_data['c_id'].isin(c_ids_to_WITHOUT_low_output)]\n",
    "    # ********************* Further data cleaning [END] *********************\n",
    "\n",
    "    # Get assist_df with c_id as index\n",
    "    assist_df_c_id = assist_df.set_index('c_id')\n",
    "\n",
    "    # Get c_id list\n",
    "    c_id_list = unaltered_data['c_id'].drop_duplicates().tolist()\n",
    "    # Set up output_df\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df.index.name = 't_stamp'\n",
    "\n",
    "    # Get data for c_id\n",
    "    data = unaltered_data[unaltered_data['c_id'] == c_id]\n",
    "\n",
    "    # Filter for sunshine hours - NOTE must add an hour to sunrise / subtract and hour from sunset since Nick's code did the opposite, but I actually would like correct sunrise/set\n",
    "    # Sunrise\n",
    "    sun_rise = assist_df_c_id.loc[c_id,'sunrise']\n",
    "    sun_rise = pd.to_datetime(sun_rise)\n",
    "    sun_rise_hour = sun_rise.hour\n",
    "    sun_rise_min = sun_rise.minute\n",
    "    if sun_rise_min <10 :\n",
    "        sun_rise_min = '0' + str(sun_rise_min)\n",
    "    else:\n",
    "        sun_rise_min = str(sun_rise_min)\n",
    "    sun_rise_for_filter = str(sun_rise_hour + 1) + ':' + sun_rise_min + ':' + str(00)\n",
    "\n",
    "    # Sunset\n",
    "    sun_set = assist_df_c_id.loc[c_id,'sunset']\n",
    "    sun_set = pd.to_datetime(sun_set)\n",
    "    sun_set_hour = sun_set.hour\n",
    "    sun_set_min = sun_set.minute\n",
    "    if sun_set_min <10 :\n",
    "        sun_set_min = '0' + str(sun_set_min)\n",
    "    else:\n",
    "        sun_set_min = str(sun_set_min)\n",
    "    sun_set_for_filter = str(sun_set_hour - 1) + ':' + sun_set_min + ':' + str(00)\n",
    "\n",
    "    data = data.between_time(sun_rise_for_filter, sun_set_for_filter)\n",
    "\n",
    "    # Calc CF\n",
    "    data['unaltered_cf'] = data['power_kW'] / data['ac']\n",
    "    # Flag approximate zeroes (cf < CF_ZERO_APPROX)\n",
    "    data['unaltered_zero_flag'] = 0\n",
    "    data.loc[data['unaltered_cf'] <= CF_ZERO_APPROX, 'unaltered_zero_flag'] = 1\n",
    "    data['non_zero_flag_count'] = data['unaltered_zero_flag']\n",
    "\n",
    "    # Remove cases where 'blip' occurs. e.g. above zero but only for a max of 2 time intervals.\n",
    "    # TODO - may be better to remove this step since we are also looking at non-clear sky days!\n",
    "    # First, count the non zeros\n",
    "    a = data['non_zero_flag_count']\n",
    "    # Now remove from data\n",
    "    data = data.drop(['non_zero_flag_count'], axis=1)\n",
    "    # Count non zeros\n",
    "    result = rcount(a)\n",
    "    # Add onto data\n",
    "    data = pd.concat([data,result], axis=1)\n",
    "\n",
    "    # Copy the unaltered zero flag - we will then remove the 'blips' from it.\n",
    "    data['zero_flag'] = data['unaltered_zero_flag']\n",
    "\n",
    "    # Case where single point of 'non zero' before returning to zero\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-1) == 0) & (data['non_zero_flag_count'] == 1),'zero_flag'] = 1\n",
    "\n",
    "    # If the non zero flag count in this row is 2 and in the next row is zero, then set zero_flag to 1 (i.e. remove 'blip')\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-1) == 0) & (data['non_zero_flag_count'] == 2),'zero_flag'] = 1\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-2) == 0) & (data['non_zero_flag_count'].shift(-1) == 2),'zero_flag'] = 1\n",
    "\n",
    "    # Set CF to zero where zero flag occurs\n",
    "    data['cf'] = data['unaltered_cf']\n",
    "    data.loc[data['zero_flag'] == 1,'cf'] = 0\n",
    "\n",
    "    # Get first derivative of cf\n",
    "    data = util.calculate_first_derivative_of_variable(data, 'cf')\n",
    "\n",
    "    # --------------------------------- Reductions immediately before zero\n",
    "    # Falling dramatically before zeros\n",
    "    data['start_deriv_flag'] = 0\n",
    "    # Just get the first instance of ramp down\n",
    "    # e.g. Times where zero flag (t+1) = 1, zero flag (t) <>1 and cf_first_deriv < limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 1) & (data['zero_flag'] == 0) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    # Dealing with 'soft' disconnect\n",
    "    # Next interval is zero flagged, current value is greater than 'zero' limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 1) & (data['cf'] > CF_ZERO_APPROX),'start_deriv_flag'] = 1\n",
    "\n",
    "    # Get the next instance of ramp down (well, previous) - repeat four times. Effectively means you can capture periods in which power falls over 5 time intervals (including initial one captured above)\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "\n",
    "    # --------------------------------- Increases immediately after zero\n",
    "    # Increasing dramatically after zeros\n",
    "    data['end_deriv_flag'] = 0\n",
    "    # Just get the first instance of ramp up\n",
    "    # e.g. Times where zero flag (t) = 1, zero flag (t+1) <>1 and cf_first_deriv > limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 0) & (data['zero_flag'] == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    # Dealing with 'soft' restarts.\n",
    "    # Previous value was zero flagged, current value is greater than the 'zero' limit\n",
    "    data.loc[(data['zero_flag'].shift(+1) == 1) & (data['cf'] > CF_ZERO_APPROX),'end_deriv_flag'] = 1\n",
    "\n",
    "    # Get next instances (x8 as slower ramp up potentially)\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "\n",
    "    # --------------------------------- Get 'ramp' start and end points\n",
    "    # Get start points\n",
    "    data['start_pts'] = 0\n",
    "    # Case where 'start_derive_flag' is zero in previous interval (t-1), and one in current interval (t)\n",
    "    data.loc[(data['start_deriv_flag'].shift(+1) == 0) & (data['start_deriv_flag'] == 1),'start_pts'] = 1\n",
    "\n",
    "    # Get end points\n",
    "    data['end_pts'] = 0\n",
    "    # Case where 'end_deriv_flag' is 1 in previous interval (t-1), and 0 in current interval (t)\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['end_deriv_flag'] == 0),'end_pts'] = 1\n",
    "\n",
    "    # --------------------------------- Add some things to data that we need\n",
    "    # Check that the first 'start point' occurs before the first 'end point'. If not, then delete the first 'end point'\n",
    "    # Else in the early hours of the day as the generation goes from zero to non zero it looks like 'soft start'\n",
    "    # Check three times over (TODO would be better to do this as a while somehow... maybe so that it stops once the first_end_point stops changing?)\n",
    "    try:\n",
    "        for i in range(0,3):\n",
    "            first_end_point = data[data['end_pts']==1]\n",
    "            first_end_point = first_end_point['time_in_seconds'].iloc[0]\n",
    "            # Find first start point\n",
    "            first_start_point = data[data['start_pts']==1]\n",
    "            first_start_point = first_start_point['time_in_seconds'].iloc[0]\n",
    "            # Check that the first start point occurs after the first end point.\n",
    "            if first_end_point < first_start_point:\n",
    "                data.loc[data['time_in_seconds']==first_end_point, 'end_pts'] = 0\n",
    "    except:\n",
    "        x = 1\n",
    "\n",
    "    # Identify the start and end pt number (so they can be matched to each other)\n",
    "    data['start_cumsum'] = data['start_pts'].cumsum()\n",
    "    data['end_cumsum'] = data['end_pts'].cumsum()\n",
    "\n",
    "    # Get cumulative zeros between 'start' and 'end' pts\n",
    "    data['count_during_period'] = data['start_pts']\n",
    "    data.loc[data['end_pts'] == 1,'count_during_period'] =1\n",
    "    a = data['count_during_period']\n",
    "\n",
    "    # Copy as a renamed column then remove ^ name from data\n",
    "    data['start_end_combined'] = data['count_during_period']\n",
    "    # NOTE - possible issue here? Dropped column but then used later. NO - all good, it's added back on a few\n",
    "    # lines below using rcount function then merge.\n",
    "    data = data.drop(['count_during_period'], axis=1)\n",
    "\n",
    "    # Do count on df 'a' which contains the 'count_during_period' data from a few lines above.\n",
    "    result = rcount(a)\n",
    "    data = pd.concat([data,result], axis=1)\n",
    "\n",
    "    # Flag 'estimate' period (i.e. between start and end pts)\n",
    "    data['est_period'] = data['start_cumsum'] - data['end_cumsum']\n",
    "\n",
    "    # --------------------------------- get start and end dfs, then get ramp df and merge onto data\n",
    "    start_df = data[data['start_pts']==1]\n",
    "    end_df = data[data['end_pts']==1]\n",
    "\n",
    "    # In cases where there are no events, need to 'try'\n",
    "    try:\n",
    "        # Create new ramp_df.\n",
    "        # NOTE use +2 in the range in order to capture additional end points if the first end point occurs before the first start point.\n",
    "        # May make sense to even add a couple.. (i.e. +3 or +4) however this will do for now.\n",
    "        count_start_pts = start_df['start_cumsum'].max()\n",
    "        ramp_df = pd.DataFrame(data=list(range(1,int(count_start_pts+2))), columns=['event_num'])\n",
    "\n",
    "        # Get data from dfs\n",
    "        # Keep only cf, time_int and start_cumsum.\n",
    "        start_df = start_df[['cf', 'time_in_seconds', 'start_cumsum']]\n",
    "        # Then merge on start_cumsum\n",
    "        ramp_df = ramp_df.merge(start_df, left_on='event_num', right_on='start_cumsum')\n",
    "        # Rename columns\n",
    "        ramp_df = ramp_df.rename(columns = {'cf' : 'start_cf'})\n",
    "        ramp_df = ramp_df.rename(columns = {'time_in_seconds' : 'start_time_int'})\n",
    "\n",
    "        # Keep only cf, time)nt and start_cumsum.\n",
    "        end_df = end_df[['cf', 'time_in_seconds', 'end_cumsum']]\n",
    "        # Then merge on start_cumsum\n",
    "        ramp_df = ramp_df.merge(end_df, left_on='event_num', right_on='end_cumsum')\n",
    "        # Rename columns\n",
    "        ramp_df = ramp_df.rename(columns = {'cf' : 'end_cf'})\n",
    "        ramp_df = ramp_df.rename(columns = {'time_in_seconds' : 'end_time_int'})\n",
    "\n",
    "        # Check for cases where end time is BEFORE start time for an event.\n",
    "        # If this is the case, then delete that end time and shift all end times up by one.\n",
    "        # Check each event from top to bottom\n",
    "        num_events = ramp_df['event_num'].max()\n",
    "        for i in range(0, int(num_events)):\n",
    "            if ramp_df.loc[i, 'end_time_int'] < ramp_df.loc[i, 'start_time_int']:\n",
    "                ramp_df['end_time_int'] = ramp_df['end_time_int'].shift(-1)\n",
    "\n",
    "        # Calc the ramp rate\n",
    "        ramp_df['m'] = (ramp_df['end_cf'] - ramp_df['start_cf']) / (ramp_df['end_time_int'] - ramp_df['start_time_int'])\n",
    "\n",
    "        # Drop end and start cumsum, then merge onto data\n",
    "        ramp_df = ramp_df.drop(['end_cumsum', 'start_cumsum'], axis=1)\n",
    "        zero_row_for_ramp_df = pd.DataFrame(data=[0], columns=['event_num'])\n",
    "        ramp_df = pd.concat([ramp_df, zero_row_for_ramp_df])\n",
    "\n",
    "        data = data.reset_index().merge(ramp_df,  left_on='start_cumsum', right_on='event_num').set_index('t_stamp')\n",
    "        # Calc estimated CF\n",
    "        data['count_during_period_using_start_time'] = data['est_period'] * (data['time_in_seconds'] - data['start_time_int'])\n",
    "        data['est_cf'] = data['est_period'] * (data['start_cf'] + data['count_during_period_using_start_time']*data['m'])\n",
    "        # Add the 'end point'\n",
    "        data.loc[data['end_pts']==1,'est_cf'] = data['cf']\n",
    "\n",
    "        # Get est kW and est kWh\n",
    "        data['est_kW'] = data['est_cf'] * data['ac']\n",
    "        data['est_kWh'] = data['est_cf'] * data['ac'] * data['duration']/(60*60)\n",
    "\n",
    "        # Get power lost estimate\n",
    "        data['gen_loss_est_kWh'] = data['est_kWh'] - (data['power_kW']* data['duration']/(60*60))\n",
    "        # Issue is that we don't want gen lost to be less than zero!\n",
    "        data.loc[data['gen_loss_est_kWh'] <0,'gen_loss_est_kWh'] = 0\n",
    "        data['no_PV_curtail'] = 0\n",
    "    except:\n",
    "        data['no_PV_curtail'] = 1\n",
    "\n",
    "    # --------------------------------- concat onto output_df\n",
    "    output_df = pd.concat([output_df, data])\n",
    "    output_df['gen_kWh'] = output_df['power_kW'] * output_df['duration']/(60*60)\n",
    "    \n",
    "    if data['no_PV_curtail'].iloc[0] == 1:\n",
    "        estimation_method = 'None'\n",
    "        tripping_response = 'None'\n",
    "        tripping_curt_energy = 0\n",
    "    else:\n",
    "        # Clean output_df before exporting to csv\n",
    "        output_df_to_export = output_df[['ac','c_id','cf','clean','con_type','duration','energy','est_cf','est_kW',\n",
    "                                             'est_kWh','reactive_power','first_ac','gen_kWh','gen_loss_est_kWh','manufacturer',\n",
    "                                             'model','power_kW','site_id','v',\n",
    "                                             'zero_flag', 'time_in_seconds']]\n",
    "\n",
    "\n",
    "        # --------------------------------- Get summary stats\n",
    "        # Get site_id list\n",
    "        site_id_list = unaltered_data['site_id'].drop_duplicates().tolist()\n",
    "        # Create df to store results\n",
    "        sum_stats_df = pd.DataFrame(index=site_id_list)\n",
    "\n",
    "        # Get data of relevance from output_df, summarised by site_id\n",
    "        meta_df = pd.DataFrame({'power_kW': output_df.groupby('site_id')['power_kW'].sum(),\n",
    "        'gen_loss_est_kWh': output_df.groupby('site_id')['gen_loss_est_kWh'].sum(),\n",
    "        'event_num': output_df.groupby('site_id')['event_num'].max(),\n",
    "        'duration': output_df.groupby('site_id')['duration'].first(),\n",
    "        'mean_v_all_daylight_hours': output_df.groupby('site_id')['v'].mean(),\n",
    "        'first_ac': output_df.groupby('site_id')['first_ac'].first(),\n",
    "        'ac': output_df.groupby('site_id')['ac'].first(),\n",
    "        'model': output_df.groupby('site_id')['model'].first(),\n",
    "        'manufacturer': output_df.groupby('site_id')['manufacturer'].first()\n",
    "        })\n",
    "\n",
    "        # Concat onto results df and name the index\n",
    "        sum_stats_df = pd.concat([sum_stats_df, meta_df], axis=1)\n",
    "        sum_stats_df.index.name = 'site_id'\n",
    "\n",
    "        # Convert generation to kWh\n",
    "        sum_stats_df['gen_kWh'] = sum_stats_df['power_kW'] * sum_stats_df['duration']/(60*60)\n",
    "        # sum_stats_df = sum_stats_df.rename(columns = {'power_kW' : 'gen_kWh'})\n",
    "\n",
    "        # Calc percentage of gen lost\n",
    "        sum_stats_df['percentage_lost'] = sum_stats_df['gen_loss_est_kWh'].abs() / (sum_stats_df['gen_loss_est_kWh'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "\n",
    "        # Get voltage box plot statistics for both curtail times and non curtail times\n",
    "        curtail_v_df = output_df[output_df['est_period'] == 1]\n",
    "        all_other_v_df = output_df[output_df['est_period'] != 1]\n",
    "        # Filter for voltage and site it then get summary stats\n",
    "        # Curtail times\n",
    "        curtail_v_df = curtail_v_df[['v','site_id']]\n",
    "        # rename 'v' to 'curtail_v' in order to see which is which when added to sum_stats_df\n",
    "        curtail_v_df = curtail_v_df.rename(columns = {'v' : 'v_curtail'})\n",
    "        curtail_v_box_plot_stats_df = curtail_v_df.groupby('site_id').describe()\n",
    "        # Non curtail times\n",
    "        all_other_v_df = all_other_v_df[['v','site_id']]\n",
    "        # rename 'v' to 'other_v' in order to see which is which when added to sum_stats_df\n",
    "        all_other_v_df = all_other_v_df.rename(columns = {'v' : 'v_all_other'})\n",
    "        all_other_v_box_plot_stats_df = all_other_v_df.groupby('site_id').describe()\n",
    "\n",
    "        # add box plot stats onto summary stats\n",
    "        sum_stats_df = pd.concat([sum_stats_df, curtail_v_box_plot_stats_df, all_other_v_box_plot_stats_df], axis=1)\n",
    "\n",
    "        # Get penetration by postcode\n",
    "        # TODO - need to update the CER and APVI data files to match the Solar Analytics data set period being analysed!\n",
    "        # TODO - could not locate the same type of APVI file (for dwellings) so may need to use the older data.\n",
    "        # TODO - the CER data will require some attention and util will have to be updated to make it accept the updated CER data.\n",
    "        # sum_stats_df = util.get_penetration_by_postcode(PC_INSTALLS_DATA_FILE_PATH, DWELLINGS_DATA_FILE_PATH, sum_stats_df, output_df)\n",
    "\n",
    "        # Sort and get % of systems\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost', ascending =False)\n",
    "\n",
    "        # Get % of systems\n",
    "        sum_stats_df['proportion_of_sites'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites'] = (sum_stats_df['proportion_of_sites'] + 1) / len(sum_stats_df)\n",
    "\n",
    "        #BELOW IS ADAPTED FROM NAOMI'S POLYFIT METHOD\n",
    "\n",
    "        # Gets PV curtailment estimate using a polynomial fit method with an iterative step to remove 'outliers'\n",
    "        # (only really useful for clear sky days! Otherwise the straight line approximation is preferable!!)\n",
    "        # See write up of method for key limitations and next steps\n",
    "\n",
    "        #------------------------ Step 0: Import required packages ------------------------\n",
    "        # Import packages required for program\n",
    "        import matplotlib.dates as mdates\n",
    "        import seaborn as sns; sns.set()\n",
    "        # For graphing time series\n",
    "        time_fmt = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "        '''\n",
    "        # Data files are located here:\n",
    "        INPUT_DATA_FILE_PATH = 'F:/05_Solar_Analytics/2021-05-31_CANVAS_Solar_Analytics_data/02_Curtail_output/'\n",
    "        OUTPUT_FILE_PATH = \"F:/05_Solar_Analytics/2021-05-31_CANVAS_Solar_Analytics_data/03_Polyfit_output/\"\n",
    "\n",
    "        # File names are here:\n",
    "        TS_DATA_FILE_PATH = '_analysis_profiles_v4.csv'\n",
    "        SUM_STATS_DATA_FILE_PATH = \"_analysis_sum_stats_v4.csv\"\n",
    "        OUTPUT_PROFILES = \"_analysis_profiles_polyfit_v4.csv\"\n",
    "        OUTPUT_SUM_STATS = \"_analysis_sum_stats_polyfit_v4.csv\"\n",
    "\n",
    "        # File path for clear sky days csv\n",
    "        CLEAR_SKY_DAYS_FILE_PATH = 'F:/CANVAS/clear_sky_days_01-2019_07-2020_manual.csv'\n",
    "        '''\n",
    "        # This value is used to remove data points when calculating the polynomial.\n",
    "        # The first polynomial uses all non zero cf values.\n",
    "        # Then the straight line correlation between polyfit and actual cf is calculated and residuals found for each cf\n",
    "        # Data points with residuals greater than or less than the allowed residual band are removed and\n",
    "        # the polynomial fit is recalculated using this smaller subset of points: 'polyfit_iter'\n",
    "        allowed_residual_band = 0.05 # NOTE - set to 0.05 after some sensitivity testing and eye balling\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        #for data_date in data_date_list:\n",
    "        data_date = date\n",
    "        # Load PV profiles\n",
    "        data_df = output_df_to_export\n",
    "        # Load clear sky days CSV and flag NON clear sky days in data_df\n",
    "        #clear_sky_days_df = pd.read_csv(CLEAR_SKY_DAYS_FILE_PATH)\n",
    "        #clear_sky_days_list = clear_sky_days_df['clear_sky_days'].astype(str).tolist()\n",
    "        #if data_date in clear_sky_days_list:\n",
    "        if is_clear_sky_day:\n",
    "            data_df['non_clear_sky_day_flag'] = 0\n",
    "        else:\n",
    "            data_df['non_clear_sky_day_flag'] = 1\n",
    "\n",
    "        # Get list of c_ids\n",
    "        c_id_list = data_df['c_id'].drop_duplicates().tolist()\n",
    "        # Set up output_df\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df.index.name = 't_stamp'\n",
    "        counter = 0\n",
    "\n",
    "        #for c_id in c_id_list:\n",
    "        c_id = c_id\n",
    "        perc_complete = counter / len(c_id_list)\n",
    "        #print(perc_complete)\n",
    "        counter += 1\n",
    "\n",
    "        # Filter for c_id\n",
    "        pv_data = data_df[data_df['c_id'] == c_id]\n",
    "        pv_data['t_stamp_copy'] = pv_data.index\n",
    "\n",
    "        # First get time in seconds for polyfit\n",
    "        pv_data['hrs'] = pv_data.index.hour\n",
    "        pv_data['min'] = pv_data.index.minute\n",
    "        pv_data['sec'] = pv_data.index.second\n",
    "        pv_data['time_in_seconds'] = pv_data['hrs'] * 60 * 60 + pv_data['min'] * 60 + pv_data['sec']\n",
    "\n",
    "        # Try applying a 2nd order polynomial **to non-zero cf points only**\n",
    "        # Needs to be 'try' because if there are ONLY zero points then error b/c we pass an empty df to polyfit function\n",
    "        try:\n",
    "            test = pv_data[pv_data['cf']>0]\n",
    "            x = test['time_in_seconds']\n",
    "            y = test['cf']\n",
    "            z = np.polyfit(x,y,2)\n",
    "\n",
    "            # Calc the fitted line\n",
    "            test['polynomial_fit'] = z[0]*test['time_in_seconds']*test['time_in_seconds'] + \\\n",
    "                                            z[1]*test['time_in_seconds'] + z[2]\n",
    "            # This is calculated for all times (not just non zero) as well for printing / checking\n",
    "            pv_data['polynomial_fit'] = z[0]*pv_data['time_in_seconds']*pv_data['time_in_seconds'] + \\\n",
    "                                            z[1]*pv_data['time_in_seconds'] + z[2]\n",
    "\n",
    "            # Get the correlation between my polynomial and the cf data (excluding zeroes) then remove points with\n",
    "            # 'large' residuals\n",
    "            # Get line of best fit\n",
    "            test['ones'] = 1\n",
    "            A = test[['cf', 'ones']]\n",
    "            y = test['polynomial_fit']\n",
    "            m,c = np.linalg.lstsq(A,y)[0]\n",
    "            test['y_line'] = c + m*test['cf']\n",
    "\n",
    "            # Remove data points where the residual is +/- allowed_residual_band from the line of best fit\n",
    "            # (in an attempt to improve our correlation)\n",
    "            test['residuals'] = test['polynomial_fit'] - test['y_line']\n",
    "            test_filtered = test[test['residuals'].abs() <= allowed_residual_band]\n",
    "\n",
    "            # Use this filtered curve to get a new polyfit\n",
    "            x = test_filtered['time_in_seconds']\n",
    "            y = test_filtered['cf']\n",
    "            z = np.polyfit(x,y,2)\n",
    "\n",
    "            test_filtered['polynomial_fit'] = z[0]*test_filtered['time_in_seconds']*test_filtered['time_in_seconds'] + \\\n",
    "                                              z[1]*test_filtered['time_in_seconds'] + z[2]\n",
    "            pv_data['polyfit_iter'] = z[0]*pv_data['time_in_seconds']*pv_data['time_in_seconds'] + \\\n",
    "                                            z[1]*pv_data['time_in_seconds'] + z[2]\n",
    "            # Where there is est_cf (i.e. it's identified as a period of curtailment and so we have a straight line\n",
    "            # estimate) then use est_cf_polyfit_iter\n",
    "            pv_data['est_cf_polyfit_iter'] = np.nan\n",
    "            pv_data.loc[pv_data['est_cf']>0, 'est_cf_polyfit_iter'] = pv_data['polyfit_iter']\n",
    "\n",
    "            # Just keep the polyfit_iter for the periods where there was already a straight line estimate as above\n",
    "            pv_data = pv_data.drop(['polynomial_fit'], axis=1)\n",
    "\n",
    "            # Get est kW and est kWh\n",
    "            pv_data['est_kW_polyfit_iter'] = pv_data['est_cf_polyfit_iter'] * pv_data['ac']\n",
    "            pv_data['est_kWh_polyfit_iter'] = pv_data['est_cf_polyfit_iter'] * \\\n",
    "                                              pv_data['ac'] * pv_data['duration'] / (60 * 60)\n",
    "            # Get power lost estimate\n",
    "            pv_data['gen_loss_est_kWh_polyfit_iter'] = pv_data['est_kWh_polyfit_iter'] - pv_data['gen_kWh']\n",
    "            # Issue is that we don't want gen lost to be less than zero!\n",
    "            pv_data.loc[pv_data['gen_loss_est_kWh_polyfit_iter'] < 0, 'gen_loss_est_kWh_polyfit_iter'] = 0\n",
    "\n",
    "        except:\n",
    "            print('Error somewhere in the polyfit process for c_id ' + str(c_id))\n",
    "\n",
    "        # --------------------------------- concat onto output_df\n",
    "        output_df = pd.concat([output_df, pv_data])\n",
    "\n",
    "        # *********************************** CHECKS and identify 'preferred' method ***********************************\n",
    "        # Check on polyfit giving large cfs (>=1) --> allowed if the cf for that c_id is already large\n",
    "        # For each c_id get max polyfit and max cf\n",
    "        cf_max_check = pd.DataFrame({'cf_max' : output_df.groupby('c_id')['cf'].max(),\n",
    "                                     'polyfit_iter_cf_max' : output_df.groupby('c_id')['est_cf_polyfit_iter'].max(),\n",
    "                                     'site_id' : output_df.groupby('c_id')['site_id'].first()})\n",
    "        # Find cases where straight line and polyfit iter methods return cf >= 1\n",
    "        cf_max_check['straight_line_max_greater_or_equal_1'] = np.nan\n",
    "        cf_max_check.loc[cf_max_check['cf_max'] >= 1, 'straight_line_max_greater_or_equal_1'] = 1\n",
    "        cf_max_check['polyfit_iter_max_greater_or_equal_1'] = np.nan\n",
    "        cf_max_check.loc[cf_max_check['polyfit_iter_cf_max'] >= 1, 'polyfit_iter_max_greater_or_equal_1'] = 1\n",
    "        # Flag cases where straight line method must be used. i.e. the polyfit iter cf max is  >= 1, but straight line cf max is not.\n",
    "        cf_max_check = cf_max_check.fillna(0)\n",
    "        cf_max_check['must_use_straight_line_method_due_to_cf_max'] = cf_max_check['polyfit_iter_max_greater_or_equal_1'] - cf_max_check['straight_line_max_greater_or_equal_1']\n",
    "        cf_max_check.loc[cf_max_check['must_use_straight_line_method_due_to_cf_max'] < 0, 'must_use_straight_line_method_due_to_cf_max'] = 0\n",
    "        # Get new df by site_id in order to merge onto output_df\n",
    "        cf_max_check_by_site_id = pd.DataFrame({'must_use_straight_line_method_due_to_cf_max' : cf_max_check.groupby('site_id')['must_use_straight_line_method_due_to_cf_max'].max()})\n",
    "\n",
    "        # Check whether the straight line or polyfit iter gives a larger total generation lost.\n",
    "        # We want to take the larger of the two.\n",
    "        gen_loss_total_check = pd.DataFrame({'straight_line_gen_loss_total' : output_df.groupby('site_id')['gen_loss_est_kWh'].sum(),\n",
    "                                             'polyfit_iter_gen_loss_total' : output_df.groupby('site_id')['gen_loss_est_kWh_polyfit_iter'].sum()})\n",
    "        gen_loss_total_check['must_use_straight_line_method_due_to_gen_loss_total'] = np.nan\n",
    "        gen_loss_total_check.loc[gen_loss_total_check['straight_line_gen_loss_total'] > gen_loss_total_check['polyfit_iter_gen_loss_total'], 'must_use_straight_line_method_due_to_gen_loss_total'] = 1\n",
    "        gen_loss_total_check = gen_loss_total_check.fillna(0)\n",
    "        gen_loss_total_check = gen_loss_total_check[['must_use_straight_line_method_due_to_gen_loss_total']]\n",
    "\n",
    "        # Merge both checks back onto output_df and create a single column: use straight line estimate over polyfit iter? Y/N\n",
    "        output_df = output_df.merge(cf_max_check_by_site_id, left_on='site_id', right_index=True, how='left')\n",
    "        output_df = output_df.merge(gen_loss_total_check, left_on='site_id', right_index=True, how='left')\n",
    "        # Get flag if either conditions are true\n",
    "        # OR if not a clear sky day\n",
    "        output_df['use_straight_line_method_flag'] = output_df['must_use_straight_line_method_due_to_gen_loss_total'] + output_df['must_use_straight_line_method_due_to_cf_max'] + output_df['non_clear_sky_day_flag']\n",
    "        output_df.loc[output_df['use_straight_line_method_flag'] > 1, 'use_straight_line_method_flag'] = 1\n",
    "        output_df['use_polyfit_iter_method_flag'] = 1 - output_df['use_straight_line_method_flag']\n",
    "\n",
    "        # Set the preferred est_cf_preferred etc to the polyfit method, unless the straight line flag is present,\n",
    "        # in which case use the straight line method\n",
    "        output_df['est_cf_preferred'] = (output_df['est_cf_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_cf'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['est_kW_preferred'] = (output_df['est_kW_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_kW'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['est_kWh_preferred'] = (output_df['est_kWh_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_kWh'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['gen_loss_est_kWh_preferred'] = (output_df['gen_loss_est_kWh_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['gen_loss_est_kWh'] * output_df['use_straight_line_method_flag'])\n",
    "\n",
    "        # Optional save data to csv\n",
    "        #output_df.to_csv(OUTPUT_FILE_PATH + data_date + OUTPUT_PROFILES)\n",
    "\n",
    "        # --------------------------------- Summary stuff\n",
    "        # Calc the new generation lost amount by site and also get the max for checking that polyfit doesn't go above 1\n",
    "        # Also add the reason for selecting polyfit or linear estimate\n",
    "        new_gen_lost = pd.DataFrame({ 'gen_loss_est_kWh_polyfit_iter' : output_df.groupby('site_id')['gen_loss_est_kWh_polyfit_iter'].sum(),\n",
    "                                      'gen_loss_est_kWh_preferred' : output_df.groupby('site_id')['gen_loss_est_kWh_preferred'].sum(),\n",
    "                                      'linear_method_preferred' : output_df.groupby('site_id')['use_straight_line_method_flag'].max(),\n",
    "                                      'polyfit_method_preferred' : output_df.groupby('site_id')['use_polyfit_iter_method_flag'].max()})\n",
    "\n",
    "        # Open previous sum stats\n",
    "        #sum_stats_df = pd.read_csv(INPUT_DATA_FILE_PATH + data_date + SUM_STATS_DATA_FILE_PATH)\n",
    "        sum_stats_df = sum_stats_df\n",
    "\n",
    "        # Append on the new gen lost\n",
    "        sum_stats_df = sum_stats_df.merge(new_gen_lost, left_on='site_id', right_index=True)\n",
    "\n",
    "        # Calc percentage of gen lost using polyfit iter and preferred\n",
    "        sum_stats_df['percentage_lost_polyfit_iter'] = sum_stats_df['gen_loss_est_kWh_polyfit_iter'].abs() / (sum_stats_df['gen_loss_est_kWh_polyfit_iter'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "        sum_stats_df['percentage_lost_preferred'] = sum_stats_df['gen_loss_est_kWh_preferred'].abs() / (sum_stats_df['gen_loss_est_kWh_preferred'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "\n",
    "        # Get proportion of sites for graphing using polyfit iter and preferred\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost_polyfit_iter', ascending =False)\n",
    "        sum_stats_df['proportion_of_sites_polyfit_iter'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites_polyfit_iter'] = (sum_stats_df['proportion_of_sites_polyfit_iter'] + 1) / len(sum_stats_df)\n",
    "        # Preferred\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost_preferred', ascending =False)\n",
    "        sum_stats_df['proportion_of_sites_preferred'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites_preferred'] = (sum_stats_df['proportion_of_sites_preferred'] + 1) / len(sum_stats_df)\n",
    "\n",
    "        # Save summary statistics to  csv\n",
    "        #sum_stats_df.to_csv(OUTPUT_FILE_PATH + data_date + OUTPUT_SUM_STATS)\n",
    "\n",
    "        tripping_curt_energy = output_df['gen_loss_est_kWh_preferred'].sum()\n",
    "        generated_energy = output_df['est_kWh_preferred'].sum()\n",
    "\n",
    "        use_polyfit_iter_method_flag = output_df['use_polyfit_iter_method_flag'].iloc[0]\n",
    "        if use_polyfit_iter_method_flag == 1:\n",
    "            estimation_method = 'Polyfit'\n",
    "        else:\n",
    "            estimation_method = 'Linear'\n",
    "        if tripping_curt_energy > 0:\n",
    "            tripping_response = 'Yes'\n",
    "        else:\n",
    "            tripping_response = 'None'\n",
    "\n",
    "    return tripping_response, tripping_curt_energy, estimation_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83034b",
   "metadata": {},
   "source": [
    "# VVAr CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single site data and relevant meta-data information\n",
    "def site_organize(c_id_idx, site_details, data, unique_cids):\n",
    "\n",
    "    #c_id = unique_cids.loc[c_id_idx][0]\n",
    "    c_id = c_id_idx\n",
    "    \n",
    "    polarity = site_details.loc[site_details['c_id'] == c_id, 'polarity'].values[0] # get the polarity of the site\n",
    "    ac_cap = site_details.loc[site_details['c_id'] == c_id, 'ac_cap_w'].values[0]\n",
    "    dc_cap = site_details.loc[site_details['c_id'] == c_id, 'dc_cap_w'].values[0]\n",
    "    inverter = site_details.loc[site_details['c_id'] == c_id, 'inverter_manufacturer'].values[0] + ' ' + site_details.loc[site_details['c_id'] == c_id, 'inverter_model'].values[0]\n",
    "\n",
    "    # Extract single site data and organize: \n",
    "    data_site = data[data['c_id'] == c_id].sort_index() # get the monthly data of the specific c_id\n",
    "\n",
    "    data_site['power'] = data_site['power'].values * polarity # polarity correction for real power\n",
    "    data_site['reactive_power'] = data_site['reactive_power'].values * polarity # polarity correction for reactive power\n",
    "    \n",
    "    data_site['reactive_power'] = [data_site['reactive_power'].values * -1 if np.percentile(data_site.loc[(data_site.index.hour >= 7) & (data_site.index.hour <= 17), 'reactive_power'], 75) < 0 else data_site['reactive_power'].values][0]  # double check the polarity for reactive power\n",
    "    \n",
    "    if (abs(np.percentile(data_site['reactive_power'], 99))> ac_cap) | (abs(np.percentile(data_site['reactive_power'], 1))> ac_cap): #some VAr measurements in energy format and needs to be divided by duration (i.e., 60 sec)\n",
    "        # data_site['reactive_power'] =  data_site['reactive_power'].values / data_site['duration'].values # unfortunately SolA data doesn't calculate energy according to respective duration but uses a fixed 60 sec values for energy calculation\n",
    "        data_site['reactive_power'] =  data_site['reactive_power'].values / 60\n",
    "        \n",
    "    data_site.index = pd.to_datetime([str(d)[0:19] for d in data_site.index]) ## convert index to make the df plottable (by removing the UTC conversion)\n",
    "    data_site.sort_index(ascending = True, inplace = True) # sort the index in ascending form\n",
    "    # System efficiency for calculating theoretical max output later on (use conservative loss estimates for DC power)\n",
    "    eff_inv = 0.98\n",
    "    eff_vdrop = 0.98 \n",
    "    eff_derating = 0.99  # module derating losses\n",
    "    eff_system = eff_inv * eff_vdrop * eff_derating\n",
    "\n",
    "    # Apparent power of the inverter\n",
    "    data_site['va'] = np.sqrt (data_site['power'].values**2 + data_site['reactive_power'].values**2)\n",
    "    data_site['pf'] = data_site['power']/data_site['va']\n",
    "    \n",
    "    return data_site, ac_cap, dc_cap, eff_system, inverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vvar_curtailment(c_id, date, data_site,  ghi, ac_cap, dc_cap, eff_system, is_clear_sky_day):\n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    data_site_certain_date = data_site.loc[data_site.index.date == date_dt]\n",
    "    ghi = ghi.loc[ghi.index.date == date_dt]\n",
    "    data_site = data_site_certain_date\n",
    "    \n",
    "    # Manipulations on the original data_site to match the GHI\n",
    "    dummy = data_site.copy()\n",
    "    dummy.index = dummy.index.round('min')   # round the timestamp to nearest minute to match with the GHI\n",
    "    dummy = dummy.groupby(level = 0 ).mean()  # average same timestamp values that fall under the same minute category\n",
    "\n",
    "    data_site_complete = pd.DataFrame (index = ghi.index)  # create a data_site_complete with complete set of dates to match with GHI\n",
    "    data_site_complete = data_site_complete.join(dummy)\n",
    "    \n",
    "    # Required conditions for V-VAr curtailment\n",
    "    var_t = 100  # min VAr condition \n",
    "    duration = 60  # we have normalized all t-stamps to 60 second previously\n",
    "    va_criteria = data_site_complete['va'] >= (ac_cap - var_t)  # this is to ensure inverter VA is close to its rated capacity (this eliminates the instances of tripping)\n",
    "    var_criteria = abs(data_site_complete['reactive_power'].values) > var_t  # this is to ensure inverter is injecting/absorbing at least 100 vars\n",
    "    curt_criteria = va_criteria & var_criteria  # curtailment criteria that satisfies the two criteria above\n",
    "\n",
    "    data_curtailment = data_site_complete[curt_criteria]  # investigate curtailment only for the instances which satisfy above criteria \n",
    "    ghi_curtailment = ghi[curt_criteria]\n",
    "    \n",
    "    if not var_criteria.any():\n",
    "        vvar_response = 'None'\n",
    "    else:\n",
    "        vvar_response = 'Yes'\n",
    "        \n",
    "    # max_real_power refers to what the system could generate if it wasn't curtailed\n",
    "    #ISSUES FOR TROUBLESHOOTING LATER: SOMETIME MAX POWER IS LESS THAN POWER?\n",
    "    if is_clear_sky_day:\n",
    "        # POLYFIT METHOD TO CALCULATE THE MAX POWER WITHOUT CURTAILMENT, UNAPPLICABLE IN NON CLEAR SKY DAYS\n",
    "        circuit_day_data = data_site_complete.reset_index(level=0)\n",
    "        circuit_day_data.rename(columns = {'timestamp':'ts'}, inplace = True)\n",
    "        circuit_day_data['ts'] = circuit_day_data['ts'].astype(str)\n",
    "\n",
    "        df = circuit_day_data\n",
    "        df = SliceEndOffDF(df) # REMOVES LAST TAIL AND HEAD OF DATA AFTER IT CHANGES TO ZERO WATTS, BUT KEEPS ZERO WATT VALUES IN THE MIDDLE OF THE LIST\n",
    "\n",
    "        df = df.loc[df['power'] > 300]\n",
    "\n",
    "        # FILTER POWER DATA TO INCLUDE ONLY INCREASING VALUES FROM EACH SIDES (WHERE SIDES ARE DETERMINED BY EITHER SIDE OF THE MAX POWER VALUE)\n",
    "        powerArray, timeArray = FilterPowerData(df)\n",
    "\n",
    "        # FILTER DATA SO ONLY A SUBSET OF GRADIENTS BETWEEN DATAPOINTS IS PERMITTED\n",
    "        powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "        polyfit = GetPolyfit(getDateTimeList(timeArray), powerArray, 2)\n",
    "\n",
    "        polyfit_result = pd.DataFrame({\n",
    "            'timestamp' : pd.date_range(start=df['ts'].iloc[0], end=df['ts'].iloc[-1], freq='1min').astype(str)\n",
    "        })\n",
    "        polyfit_result['max_real_power'] = polyfit(getDateTimeList(polyfit_result['timestamp']))\n",
    "        polyfit_result.index = pd.to_datetime(polyfit_result['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        polyfit_result.drop(columns = 'timestamp', inplace = True)\n",
    "\n",
    "        data_curtailment = pd.merge(data_curtailment, polyfit_result, left_index = True, right_index = True)\n",
    "        data_curtailment ['curtailment'] = data_curtailment['max_real_power'].values - data_curtailment ['power'].values\n",
    "        data_curtailment['curtailment_energy'] = data_curtailment['curtailment'].values * (duration/3600/1000) # Wmin to kWh energy: some sites have variable duration so finding curtailment in energy form (Wh)\n",
    "        \n",
    "        if not data_curtailment[data_curtailment['curtailment_energy'] > 0]['curtailment_energy'].sum() > 0:\n",
    "                data_curtailment['max_real_power'] = [min(ghi_t/1000 * dc_cap * eff_system, ac_cap) for ghi_t in ghi_curtailment['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    \n",
    "    else: #if it is not clear sky day, use ghi to estimate maximum power without curtailmentz\n",
    "        data_curtailment['max_real_power'] = [min(ghi_t/1000 * dc_cap * eff_system, ac_cap) for ghi_t in ghi_curtailment['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    # =============================================================================================\n",
    "    \n",
    "    data_curtailment ['curtailment'] = data_curtailment['max_real_power'].values - data_curtailment ['power'].values\n",
    "    data_curtailment['curtailment_energy'] = data_curtailment['curtailment'].values * (duration/3600/1000) # Wmin to kWh energy: some sites have variable duration so finding curtailment in energy form (Wh)\n",
    "    vvar_curt_energy = data_curtailment[data_curtailment['curtailment_energy'] > 0]['curtailment_energy'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4f261",
   "metadata": {},
   "source": [
    "# VWATT CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0db49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VWATT CLASSES AND FUNCTIONS\n",
    "\n",
    "# SITE AND CIRCUIT CLASSES USED TO ORGANISE THE SITES TELEMETRY AND META DATA FOR EASE IN PROCESSING\n",
    "\n",
    "class Site:\n",
    "  def __init__(self, site_id, s_postcode, pv_install_date, ac_cap_w, dc_cap_w, inverter_manufacturer, inverter_model):\n",
    "    self.site_id = site_id\n",
    "    self.s_postcode = s_postcode\n",
    "    self.pv_install_date = pv_install_date\n",
    "    self.ac_cap_w = ac_cap_w\n",
    "    self.dc_cap_w = dc_cap_w\n",
    "    self.inverter_manufacturer = inverter_manufacturer\n",
    "    self.inverter_model = inverter_model\n",
    "    self.c_id_data = {}\n",
    "\n",
    "class Circuit:\n",
    "  def __init__(self, c_id, site_id, con_type, polarity):\n",
    "    self.c_id = c_id\n",
    "    self.con_type = con_type\n",
    "    self.polarity = polarity\n",
    "    self.day_data = {}\n",
    "    \n",
    "# ADJUST FORMATE FOR TIMESTAMP STRINGS\n",
    "def Get_timestamp_date_string(string):\n",
    "    x = string.split(\"_\")\n",
    "    return x[0] + \"-\" + x[1]\n",
    "\n",
    "# SEPARATE THE BoM GHI DATA FILES PER DAY TO SEARCH FOR CLEAR SKY DAYS\n",
    "def Separate_ghi_data(month, ghi):\n",
    "    ghi['ts'] = pd.to_datetime(pd.DataFrame({'year': ghi['Year Month Day Hours Minutes in YYYY'].values,\n",
    "                                                    'month': ghi['MM'],\n",
    "                                                    'day': ghi['DD'],\n",
    "                                                    'hour': ghi['HH24'],\n",
    "                                                    'minute': ghi['MI format in Local standard time']}))\n",
    "    ghi.rename(columns={'Mean global irradiance (over 1 minute) in W/sq m': 'mean_ghi',\n",
    "                        'Minimum 1 second global irradiance (over 1 minute) in W/sq m': 'min_ghi',\n",
    "                        'Maximum 1 second global irradiance (over 1 minute) in W/sq m': 'max_ghi',\n",
    "                        'Standard deviation of global irradiance (over 1 minute) in W/sq m': 'sd_ghi',\n",
    "                        'Uncertainty in mean global irradiance (over 1 minute) in W/sq m': 'uncertainty_ghi'},\n",
    "               inplace=True)\n",
    "    key_ghi_values = ghi[['ts', 'mean_ghi', 'min_ghi', 'max_ghi', 'sd_ghi', 'uncertainty_ghi']].copy()\n",
    "    key_ghi_values['mean_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['mean_ghi']), axis=1)\n",
    "    key_ghi_values['min_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['min_ghi']), axis=1)\n",
    "    key_ghi_values['max_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['max_ghi']), axis=1)\n",
    "\n",
    "\n",
    "    combined_ghi_dict = {}\n",
    "    month_number = int(month.split('-')[1])\n",
    "\n",
    "    for day in range(1, Days_in_month(month_number) + 1):\n",
    "        day_string = str(day)\n",
    "        if day < 10:\n",
    "            day_string = \"0\" + day_string\n",
    "\n",
    "        date = month + \"-\" + day_string\n",
    "        df = key_ghi_values.loc[key_ghi_values['ts'] > date + \" 00:00:01\"]\n",
    "        df = df.loc[key_ghi_values['ts'] < date + \" 23:59:01\"]\n",
    "\n",
    "        combined_ghi_dict[date] = df\n",
    "\n",
    "    return combined_ghi_dict\n",
    "\n",
    "# REMOVE SPACES AND CHECK IF VALUE NULL\n",
    "def String_to_Float(string):\n",
    "    x = string.strip()\n",
    "    if not x:\n",
    "        x = 0\n",
    "    else:\n",
    "        x = float(x)\n",
    "    return x\n",
    "\n",
    "def Days_in_month(month):\n",
    "        switcher = {\n",
    "            1: 31,\n",
    "            2: 29,\n",
    "            3: 31,\n",
    "            4: 30,\n",
    "            5: 31,\n",
    "            6: 30,\n",
    "            7: 31,\n",
    "            8: 31,\n",
    "            9: 30,\n",
    "            10: 31,\n",
    "            11: 30,\n",
    "            12: 31,\n",
    "        }\n",
    "        return switcher.get(month, 0)\n",
    "    \n",
    "# LOOK FOR FOR SUDDEN VARIATIONS IN SOLAR INSOLATION DATA WHICH INDICATES LIKELY CLOUD COVER, AS OPPOSED TO CLEAR PARABOLIC SHAPE OF CLEAR SKY DAY GHI CURVES\n",
    "def Detect_clear_sky_day(ghi_df, min_max_ghi):\n",
    "    df_daytime = ghi_df.loc[ghi_df['mean_ghi'] > 0]\n",
    "\n",
    "    collective_change = 0\n",
    "    ghi_list = df_daytime.mean_ghi.tolist()\n",
    "\n",
    "    for i in range(len(ghi_list)-1):\n",
    "        collective_change += abs(ghi_list[i+1] - ghi_list[i])\n",
    "\n",
    "    if len(df_daytime.index) == 0:\n",
    "        return False, 0\n",
    "    \n",
    "    average_delta_y = collective_change/len(df_daytime.index)\n",
    "\n",
    "    if average_delta_y < 5 and max(ghi_df.mean_ghi) > min_max_ghi:\n",
    "        return True, average_delta_y\n",
    "    else:\n",
    "        return False, average_delta_y\n",
    "    \n",
    "def Get_telemetry_string(string):\n",
    "    x = string.split(\"_\")\n",
    "    return x[0] + x[1]\n",
    "\n",
    "def Filter_data_clear_sky_days(data, clear_sky_days):\n",
    "    filtered_df = None\n",
    "    \n",
    "    for day in clear_sky_days:\n",
    "        tmp_df = data.loc[data['utc_tstamp'] > Convert_SA_time_to_UTC(day + \" 00:00:01\")]\n",
    "        tmp_df = tmp_df.loc[tmp_df['utc_tstamp'] < Convert_SA_time_to_UTC(day + \" 23:59:01\")]\n",
    "\n",
    "        if filtered_df is None:\n",
    "            filtered_df = tmp_df\n",
    "        else:\n",
    "            filtered_df = filtered_df.append(tmp_df, ignore_index=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def Convert_SA_time_to_UTC(sa_time):\n",
    "    timeFormat = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(sa_time, timeFormat)\n",
    "    sa_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    utc_time = pytz.utc\n",
    "    sa_moment = sa_local_time.localize(x, is_dst=None)\n",
    "    utc_time = sa_moment.astimezone(utc_time)\n",
    "    a = utc_time.strftime(timeFormat)\n",
    "    return a\n",
    "\n",
    "# ORGANISES ALL TELEMETRY IN THE HIERARCHY: SITE->CIRCUITS->DAYS_OF_DATA\n",
    "def Organise_sites(clear_sky_days, site_id_list, month, inverter_telemetry, site_details, cicuit_details):  # add gen data\n",
    "\n",
    "    overall_site_organiser = {}\n",
    "\n",
    "    for site_id in site_id_list:\n",
    "        if site_id not in site_details.site_id.unique():\n",
    "            continue\n",
    "        overall_site_organiser[site_id] = Organise_individual_site(clear_sky_days, site_id, month, inverter_telemetry,\n",
    "                                        site_details.loc[site_details['site_id'] == site_id],\n",
    "                                        cicuit_details.loc[\n",
    "                                            cicuit_details['site_id'] == site_id])\n",
    "\n",
    "    return overall_site_organiser\n",
    "\n",
    "\n",
    "def Organise_individual_site(clear_sky_days, site_id, month, inverter_telemetry, site_details, cicuit_details):\n",
    "    site = Site(site_id, site_details.iloc[0].s_postcode, site_details.iloc[0].pv_install_date,\n",
    "                site_details.iloc[0].ac_cap_w,\n",
    "                site_details.iloc[0].dc_cap_w, site_details.iloc[0].inverter_manufacturer,\n",
    "                site_details.iloc[0].inverter_model)\n",
    "\n",
    "    for row in cicuit_details.iterrows():\n",
    "        c_id = row[1].c_id\n",
    "        site.c_id_data[c_id] = Organise_individual_circuit(clear_sky_days, c_id, site_id, month,\n",
    "                                inverter_telemetry.loc[inverter_telemetry['c_id'] == c_id],\n",
    "                                row[1].con_type, row[1].polarity)\n",
    "\n",
    "    return site\n",
    "\n",
    "\n",
    "def Organise_individual_circuit(clear_sky_days, c_id, site_id, month, inverter_telemetry, con_type, polarity):\n",
    "    circuit = Circuit(c_id, site_id, con_type, polarity)\n",
    "    inverter_telemetry['ts'] = inverter_telemetry.apply(lambda row: Convert_to_SA_time(row['utc_tstamp']), axis=1)\n",
    "\n",
    "\n",
    "    month_number = int(month.split('-')[1])\n",
    "    for day in clear_sky_days:\n",
    "\n",
    "        circuit.day_data[day] = Organise_individual_day(day, inverter_telemetry)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def Organise_individual_day(date, inverter_telemetry):\n",
    "    inverter_telemetry = inverter_telemetry.loc[inverter_telemetry['ts'] > date + \" 00:00:01\"]\n",
    "    inverter_telemetry = inverter_telemetry.loc[inverter_telemetry['ts'] < date + \" 23:59:01\"]    \n",
    "    return inverter_telemetry.sort_values('ts', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='first', ignore_index=False, key=None)\n",
    "\n",
    "def Find_over_voltage_sites(v, clear_sky_data, cicuit_details):\n",
    "    site_id_list_ov = {}\n",
    "    # Determine subsets of sites that experience over voltages to different extents for later selection\n",
    "    testVs = list(range(235, 256))\n",
    "    for i in testVs:\n",
    "        site_id_list_ov[i] = []\n",
    "\n",
    "    c_id_list = clear_sky_data.c_id.unique()\n",
    "\n",
    "    for c_id in c_id_list:\n",
    "        C_id_to_site_id(c_id, cicuit_details)\n",
    "\n",
    "    for c_id in c_id_list:\n",
    "\n",
    "        df = clear_sky_data.loc[clear_sky_data['c_id'] == c_id]\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "\n",
    "        df = df.loc[df['power'] > 0]\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "\n",
    "        maxV = max(df.voltage)\n",
    "\n",
    "        site_id = C_id_to_site_id(c_id, cicuit_details)\n",
    "\n",
    "        for i in testVs:\n",
    "            if maxV > i:\n",
    "                if c_id not in site_id_list_ov[i]:\n",
    "                    site_id_list_ov[i].append(site_id)\n",
    "\n",
    "    for i in testVs:\n",
    "        print(\"Length vMax > \" + str(i) + \": \" + str(len(site_id_list_ov[i])))\n",
    "\n",
    "    return site_id_list_ov\n",
    "\n",
    "# REUTRN THE SITE ID THAT CORRESPONDS TO A GIVEN CIRCUIT ID\n",
    "def C_id_to_site_id(c_id, cicuit_details):\n",
    "    return cicuit_details.loc[cicuit_details['c_id'] == c_id].iloc[0].site_id\n",
    "\n",
    "# CONVERT TIMESTAMP STRINGS FROM UTC TO LOCAL SOUTH AUSTRALIA TIME, TODO: ADJUST FOR TELEMETRY ANALYSIS IN OTHER CITIES\n",
    "def Convert_to_SA_time(utc_tstamp):\n",
    "    timeFormat1 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    timeFormat2 = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(utc_tstamp, timeFormat1)\n",
    "    adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    utc_time = pytz.utc\n",
    "    utc_moment = utc_time.localize(x, is_dst=None)\n",
    "    adelaide_local_time = utc_moment.astimezone(adelaide_local_time)\n",
    "    a = adelaide_local_time.strftime(timeFormat2)\n",
    "    return a\n",
    "\n",
    "# ASSESS AGGREGATED V-WATT DATA FOR A SITE\n",
    "def Assess_Volt_Watt_behaviour_site(site, clear_sky_days, overall_volt_watt_dict):\n",
    "\n",
    "    for c_id in site.c_id_data.keys():\n",
    "        circuit = site.c_id_data[c_id]\n",
    "        Assess_Volt_Watt_behaviour_circuit(circuit, clear_sky_days, site.dc_cap_w, site.ac_cap_w, overall_volt_watt_dict)\n",
    "\n",
    "def Assess_Volt_Watt_behaviour_circuit(circuit, clear_sky_days, dc_cap_w, ac_cap_w, overall_volt_watt_dict):\n",
    "    \n",
    "    for date in clear_sky_days:\n",
    "        voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray = Append_Volt_Watt_behaviour_data(circuit.day_data[date], circuit.c_id, date, ac_cap_w)\n",
    "        if voltArray is not None:\n",
    "            \n",
    "            Display_day(circuit.c_id, date, circuit.day_data[date], ac_cap_w, voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray)\n",
    "            if circuit.c_id not in overall_volt_watt_dict.keys():\n",
    "                overall_volt_watt_dict[circuit.c_id] = {\"v\": [], 'p': [], 'd': 0}\n",
    "\n",
    "            overall_volt_watt_dict[circuit.c_id]['v'] += voltArray\n",
    "            overall_volt_watt_dict[circuit.c_id]['p'] += relativeWattArray\n",
    "            overall_volt_watt_dict[circuit.c_id]['d'] += 1\n",
    "    print(\"Length of sites determined to be assessable: \" + str(len(overall_volt_watt_dict.keys())))\n",
    "\n",
    "# ORGANISE DATA FOR DETERMINING COMPLIANCE FUNCTION BELOW\n",
    "def Append_Volt_Watt_behaviour_data(df, c_id, date, dc_cap_w):    \n",
    "\n",
    "    if df is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if len(df.index) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    if max(df.power) < 0.3:\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = SliceEndOffDF(df)\n",
    "\n",
    "    df = df.loc[df['power'] > 300]\n",
    "\n",
    "    if len(df.index) < 20:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Filter power data for only 'uncurtailed instances' (estimation as it is unknown when inverter is actively curtailing output)\n",
    "    powerArray, timeArray = FilterPowerData(df)\n",
    "\n",
    "    # Filter data for limited gradients, useful in creating more accurate polyfits, as determined by visual verification\n",
    "    powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "    if powerArray is None or len(powerArray) < 20:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Get polyfit estimation\n",
    "    polyfit = GetPolyfit(getDateTimeList(timeArray), powerArray, 2)\n",
    "\n",
    "    # Simple filter for very high and low values to aid in displaying data in figures\n",
    "    filteredPowerArray, filteredTimeArray = FilterArray(polyfit(getDateTime(df)), getDateTime(df), 100000, 0)\n",
    "    \n",
    "    filteredPowerArray = Change_W_to_kW(filteredPowerArray)\n",
    "    \n",
    "    maxPower = max(filteredPowerArray)\n",
    "    \n",
    "\n",
    "    maxCompliance = 0\n",
    "    bestVWLimit = 248\n",
    "    bestTotalPoints = 1\n",
    "\n",
    "    # Determine which data points are of interest for compliance by comparing actual output vs polyfit predicted output, and voltage conditions\n",
    "    # Ie. W-Watt curtailment can only occur when P_modelled > P_max_allowed.\n",
    "    complianceCount, voltArrayCompliance, timeArrayCompliance, absoluteWattArrayCompliance, relativeWattArrayCompliance, successfulRelativeWattArray, successfulVoltArray = DetermineCompliance(\n",
    "        polyfit, df, dc_cap_w, 248)\n",
    "    maxVoltWattTimeArray, maxVoltWattPowerArray = getMaxVoltWattCurve(dc_cap_w, df, 249)\n",
    "\n",
    "\n",
    "    if len(voltArrayCompliance) > 0:\n",
    "        return voltArrayCompliance, relativeWattArrayCompliance, filteredTimeArray, filteredPowerArray\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "def SliceEndOffDF(df):\n",
    "    if df is None or len(df.index) == 0:\n",
    "        return None\n",
    "\n",
    "    tmpDF = df.loc[df['power'] > 0]\n",
    "    if len(tmpDF.index) == 0:\n",
    "        return None\n",
    "\n",
    "    startTime = tmpDF.iloc[0].ts\n",
    "    endTime = tmpDF.iloc[len(tmpDF.index) - 1].ts\n",
    "\n",
    "    df = df.loc[df['ts'] > startTime]\n",
    "    df = df.loc[df['ts'] < endTime]\n",
    "\n",
    "    return df\n",
    "\n",
    "# FILTER POWER DATA TO INCLUDE ONLY INCREASING VALUES FROM EACH SIDES (WHERE SIDES ARE DETERMINED BY EITHER SIDE OF THE MAX POWER VALUE)\n",
    "def FilterPowerData(graphDF):\n",
    "    if len(graphDF.index) == 0:\n",
    "        return None, None\n",
    "\n",
    "    maxDailyPower = max(graphDF.power)\n",
    "\n",
    "    if len(graphDF.loc[graphDF['power'] == maxDailyPower].index) > 1:\n",
    "        return None, None\n",
    "\n",
    "    filterArray1 = []\n",
    "    filterArray2 = []\n",
    "    powerArray = graphDF.power\n",
    "    timeArray = graphDF.ts\n",
    "\n",
    "    halfFlag = True  # True is first half, False is second half\n",
    "    waterMark = 0\n",
    "\n",
    "    for currPower in powerArray:\n",
    "\n",
    "        # IF currPower IS GREATER THAN waterMark (LAST HIGHEST VALUE) THEN INCLUDE currPower AND INCREASE waterMark\n",
    "        if currPower > waterMark:\n",
    "            waterMark = currPower\n",
    "            filterArray1.append(True)\n",
    "        else:\n",
    "            filterArray1.append(False)\n",
    "\n",
    "        if currPower == maxDailyPower:\n",
    "            break\n",
    "\n",
    "    waterMark = 0\n",
    "\n",
    "    # PERFORM SAME FILTER ON SECOND SIDE OF POWER ARRAY\n",
    "    for currPower in powerArray.iloc[::-1]:\n",
    "\n",
    "        if currPower == maxDailyPower:\n",
    "            break\n",
    "\n",
    "        if currPower > waterMark:\n",
    "            waterMark = currPower\n",
    "            filterArray2.append(True)\n",
    "        else:\n",
    "            filterArray2.append(False)\n",
    "\n",
    "    # COMBINE TO FILTERED SIDES\n",
    "    filterArray2.reverse()\n",
    "    filterArray = filterArray1 + filterArray2\n",
    "    return powerArray[filterArray], timeArray[filterArray]\n",
    "\n",
    "# FILTER DATA SO ONLY A SUBSET OF GRADIENTS BETWEEN DATAPOINTS IS PERMITTED\n",
    "def FilterDataLimitedGradients(powerArray, timeArray):\n",
    "\n",
    "    if powerArray is None:\n",
    "        return None, None\n",
    "\n",
    "    # IN GENERAL ANLGE MUST BE BETWEEN THESE VALUES\n",
    "    angleLowerLimit = 80\n",
    "    angleUpperLimit = 90\n",
    "\n",
    "    # BUT AFTER 'continuanceLimit' CONTINUOUS VALUES HAVE BEEN ACCEPTED, THE LOWER ANGLE LIMIT IS RELAXED TO THIS VALUE BELOW\n",
    "    widerAngleLowerLimit = 70\n",
    "    continuanceLimit = 2\n",
    "\n",
    "    gradients = []\n",
    "    timeGradients = []\n",
    "    powerArray = powerArray.tolist()\n",
    "    timeArray = timeArray.tolist()\n",
    "    filterArray = []\n",
    "\n",
    "    n = len(powerArray)\n",
    "    gradientsCompliance = [0] * n\n",
    "\n",
    "    runningCount = 0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        g = abs(math.degrees(math.atan((powerArray[i] - powerArray[i - 1]) / (\n",
    "                    getSingleDateTime(timeArray[i]) - getSingleDateTime(timeArray[i - 1])))))\n",
    "\n",
    "        addFlag = False\n",
    "\n",
    "        if g > angleLowerLimit and g < angleUpperLimit:\n",
    "            addFlag = True\n",
    "            runningCount += 1\n",
    "\n",
    "        elif runningCount > continuanceLimit and g > widerAngleLowerLimit:\n",
    "            addFlag = True\n",
    "\n",
    "        else:\n",
    "            runningCount = 0\n",
    "\n",
    "        if addFlag:\n",
    "            gradientsCompliance[i - 1] += 1\n",
    "            gradientsCompliance[i] += 1\n",
    "\n",
    "        if g > 85:\n",
    "            gradients.append(g)\n",
    "            timeGradients.append(timeArray[i])\n",
    "\n",
    "    if gradientsCompliance[0] == 1 and gradientsCompliance[1] == 2:\n",
    "        filterArray.append(True)\n",
    "    else:\n",
    "        filterArray.append(False)\n",
    "\n",
    "    for i in range(1, n - 1):\n",
    "        if gradientsCompliance[i] == 2:\n",
    "            filterArray.append(True)\n",
    "        elif gradientsCompliance[i] == 1 and (gradientsCompliance[i - 1] == 2 or gradientsCompliance[i + 1] == 2):\n",
    "            filterArray.append(True)\n",
    "        else:\n",
    "            filterArray.append(False)\n",
    "\n",
    "    if gradientsCompliance[n - 1] == 1 and gradientsCompliance[n - 2] == 2:\n",
    "        filterArray.append(True)\n",
    "    else:\n",
    "        filterArray.append(False)\n",
    "    \n",
    "\n",
    "    powerArray = pd.Series(powerArray)\n",
    "    timeArray = pd.Series(timeArray)\n",
    "\n",
    "    powerArray = powerArray[filterArray]\n",
    "    timeArray = timeArray[filterArray]\n",
    "\n",
    "    return powerArray, timeArray\n",
    "\n",
    "# INTEGRATE POWER OUTPUT DATA OVER EACH DAY FOR COMPARISON WITH CURTAILMENT CALCUALTIONS\n",
    "def determine_total_energy_yields(month, monthly_data, site_organiser):\n",
    "    count = 0\n",
    "    for site in site_organiser.values():\n",
    "        \n",
    "        for c in site.c_id_data.values():\n",
    "            if c.c_id not in total_energy_yield_dict.keys():\n",
    "                total_energy_yield_dict[c.c_id] = {}\n",
    "            count += 1\n",
    "            print(\"count: \" + str(count))\n",
    "            total_energy_yield_dict[c.c_id][month] = calculate_months_energy_yield(c.c_id, monthly_data)\n",
    "            \n",
    "# INTEGRATE POWER OUTPUT DATA OVER EACH DAY FOR COMPARISON WITH CURTAILMENT CALCUALTIONS\n",
    "def calculate_months_energy_yield(c_id, monthly_data):\n",
    "    c_data = monthly_data.loc[monthly_data['c_id'] == c_id]\n",
    "\n",
    "    c_data['utc_tstamp'] = c_data.apply(lambda row: remove_tstamp_ms(row['utc_tstamp']), axis=1)\n",
    "    \n",
    "    c_data = c_data.sort_values('utc_tstamp', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='first', ignore_index=False, key=None)\n",
    "\n",
    "    powerData = c_data.power.tolist()\n",
    "    timeData = c_data.utc_tstamp.tolist()\n",
    "    MeasuredEnergy = AreaUnderCurve(timeData, powerData)/1000\n",
    "    return MeasuredEnergy  \n",
    "\n",
    "# REMOVING MILISECOND VALUE IN TIMESTAMP STRINGS\n",
    "def remove_tstamp_ms(tstamp_string):\n",
    "    timeFormat1 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    timeFormat2 = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(tstamp_string, timeFormat1)\n",
    "    return x.strftime(timeFormat2)\n",
    "\n",
    "def AreaUnderCurve(timeData, powerData):\n",
    "    \n",
    "    energy = 0\n",
    "    \n",
    "    for i in range(0, len(timeData) - 1):\n",
    "        t2 = ChangeToTimestamp(timeData[i+1])\n",
    "        t1 = ChangeToTimestamp(timeData[i])\n",
    "        \n",
    "        dt = t2-t1\n",
    "        \n",
    "        trapArea = (dt / 3600) * 0.5 * (powerData[i] + powerData[i+1])\n",
    "        energy += trapArea\n",
    "        \n",
    "    return energy\n",
    "\n",
    "def ChangeToTimestamp(timeString):\n",
    "    element = datetime.strptime(timeString,'%Y-%m-%d %H:%M:%S')\n",
    "    return datetime.timestamp(element)\n",
    "\n",
    "# CONVERT A SINGLE STRING TIMESTAMP TO DATETIME OBJECTS\n",
    "def getSingleDateTime(d):\n",
    "    return md.date2num(datetime.strptime(d, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# GET POLYFIT OF DESIRED DEGREE, TRANSFORMING DATETIMES INTO UNIX TIMESTAMPS \n",
    "def GetPolyfit(xArray, yArray, functionDegree):\n",
    "    timestamps = xArray\n",
    "    xp = np.linspace(timestamps[0], timestamps[len(timestamps) - 1], 1000)\n",
    "    z = np.polyfit(timestamps, yArray, functionDegree)\n",
    "    polyfit = np.poly1d(z)\n",
    "\n",
    "    return polyfit\n",
    "\n",
    "# FILTER ARRAY TO INCLUDE VALUES WITHIN A CERTAIN RANGE\n",
    "def FilterArray(xArray, yArray, maxVal, minVal):\n",
    "    filter_arr = []\n",
    "    for val in xArray:\n",
    "        if val > maxVal or val < minVal:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    # NOTE: conversion between series and lists was for conveniences of used filter operator, but could be adjusted for better time performance\n",
    "    xSeries = pd.Series(xArray)\n",
    "    ySeries = pd.Series(yArray)\n",
    "\n",
    "    return xSeries[filter_arr].tolist(), ySeries[filter_arr].tolist()\n",
    "\n",
    "# CONVERT A LIST STRING TIMESTAMP TO DATETIME OBJECTS\n",
    "def getDateTimeList(List):\n",
    "    dates = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in List]\n",
    "    datenums = md.date2num(dates)\n",
    "    return datenums\n",
    "\n",
    "# TRANSFORM A TIMESTAMP STRING INTO A TIMESTAMP INT VALUE (SECONDS SINCE 1970)\n",
    "def getDateTime(df):\n",
    "    dates = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in df.ts]\n",
    "    datenums = md.date2num(dates)\n",
    "    return datenums\n",
    "\n",
    "def Change_W_to_kW(filteredPowerArray):\n",
    "    l = []\n",
    "    for x in filteredPowerArray:\n",
    "        l.append(x/1000)\n",
    "        \n",
    "    return l\n",
    "\n",
    "# INDIVIDUAL DAY/SITE ANALYSIS \n",
    "def DetermineCompliance(polyfit, graphDF, maxPower, vwLimit):\n",
    "\n",
    "    voltArray = []\n",
    "    timeArray = []\n",
    "    absoluteWattArray = []\n",
    "    relativeWattArray = []\n",
    "\n",
    "    successfulRelativeWattArray = []\n",
    "    successfulVoltArray = []\n",
    "\n",
    "    complianceArray = []\n",
    "\n",
    "    # TODO: Changing to list aided with how analysis functions were created, should be kept as pd series and adjust analysis functions for better time performance\n",
    "    dfPower = graphDF.power.tolist()\n",
    "    dfTime = graphDF.ts.tolist()\n",
    "    dfVoltage = graphDF.voltage.tolist()\n",
    "\n",
    "    for i in range(len(dfPower)):\n",
    "\n",
    "        actualPower = dfPower[i]\n",
    "        voltage = dfVoltage[i]\n",
    "        timestamp = getSingleDateTime(dfTime[i])\n",
    "\n",
    "        # Expected power for the time of day\n",
    "        expectedPower = polyfit(timestamp)\n",
    "\n",
    "        # Expected max power based on volt-watt\n",
    "        maxVWPower = voltWattCurve(voltage, vwLimit) * maxPower\n",
    "\n",
    "        # CALCULATING THE AMOUNT OF OBSERVED CURTAILMENT\n",
    "        if maxVWPower < expectedPower:\n",
    "            voltArray.append(voltage)\n",
    "            timeArray.append(timestamp)\n",
    "\n",
    "            absoluteWattArray.append(actualPower)\n",
    "            relativeWattArray.append(actualPower / maxPower)\n",
    "\n",
    "    # Perform compliance count\n",
    "    complianceCount = 0\n",
    "    bufferHighVals = 0.03 * 1000\n",
    "    bufferLowVals = 0.09 * 1000\n",
    "\n",
    "    # for i in range(len(relativeWattArray)):\n",
    "    #\n",
    "    #     relativeWatt = relativeWattArray[i]\n",
    "    #     expectedWatt = voltWattCurve(voltArray[i], vwLimit)\n",
    "    #\n",
    "    #     if relativeWatt > 0.9:\n",
    "    #         if expectedWatt - bufferHighVals < relativeWatt < expectedWatt + bufferHighVals:\n",
    "    #             complianceCount += 1\n",
    "    #             successfulRelativeWattArray.append(relativeWatt)\n",
    "    #             successfulVoltArray.append(voltArray[i])\n",
    "    #\n",
    "    #     else:\n",
    "    #         if expectedWatt - bufferLowVals < relativeWatt < expectedWatt + bufferLowVals:\n",
    "    #             complianceCount += 1\n",
    "\n",
    "    return complianceCount, voltArray, timeArray, absoluteWattArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray\n",
    "\n",
    "# VOLT-WATT LIST BASED ON V3 INVERTER SETTING AND VOLTAGE INPUT\n",
    "def voltWattCurve(v, limit):\n",
    "    if v < limit:\n",
    "        return 1\n",
    "    if v < 265:\n",
    "        return (1 - 0.8 * (v - limit) / (265 - limit))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# RETURNS THE MAXIMUM ALLOWED W/VA AND TIME LIST BASED ON AN INVERTER'S VOLTAGE DATA\n",
    "def getMaxVoltWattCurve(maxPower, graphDF, vwLimit):\n",
    "    maxVoltWattTimeArray = []\n",
    "    maxVoltWattPowerArray = []\n",
    "\n",
    "    # TODO: SHOULD BE CHANGED TO A COLUMN WISE FUNCTION FOR BETTER TIME PERFORMANCE\n",
    "    for row in graphDF.iterrows():\n",
    "        voltage = row[1].voltage\n",
    "\n",
    "        maxVoltWattTimeArray.append(getSingleDateTime(row[1].ts))\n",
    "\n",
    "        maxVoltWattPowerArray.append(voltWattCurve(voltage, vwLimit) * maxPower / 1000)\n",
    "\n",
    "    return maxVoltWattTimeArray, maxVoltWattPowerArray\n",
    "\n",
    "# GO THROUGH THE COMBINED VW BEHAVIOUR DATA FOR ALL SITES \n",
    "def Overall_volt_watt_assessment(overall_volt_watt_dict, complaincePercentageLimit, bufferHighVals, bufferLowVals): #buf \n",
    "    bestVWLimit = 248\n",
    "    \n",
    "    countVW = 0\n",
    "    countNVW = 0\n",
    "    countNA = 0\n",
    "    \n",
    "    # AGGREGATE RESULTS FOR STATISTICAL ANALYSIS\n",
    "    for c_id in overall_volt_watt_dict.keys():\n",
    "        if c_id not in overall_volt_watt_dict.keys():\n",
    "            continue\n",
    "        res = Site_volt_watt_assessment(c_id, overall_volt_watt_dict[c_id], complaincePercentageLimit, bufferHighVals, bufferLowVals)\n",
    "        \n",
    "        if res is None:\n",
    "            countNA += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"NA\"].append(c_id)\n",
    "            print(\"\\n!!! NOT ENOUGH POINTS !!!\\n\")\n",
    "            \n",
    "        elif res == True:\n",
    "            countVW += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"VW\"].append(c_id)\n",
    "            print(\"\\n!!! VOLT-WATT !!!\\n\")\n",
    "            \n",
    "        elif res == False:\n",
    "            countNVW += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"NVW\"].append(c_id)\n",
    "            print(\"\\n!!! NON-VOLT-WATT !!!\\n\")\n",
    "\n",
    "        if res is None:\n",
    "            countNA += 1\n",
    "            site_id_dict[\"NA\"].append(c_id)\n",
    "            print(\"\\n!!! NOT ENOUGH POINTS !!!\\n\")\n",
    "\n",
    "        elif res == True:\n",
    "            countVW += 1\n",
    "            site_id_dict[\"VW\"].append(c_id)\n",
    "            print(\"\\n!!! VOLT-WATT !!!\\n\")\n",
    "\n",
    "        elif res == False:\n",
    "            countNVW += 1\n",
    "            site_id_dict[\"NVW\"].append(c_id)\n",
    "            print(\"\\n!!! NON-VOLT-WATT !!!\\n\")\n",
    "    \n",
    "    totalSites = countVW + countNVW\n",
    "    \n",
    "    \n",
    "    if totalSites == 0: totalSites = 1\n",
    "    print(\"FOR4 buffer: \" + str(bufferLowVals))\n",
    "    print(\"\\n\\nVolt-Watt sites: \" + str(countVW) + \" = \" + str2(countVW/totalSites*100) + \"%\")\n",
    "    print(\"NON Volt-Watt sites: \" + str(countNVW) + \" = \" + str2(countNVW/totalSites*100) + \"%\")\n",
    "    print(\"Not enough points to assess: \" + str(countNA))\n",
    "    print(\"Total sites: \" + str(countNA + totalSites))\n",
    "    \n",
    "# ROUND TO 2DP AND STRINGIFY A FLOAT\n",
    "def str2(num):\n",
    "    return str(round(num, 2))\n",
    "\n",
    "# DISPLAY BOTH THE POWER/VOLTAGE vs TIME PLOT, AS WELL AS W/VA vs VOLTAGE\n",
    "def Display_day(c_id, date, df, dc_cap_w, voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray):\n",
    "    \n",
    "    # Returns the maxmimum permitted real power output based on the inverter's voltage conditions\n",
    "    maxVoltWattTimeArray, maxVoltWattPowerArray = getMaxVoltWattCurve(dc_cap_w, df, 250)\n",
    "    \n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    plt.xticks(rotation=25)\n",
    "    ax = plt.gca()\n",
    "    xfmt = md.DateFormatter('%H:%M:%S')\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax.tick_params(axis='y', labelcolor='red')\n",
    "    lns1 = ax.plot(getDateTime(df), df.voltage, 'tomato', label='Local voltage')\n",
    "    plt.ylabel(\"Voltage (V)\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    plt.plot(maxVoltWattTimeArray,maxVoltWattPowerArray, 'limegreen')\n",
    "    plt.plot(filteredTimeArray, filteredPowerArray, 'blue')\n",
    "    \n",
    "    lns4 = ax2.plot(getDateTime(df), df.power/1000, 'skyblue', label='Real power')\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.title(\"c_id: \" + str(c_id) + \"   Date: \" + date + \"   DC cap: \" + str(dc_cap_w))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.title(\"c_id: \" + str(c_id) + \"   Date: \" + date + \"   DC cap: \" + str(dc_cap_w))\n",
    "\n",
    "    z = np.polyfit(voltArray, relativeWattArray, 1)\n",
    "\n",
    "    slope, intercept = np.polyfit(voltArray, relativeWattArray, 1)\n",
    "    \n",
    "    p = np.poly1d(z)\n",
    "    xRange = list(range(248,260))\n",
    "    \n",
    "    plt.plot(xRange,p(xRange),\"r--\")\n",
    "    \n",
    "    plt.plot(getSampleVoltages(230, 266), getWattsCurve(250), label='Best VW limit fit')\n",
    "    plt.plot(getSampleVoltages(250, 266), getWattsCurveBuffer(250, 0.05), label='Upper buffer')\n",
    "    plt.plot(getSampleVoltages(250, 266), getWattsCurveBuffer(250, -0.05), label='Lower buffer')\n",
    "\n",
    "    plt.scatter(voltArray, relativeWattArray, c=\"purple\", marker='.', label='Inverter data')\n",
    "    plt.xlabel(\"Voltage (V)\")\n",
    "    plt.ylabel(\"Power (p.u.)\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# RETURN A LIST OF NUMBERS WITHIN SPECIFIED RANGE\n",
    "def getSampleVoltages(a, b):\n",
    "    return list(range(a, b))\n",
    "\n",
    "# PRODUCES V-WATT REDUCTION CURVE FOR A SPECIFIC V-WATT LIMIT\n",
    "def getWattsCurve(vwLimit):\n",
    "    curve = []\n",
    "    vs = getSampleVoltages(230, 266)\n",
    "    for v in vs:\n",
    "        curve.append(voltWattCurve(v, vwLimit))\n",
    "    return curve\n",
    "\n",
    "# PRODUCES V-WATT REDUCTION CURVE FOR A SPECIFIC V-WATT LIMIT WITH A SPECIFIED BUFFER\n",
    "def getWattsCurveBuffer(vwLimit, buffer):\n",
    "    curve = []\n",
    "    vs = list(range(vwLimit, 266))\n",
    "    for v in vs:\n",
    "        curve.append(min([voltWattCurve(v, vwLimit) + buffer, 1]))\n",
    "    return curve\n",
    "\n",
    "def Site_volt_watt_assessment(c_id, site_volt_watt_dict, complaincePercentageLimit, bufferHighVals, bufferLowVals): #buf\n",
    "    bestComplianceCount = 0\n",
    "    bestCompliancePercentage = 0\n",
    "    bestVWLimit = None\n",
    "    bestVoltArray = None\n",
    "    bestRelativeWattArray = None\n",
    "    bestSuccessfulRelativeWattArray = None\n",
    "    bestSuccessfulVoltArray = None\n",
    "        \n",
    "    # BUFFER AND ANGLE SETTINGS FOR THE ANALYSIS\n",
    "    complainceCountLimit = 150\n",
    "    totalPointsCountLimit = 150\n",
    "    upperAngleLimit = -0.03\n",
    "    lowerAngleLimit = -0.06\n",
    "    \n",
    "    # VARIABLE TO CHECK IF THE ANALYSIS RAN OUT OF POINTS AT 256V OR BEFORE. \n",
    "    # IF AT 256V AND NO VW BEHAVIOUR IDENTIFIED THEN INCONCLUSIVE RESULT\n",
    "    notEnoughPointsV = 256\n",
    "    \n",
    "    print(\"\\n\\nc_id: \" + str(c_id))\n",
    "    for vwLimit in list(range(246,258)):\n",
    "        complianceCount, compliancePercentage, voltArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray = Determine_volt_watt_scatter_compliance(vwLimit, site_volt_watt_dict['v'], site_volt_watt_dict['p'], bufferHighVals, bufferLowVals)\n",
    "        if len(voltArray) == 0:\n",
    "            print(\"Ran out of points at VWLimit \" + str(vwLimit))\n",
    "            notEnoughPointsV = vwLimit\n",
    "            break\n",
    "        \n",
    "        # IF THE RESULT HAS HIGHER COMPLIANCE THAN PREVIOUS V THRESHOLD MEASURE, USE IT INSTEAD\n",
    "        if bestComplianceCount < complianceCount:\n",
    "            bestComplianceCount = complianceCount\n",
    "            bestVWLimit = vwLimit\n",
    "            bestTotalPoints = len(voltArray)\n",
    "            bestVoltArray = voltArray\n",
    "            bestRelativeWattArray = relativeWattArray\n",
    "            bestSuccessfulRelativeWattArray = successfulRelativeWattArray\n",
    "            bestSuccessfulVoltArray = successfulVoltArray\n",
    "            bestCompliancePercentage = compliancePercentage\n",
    "           \n",
    "    \n",
    "    if bestComplianceCount > 0:        \n",
    "        print(\"Best VWLimit: \" + str(bestVWLimit)) \n",
    "        \n",
    "    else:\n",
    "        print(\"No VWLimit results in any compliance\")\n",
    "    \n",
    "    if bestComplianceCount > 0 and bestTotalPoints > totalPointsCountLimit:\n",
    "        slope, intercept = np.polyfit(bestVoltArray, bestRelativeWattArray, 1)\n",
    "        print(\"Slope: \" + str(slope))\n",
    "        \n",
    "        \n",
    "        if bestComplianceCount > complainceCountLimit and bestCompliancePercentage > complaincePercentageLimit and lowerAngleLimit < slope and slope < upperAngleLimit:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            if notEnoughPointsV < 256:\n",
    "                return None\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# CHECKS EACH DATA POINT TO SEE IF IT FITS WITHIN THE NECESSARY BUFFER TO BE ADDED TO THE SUCCESSFUL DATAPOINT LIST\n",
    "def Determine_volt_watt_scatter_compliance(vwLimit, originalVoltArray, originalRelativeWattArray, bufferHighVals, bufferLowVals):\n",
    "    complianceCount = 0\n",
    "    successfulRelativeWattArray = []\n",
    "    successfulVoltArray = []\n",
    "\n",
    "    # FILTER DATA TO ONLY EXAMINE VALUES HIGHER THAN THE VW LIMIT (AND LOWER THAN 1000, USED AS FilterArray FUNCTION IS USED ELSEWHERE)\n",
    "    voltArray, relativeWattArray = FilterArray(originalVoltArray, originalRelativeWattArray, 1000, vwLimit)\n",
    "\n",
    "    for i in range(len(relativeWattArray)):\n",
    "\n",
    "        relativeWatt = relativeWattArray[i]\n",
    "        expectedWatt = voltWattCurve(voltArray[i], vwLimit)\n",
    "\n",
    "        # FOR HIGHER W/VA USE A SMALLER BUFFER, AS THESE VALUES ARE MORE LIKELY TO SUFFER RANDOM VARIATIONS\n",
    "        if relativeWatt > 0.9:\n",
    "            if expectedWatt - bufferHighVals < relativeWatt < expectedWatt + bufferHighVals:\n",
    "                complianceCount += 1\n",
    "                successfulRelativeWattArray.append(relativeWatt)\n",
    "                successfulVoltArray.append(voltArray[i])\n",
    "\n",
    "        # FOR LOWER W/VA USE A LARGER BUFFER, AS THESE VALUES ARE LESS LIKELY TO SUFFER RANDOM VARIATIONS\n",
    "        else:\n",
    "            if expectedWatt - bufferLowVals < relativeWatt < expectedWatt + bufferLowVals:\n",
    "                complianceCount += 1\n",
    "                successfulRelativeWattArray.append(relativeWatt)\n",
    "                successfulVoltArray.append(voltArray[i])\n",
    "\n",
    "    compliancePercentage = 0\n",
    "    if len(voltArray) > 0:\n",
    "        compliancePercentage = complianceCount/len(voltArray)\n",
    "    return complianceCount, compliancePercentage, voltArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af76a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vwatt_curtailment(is_clear_sky_day):\n",
    "    if not is_clear_sky_day:\n",
    "        vwatt_response = 'WORK IN PROGRESS'\n",
    "        vwatt_curt_energy = 'WORK IN PROGRESS'\n",
    "    else:\n",
    "        vwatt_response = 'WORK IN PROGRESS'\n",
    "        vwatt_curt_energy = 'WORK IN PROGRESS'\n",
    "    return vwatt_response, vwatt_curt_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a812d60",
   "metadata": {},
   "source": [
    "# MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e9fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_general_files(file_path):\n",
    "    circuit_details = pd.read_csv(file_path + r\"\\unsw_20190701_circuit_details.csv\")\n",
    "    site_details = pd.read_csv (file_path + r\"\\unsw_20190701_site_details.csv\")\n",
    "    site_details = site_details.merge(circuit_details, left_on = 'site_id', right_on = 'site_id')\n",
    "    unique_cids = pd.read_csv(file_path + r\"\\UniqueCids.csv\", index_col = 0)\n",
    "    return site_details, unique_cids\n",
    "\n",
    "def input_monthly_files(file_path, data_date_idx):\n",
    "    data_path = file_path + r\"\\processed_unsw_\" + data_date_idx + '_data_raw.csv'\n",
    "    data = pd.read_csv(data_path, index_col = 1)\n",
    "\n",
    "    # Convert timestamp to local Adelaide time\n",
    "    data.index = pd.to_datetime(data.index) # convert index from object type to datetime\n",
    "    Adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    data.index = data.index.tz_localize(pytz.utc).tz_convert(Adelaide_local_time) # convert utc to local adelaide time\n",
    "    data.index.rename('Timestamp', inplace = True)\n",
    "\n",
    "    # Load GHI data\n",
    "    ghi_date_idx = data_date_idx[0:4] + '_' + data_date_idx[4:]\n",
    "    ghi_path = file_path + r\"\\sl_023034_\" + ghi_date_idx +'.txt'\n",
    "    ghi = pd.read_csv (ghi_path) \n",
    "\n",
    "    ghi['timestamp'] = pd.to_datetime(pd.DataFrame ({'year' : ghi['Year Month Day Hours Minutes in YYYY'].values, \n",
    "                                                    'month' : ghi['MM'], \n",
    "                                                    'day' : ghi['DD'], \n",
    "                                                   'hour' : ghi['HH24'], \n",
    "                                                   'minute' : ghi['MI format in Local standard time']}))\n",
    "    ghi.set_index('timestamp', inplace = True)\n",
    "    # Deal with the space characters (ghi is in object/string form at the moment)\n",
    "    ghi['Mean global irradiance (over 1 minute) in W/sq m'] = [float(ghi_t) if ghi_t.count(' ')<= 3 else np.nan for ghi_t in ghi['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    return data, ghi\n",
    "\n",
    "def filter_date(data, date):\n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    data = data[data.index.date == date_dt] #focus only on the date\n",
    "    return data\n",
    "\n",
    "def summarize_result_into_dataframe(c_id, date, energy_generated, is_clear_sky_day, tripping_response, tripping_curt_energy, vvar_response, vvar_curt_energy, vwatt_response, vwatt_curt_energy):\n",
    "    summary = pd.DataFrame({\n",
    "        'c_id' : [c_id],\n",
    "        'date' : [date],\n",
    "        'energy generated (kWh)' : [energy_generated],\n",
    "        'clear sky day': [is_clear_sky_day],\n",
    "        'tripping response' : [tripping_response],\n",
    "        'tripping curtailment (kWh)' : [tripping_curt_energy],\n",
    "        'V-VAr response' : [vvar_response],\n",
    "        'V-VAr curtailment (kWh)' : [vvar_curt_energy],\n",
    "        'V-Watt response' : [vwatt_response],\n",
    "        'V-Watt curtailment (kWh)' : [vwatt_curt_energy]\n",
    "    })\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d4152",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f0106d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 76-77: truncated \\UXXXXXXXX escape (1046678838.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [15]\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 76-77: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "site_details, unique_cids, data, ghi = input_general_files(file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\")\n",
    "for site in site_list:\n",
    "    for month in month_list:\n",
    "    data, ghi = input_monthly_file(file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\")\n",
    "    organized_site_file = site_organize(c_id_idx, site_details, data, unique_cids)\n",
    "        for date in month:\n",
    "            is_clear_sky_day = check_clear_sky(date)\n",
    "            tripping_response, tripping_curtailed_energy = check_tripping_curtailment(c_id, date)\n",
    "            vvar_response, vvar_curtailed_energy = check_vvar_curtailment(c_id, date)\n",
    "            vwatt_response, vwatt_curtailed_energy = check_vwatt_curtailment(c_id, date)\n",
    "summarize_result_into_dataframe()\n",
    "show_summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10826e15",
   "metadata": {},
   "source": [
    "# TRIAL FOR ONLY 1 SITE AND # 1 DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc91ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\"\n",
    "site_details, unique_cids= input_general_files(file_path)\n",
    "\n",
    "month = '201909'\n",
    "#c_id = 1018350709 #this is row 436 in the site id, and having the biggest vvar curtailment on 2019-07-20\n",
    "c_id = 1317822057 #this is site with tripping response on 2019-09-03\n",
    "data, ghi = input_monthly_files(file_path, month)\n",
    "data_site, ac_cap, dc_cap, eff_system, inverter = site_organize(c_id, site_details, data, unique_cids)\n",
    "\n",
    "#date = '2019-07-21' #non clear sky day\n",
    "date = '2019-09-03' #non clear sky day, good sample for tripping\n",
    "#date = '2019-07-20' #clear sky day, good sample for vvar\n",
    "#date = '2020-01-25' #random\n",
    "\n",
    "data_site = filter_date(data_site, date)\n",
    "is_clear_sky_day = check_clear_sky_day(date)\n",
    "energy_generated = check_energy_generated(data_site, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e237e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIPPING\n",
    "tripping_response, tripping_curt_energy, estimation_method = check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date)\n",
    "\n",
    "#VVAR, VWATT, SUMMARY\n",
    "vvar_response, vvar_curt_energy = check_vvar_curtailment(c_id, date, data_site, ghi, ac_cap, dc_cap, eff_system, is_clear_sky_day)\n",
    "vwatt_response, vwatt_curt_energy = check_vwatt_curtailment(is_clear_sky_day)\n",
    "summary = summarize_result_into_dataframe(c_id, date, energy_generated, is_clear_sky_day, tripping_response, tripping_curt_energy, vvar_response, vvar_curt_energy, vwatt_response, vwatt_curt_energy)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_clear_sky_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067058c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_site['power'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eba2f",
   "metadata": {},
   "source": [
    "# BUGS LIST\n",
    "## IMPORTANT\n",
    "1. Naomi's calculation of generated energy (kWh) daily is different from Baran's (probably dataset issue? cleaning issue?)\n",
    "2. Tim's polyfit sometime underestimate the expected generation without curtailment, so the curtailed energy is always negative\n",
    "3. No sunrise and sunset data from Baran's data, while it is used in Naomi's code\n",
    "4. Trippping curtailment calculated using Baran's dataset (processed_unsw_201909_data_raw) is different from using Naomi's dataset (which I don't have) so I can't test\n",
    "5. How to make sure that estimated tripping_curt_energy doesn't include VVAr and VWatt?\n",
    "6. Incomplete dataset? Only midnight data, eg on 2020-03-01, c_id = 1317822057\n",
    "7. Tripping case not detected by Naomi's script? eg on 2020-03-02, c_id = 1317822057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cadfd",
   "metadata": {},
   "source": [
    "# BUG 6. INCOMPLETE DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f4d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_id</th>\n",
       "      <th>energy</th>\n",
       "      <th>power</th>\n",
       "      <th>reactive_power</th>\n",
       "      <th>voltage</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:33:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6148</td>\n",
       "      <td>245.3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:36:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6100</td>\n",
       "      <td>243.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:37:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6013</td>\n",
       "      <td>242.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:38:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5996</td>\n",
       "      <td>242.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:39:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5976</td>\n",
       "      <td>242.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:41:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>5990</td>\n",
       "      <td>242.3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:42:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-99</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>5986</td>\n",
       "      <td>242.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:43:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5975</td>\n",
       "      <td>241.5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:44:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-99</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>5965</td>\n",
       "      <td>241.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:46:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6083</td>\n",
       "      <td>244.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:47:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6078</td>\n",
       "      <td>244.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:30:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6143</td>\n",
       "      <td>245.6</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:31:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-103</td>\n",
       "      <td>-1.716667</td>\n",
       "      <td>6141</td>\n",
       "      <td>245.7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:32:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6145</td>\n",
       "      <td>245.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:48:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6095</td>\n",
       "      <td>244.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:49:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6091</td>\n",
       "      <td>243.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:51:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6068</td>\n",
       "      <td>243.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:52:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>6061</td>\n",
       "      <td>243.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:53:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>6028</td>\n",
       "      <td>243.7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:55:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-98</td>\n",
       "      <td>-1.633333</td>\n",
       "      <td>5875</td>\n",
       "      <td>240.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:56:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-97</td>\n",
       "      <td>-1.616667</td>\n",
       "      <td>5863</td>\n",
       "      <td>239.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:58:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-98</td>\n",
       "      <td>-1.633333</td>\n",
       "      <td>5853</td>\n",
       "      <td>238.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 c_id  energy     power  reactive_power  \\\n",
       "Timestamp                                                                 \n",
       "2020-03-01 23:33:55+10:30  1317822057    -102 -1.700000            6148   \n",
       "2020-03-01 23:36:55+10:30  1317822057    -102 -1.700000            6100   \n",
       "2020-03-01 23:37:55+10:30  1317822057    -101 -1.683333            6013   \n",
       "2020-03-01 23:38:55+10:30  1317822057    -101 -1.683333            5996   \n",
       "2020-03-01 23:39:55+10:30  1317822057    -101 -1.683333            5976   \n",
       "2020-03-01 23:41:55+10:30  1317822057    -100 -1.666667            5990   \n",
       "2020-03-01 23:42:55+10:30  1317822057     -99 -1.650000            5986   \n",
       "2020-03-01 23:43:55+10:30  1317822057    -101 -1.683333            5975   \n",
       "2020-03-01 23:44:55+10:30  1317822057     -99 -1.650000            5965   \n",
       "2020-03-01 23:46:55+10:30  1317822057    -101 -1.683333            6083   \n",
       "2020-03-01 23:47:55+10:30  1317822057    -101 -1.683333            6078   \n",
       "2020-03-01 23:30:55+10:30  1317822057    -102 -1.700000            6143   \n",
       "2020-03-01 23:31:55+10:30  1317822057    -103 -1.716667            6141   \n",
       "2020-03-01 23:32:55+10:30  1317822057    -102 -1.700000            6145   \n",
       "2020-03-01 23:48:55+10:30  1317822057    -101 -1.683333            6095   \n",
       "2020-03-01 23:49:55+10:30  1317822057    -101 -1.683333            6091   \n",
       "2020-03-01 23:51:55+10:30  1317822057    -101 -1.683333            6068   \n",
       "2020-03-01 23:52:55+10:30  1317822057    -100 -1.666667            6061   \n",
       "2020-03-01 23:53:55+10:30  1317822057    -100 -1.666667            6028   \n",
       "2020-03-01 23:55:55+10:30  1317822057     -98 -1.633333            5875   \n",
       "2020-03-01 23:56:55+10:30  1317822057     -97 -1.616667            5863   \n",
       "2020-03-01 23:58:55+10:30  1317822057     -98 -1.633333            5853   \n",
       "\n",
       "                           voltage  duration  \n",
       "Timestamp                                     \n",
       "2020-03-01 23:33:55+10:30    245.3        60  \n",
       "2020-03-01 23:36:55+10:30    243.0        60  \n",
       "2020-03-01 23:37:55+10:30    242.8        60  \n",
       "2020-03-01 23:38:55+10:30    242.0        60  \n",
       "2020-03-01 23:39:55+10:30    242.2        60  \n",
       "2020-03-01 23:41:55+10:30    242.3        60  \n",
       "2020-03-01 23:42:55+10:30    242.2        60  \n",
       "2020-03-01 23:43:55+10:30    241.5        60  \n",
       "2020-03-01 23:44:55+10:30    241.9        60  \n",
       "2020-03-01 23:46:55+10:30    244.2        60  \n",
       "2020-03-01 23:47:55+10:30    244.4        60  \n",
       "2020-03-01 23:30:55+10:30    245.6        60  \n",
       "2020-03-01 23:31:55+10:30    245.7        60  \n",
       "2020-03-01 23:32:55+10:30    245.4        60  \n",
       "2020-03-01 23:48:55+10:30    244.4        60  \n",
       "2020-03-01 23:49:55+10:30    243.8        60  \n",
       "2020-03-01 23:51:55+10:30    243.9        60  \n",
       "2020-03-01 23:52:55+10:30    243.8        60  \n",
       "2020-03-01 23:53:55+10:30    243.7        60  \n",
       "2020-03-01 23:55:55+10:30    240.2        60  \n",
       "2020-03-01 23:56:55+10:30    239.8        60  \n",
       "2020-03-01 23:58:55+10:30    238.9        60  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_date_idx = '202003'\n",
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\"\n",
    "data_path = file_path + r\"\\processed_unsw_\" + data_date_idx + '_data_raw.csv'\n",
    "data = pd.read_csv(data_path, index_col = 1)\n",
    "\n",
    "# Convert timestamp to local Adelaide time\n",
    "data.index = pd.to_datetime(data.index) # convert index from object type to datetime\n",
    "Adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "data.index = data.index.tz_localize(pytz.utc).tz_convert(Adelaide_local_time) # convert utc to local adelaide time\n",
    "data.index.rename('Timestamp', inplace = True)\n",
    "\n",
    "date = '2020-03-01'\n",
    "date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "data = data[data.index.date == date_dt] #focus only on the date\n",
    "\n",
    "data [ data['c_id'] == 1317822057]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2076a9",
   "metadata": {},
   "source": [
    "# MESSY BELOW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
