{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfde0e30",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY AND PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c632c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pytz #for timezone calculation\n",
    "import math\n",
    "import matplotlib.dates as md\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "%matplotlib qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177c4fd",
   "metadata": {},
   "source": [
    "# SET GLOBAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686a2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Global parameters for fonts & sizes =================\n",
    "font_size = 10\n",
    "rc={'font.size': font_size, 'axes.labelsize': font_size, 'legend.fontsize': font_size, \n",
    "    'axes.titlesize': font_size, 'xtick.labelsize': font_size, 'ytick.labelsize': font_size}\n",
    "plt.rcParams.update(**rc)\n",
    "plt.rc('font', weight='bold')\n",
    " \n",
    "# For label titles\n",
    "fontdict={'fontsize': font_size, 'fontweight' : 'bold'}\n",
    "# can add in above dictionary: 'verticalalignment': 'baseline' \n",
    "\n",
    "style = 'ggplot' # choose a style from the above options\n",
    "plt.style.use(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713a410",
   "metadata": {},
   "source": [
    "# ENERGY GENERATED CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc2e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_energy_generated(data_site, date):\n",
    "    \"\"\"Get the amount of energy generated in a certain site in a certain day, unit kWh.\n",
    "\n",
    "    Args:\n",
    "        data_site (df): Cleaned D-PV time-series data, output of site_orgaize function\n",
    "        date (str): date in focus\n",
    "\n",
    "    Returns:\n",
    "        energy_generated (float): Single value of the total energy generated in that day\n",
    "    \"\"\"\n",
    "    \n",
    "    #sh_idx = (data_site.index.hour>= 7) & (data_site.index.hour <= 17)\n",
    "    #hour filter should not be necessary since outside of that hour, the power is zero anyway.\n",
    "    \n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    date_idx = data_site.index.date == date_dt\n",
    "    energy_generated = data_site.loc[date_idx, 'power'].resample('h').mean().sum()/1000\n",
    "    return energy_generated\n",
    "\n",
    "def check_energy_expected_generated(data_site, date):\n",
    "    \"\"\"Get the amount of expected energy generated in a certain site in a certain day, unit kWh.\n",
    "\n",
    "    Args:\n",
    "        data_site (df): Cleaned D-PV time-series data, with power_expected column\n",
    "        date (str): date in focus\n",
    "\n",
    "    Returns:\n",
    "        energy_generated_expected (float): Single value of the total expected energy generated in that day\n",
    "    \"\"\"\n",
    "    \n",
    "    #sh_idx = (data_site.index.hour>= 7) & (data_site.index.hour <= 17)\n",
    "    #hour filter should not be necessary since outside of that hour, the power is zero anyway.\n",
    "    \n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    date_idx = data_site.index.date == date_dt\n",
    "    energy_generated_expected = data_site.loc[date_idx, 'power_expected'].resample('h').mean().sum()/1000\n",
    "    return energy_generated_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06afb5",
   "metadata": {},
   "source": [
    "# CHECK CLEAR SKY DAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab69dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clear_sky_day(date):\n",
    "    \"\"\"Check whether a certain date is a clear sky day based on the ghi data of that day. Needs ghi data.\n",
    "\n",
    "    Args:\n",
    "    date (str): dat ein YYYYMMDD format\n",
    "\n",
    "    Returns:\n",
    "    clear_sky_day (bool): is it a clear sky day or not\n",
    "    \n",
    "    Funcitons needed:\n",
    "    - Get_timestamp_date_string\n",
    "    - Separate_ghi_data\n",
    "    - Detect_clear_sky_day\n",
    "\n",
    "    Originally written by Tim for VWatt Curtailment Calculation.\n",
    "    \"\"\"\n",
    "    dateFile = date[:4]+'_'+ date[5:7]\n",
    "    ghi = pd.read_csv(file_path +'/sl_023034_' + dateFile + \".txt\")\n",
    "    timestamp_date_string = Get_timestamp_date_string(dateFile)\n",
    "    separated_ghi_data = Separate_ghi_data(timestamp_date_string, ghi)\n",
    "    ghi_df = separated_ghi_data[date]\n",
    "    res, average_delta_y = Detect_clear_sky_day(ghi_df, 530)\n",
    "\n",
    "    if res:\n",
    "        #clear_sky_days.append(date)\n",
    "        #overall_clear_sky_days_dict[dateFile].append(date)\n",
    "        is_clear_sky_day = True\n",
    "    else:\n",
    "        is_clear_sky_day = False\n",
    "    return is_clear_sky_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba8379",
   "metadata": {},
   "source": [
    "# TRIPPING CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ee0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util #customized module to calculate the first derivative of the power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89428a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting cumulative count of 0 with reset on 1\n",
    "def rcount(a):\n",
    "    without_reset = (a == 0).cumsum()\n",
    "    reset_at = (a == 1)\n",
    "    overcount = np.maximum.accumulate(without_reset * reset_at)\n",
    "    result = without_reset - overcount\n",
    "    return result\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ed6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date):\n",
    "    \"\"\"Check the tripping response and the amount of energy curtailed bcs of tripping in kWh for certain data and site.\n",
    "\n",
    "    Args:\n",
    "    is_clear_sky_day (bool) : is the certain date a clear sky date or not\n",
    "    c_id (str): c_id\n",
    "    data_site (df): Cleaned D-PV time-series data, output of site_organize function\n",
    "    unique_cids (df): Unique c_id and site_id data\n",
    "    ac_cap (float): Inverter capacity in watt\n",
    "    site_details (df): Site data from unsw_20190701_site_details.csv and unsw_20190701_circuit_details.csv\n",
    "    date (str): Date in YYYYMMDD\n",
    "\n",
    "    Returns:\n",
    "    tripping_response (str): Yes or None, whether there is a tripping response by the inverter\n",
    "    tripping_curt_energy (float): The amount of energy curtailed in that date and site in kWh\n",
    "    estimation_method (str): Linear or polyfit. Polyfit is only used when it is a clear sky day and the fit passes some \n",
    "                             criterias. Otherwise always use linear.\n",
    "    \n",
    "\n",
    "    Originally written by Naomi. Some parts are modified to be compatible for SolA dataset input and implementation\n",
    "    together with VVAr and VWatt script.\n",
    "    \n",
    "    TODO: Reconsider whether the cleaning part of Naomi's script is still necessary in Baran's SolA dataset.\n",
    "    TODO: Compare between Naomi's and Tim's polyfit algorithm, pick the better one, upgrade it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # In Naomi's script, the first step is to clean the data (remove duplicates, convert to numeric, etc). However, since the inputted data here is already cleaned, we directly use the data without further cleaning process.\n",
    "    # Approx capacity factor value considered to be 'zero', e.g. less than 1% CF is zero.\n",
    "\n",
    "    # List of connection types for filtering\n",
    "    pv_list = ['pv_site_net', 'pv_site', 'pv_inverter_net']\n",
    "    CF_ZERO_APPROX = 0.01\n",
    "\n",
    "    # Approx cf derivative at which considered to be 'ramp'. That is, for at least a 10% change in capacity factor (ABSOLUTE!) expect to be ramping up or down.\n",
    "    # Note, only applied 'next to' zeros. So should not capture shading effects.\n",
    "    FIRST_DERIV_FALL_LIMIT = -0.05\n",
    "    FIRST_DERIV_INCREASE_LIMIT = 0.05\n",
    "    # For missing data check\n",
    "    ALLOWED_MISSING_DATA_PERCENTAGE = 0.05\n",
    "    # Average percentage of capacity at which a system must operate over the course of the day in order to be included in analysis\n",
    "    VERY_LOW_OUTPUT_AVE_PERCENTAGE_CAPACITY = 0.05\n",
    "\n",
    "\n",
    "    # Get data\n",
    "    unaltered_data = pd.DataFrame({\n",
    "        'c_id' : c_id,\n",
    "        'v' : data_site['voltage'],\n",
    "        'd' : data_site['duration'],\n",
    "        'site_id' : unique_cids.loc[unique_cids['c_id'] == c_id, 'site_id'].iloc[0],\n",
    "        'e' : data_site['energy'],\n",
    "        'con_type' : site_details.loc[site_details['c_id'] == c_id, 'con_type'].iloc[0],\n",
    "        'first_ac' : ac_cap/1000, #IDK what is this, but let's assume it is just the same with sum_ac, which is the inverter ac capacity\n",
    "        'power_kW' : data_site['power']/1000,\n",
    "        'reactive_power' : data_site['reactive_power'],\n",
    "        'clean' : 'cleaned',\n",
    "        'manufacturer' : site_details.loc[site_details['c_id'] == c_id, 'inverter_manufacturer'].iloc[0],\n",
    "        'model' : site_details.loc[site_details['c_id'] == c_id, 'inverter_model'].iloc[0],\n",
    "        'sum_ac' : ac_cap/1000,\n",
    "        'time_offset' : float(\"nan\")\n",
    "    }, index = data_site.index)\n",
    "    unaltered_data.index.rename('ts', inplace=True) \n",
    "\n",
    "    # rename energy column\n",
    "    unaltered_data = unaltered_data.rename(columns = {'e' : 'energy', 'd':'duration', 'sum_ac':'ac'})\n",
    "    # filter for clean\n",
    "    unaltered_data = unaltered_data[unaltered_data['clean']=='cleaned']\n",
    "    # Attempt to fix issues by sorting the index at the beginning\n",
    "    unaltered_data = unaltered_data.sort_index()\n",
    "    # Change name to t_stamp\n",
    "    unaltered_data.index.name = 't_stamp'\n",
    "    # Add time by seconds from start of the day\n",
    "    unaltered_data['hrs'] = unaltered_data.index.hour\n",
    "    unaltered_data['min'] = unaltered_data.index.minute\n",
    "    unaltered_data['sec'] = unaltered_data.index.second\n",
    "    unaltered_data['time_in_seconds'] = unaltered_data['hrs'] * 60 * 60 + unaltered_data['min'] * 60 + unaltered_data['sec']\n",
    "\n",
    "    # Get list of time_intervals\n",
    "    time_interval_list = unaltered_data['duration'].drop_duplicates().tolist()\n",
    "\n",
    "    # ********************* Further data cleaning [START] *********************\n",
    "    # Check for missing data issues\n",
    "    # TODO - this may not longer work since the Solar Analytics data can contain a mix of 60s and 5s data\n",
    "    # Flag sites with too much missing data (based on threshold), need to also keep the duration\n",
    "    missing_data_df = pd.DataFrame({'num_data_pts': unaltered_data.groupby('c_id')['energy'].count(), 'duration': unaltered_data.groupby('c_id')['duration'].first()}).reset_index()\n",
    "    # We now have two possible time intervals: 30s or 60s.\n",
    "    # Therefore, we need to run twice?\n",
    "    for time_interval in time_interval_list:\n",
    "        # Expected number of time periods\n",
    "        num_time_periods = 24 * 60 * (60 / time_interval)\n",
    "        # Get the minimum number of data points required in order to have enough data (i.e. not lots of missing data)\n",
    "        missing_data_threshold = num_time_periods * (1 - ALLOWED_MISSING_DATA_PERCENTAGE)\n",
    "        missing_data_df['missing_data_flag'] = np.nan\n",
    "        missing_data_df.loc[(missing_data_df['num_data_pts'] <= missing_data_threshold) & missing_data_df['duration']==time_interval , 'missing_data_flag'] = 1.0\n",
    "    # Merge information about missing data back onto time series df\n",
    "    unaltered_data = unaltered_data.reset_index().merge(missing_data_df, on='c_id', how='left').set_index('t_stamp')\n",
    "    # Filter unaltered data for only sites with sufficient data points\n",
    "    unaltered_data = unaltered_data[unaltered_data['missing_data_flag'] != 1.0]\n",
    "\n",
    "    # Filter for PV only\n",
    "    unaltered_data = unaltered_data[unaltered_data['con_type'].isin(pv_list)]\n",
    "\n",
    "    # First fix duratoin_x / duration_y problem. Not sure where it is coming from\n",
    "    # I don't know why, but there is duration_x and duration_y. Drop one and rename the other\n",
    "    unaltered_data = unaltered_data.drop(['duration_x'], axis=1)\n",
    "    unaltered_data = unaltered_data.rename(columns={'duration_y': 'duration'})\n",
    "\n",
    "    # Open _circuit_details_for_editing.csv file for sunrise/set times\n",
    "    assist_df = pd.DataFrame({\n",
    "        'c_id' : [c_id],\n",
    "        'energy_day' : [float(\"nan\")],\n",
    "        'energy_night' : [float(\"nan\")],\n",
    "        'con_type' : [site_details.loc[site_details['c_id'] == c_id, 'con_type'].iloc[0]],\n",
    "        'sunrise' : ['2/09/2019  6:06:06'], #later need to be edited\n",
    "        'sunset' : ['2/09/2019  19:28:19'], #later need to be edited\n",
    "        'min_power' : [data_site['power'].min()/1000],\n",
    "        'max_power' : [data_site['power'].max()/1000],\n",
    "        'polarity' : [site_details.loc[site_details['c_id'] == c_id, 'polarity'].iloc[0]],\n",
    "        'frac_day' : [float('nan')],\n",
    "        'old_con_type' : [float('nan')],\n",
    "        'con_type_change' : [float('nan')],\n",
    "        'site_id' : [unique_cids.loc[unique_cids['c_id'] == c_id, 'site_id'].iloc[0]]\n",
    "    })\n",
    "\n",
    "    # Check for PV sites with very low output and remove them\n",
    "    get_site_ac_df = unaltered_data[['site_id', 'first_ac', 'ac']]\n",
    "    get_site_ac_df = get_site_ac_df.drop_duplicates(subset='site_id')\n",
    "    # merge keeping only the site_ids in the time series df.\n",
    "    assist_df = assist_df.merge(get_site_ac_df, left_on='site_id', right_on='site_id', how='right')\n",
    "\n",
    "    # Check whether c_ids operated at less than an average of 5% capacity\n",
    "    # Compare using max power output compared with first_ac.\n",
    "    max_p_df = pd.DataFrame({'max_p_kW': unaltered_data.groupby('c_id')['power_kW'].max(), 'first_ac' : unaltered_data.groupby('c_id')['first_ac'].first()})\n",
    "    max_p_df['low_output_flag'] = np.nan\n",
    "    max_p_df.loc[max_p_df['max_p_kW'] < VERY_LOW_OUTPUT_AVE_PERCENTAGE_CAPACITY * max_p_df['first_ac'] , 'low_output_flag'] = 1\n",
    "    # Copy c_ids to a column (from index)\n",
    "    max_p_df['c_id'] = max_p_df.index\n",
    "    # Get list of c_ids to be excluded\n",
    "    c_ids_to_WITHOUT_low_output = max_p_df[max_p_df['low_output_flag'] != 1]\n",
    "    c_ids_to_WITHOUT_low_output = c_ids_to_WITHOUT_low_output['c_id'].tolist()\n",
    "\n",
    "    # Only keep sites that have enough output\n",
    "    unaltered_data = unaltered_data[unaltered_data['c_id'].isin(c_ids_to_WITHOUT_low_output)]\n",
    "    # ********************* Further data cleaning [END] *********************\n",
    "\n",
    "    # Get assist_df with c_id as index\n",
    "    assist_df_c_id = assist_df.set_index('c_id')\n",
    "\n",
    "    # Get c_id list\n",
    "    c_id_list = unaltered_data['c_id'].drop_duplicates().tolist()\n",
    "    # Set up output_df\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df.index.name = 't_stamp'\n",
    "\n",
    "    # Get data for c_id\n",
    "    data = unaltered_data[unaltered_data['c_id'] == c_id]\n",
    "\n",
    "    # Filter for sunshine hours - NOTE must add an hour to sunrise / subtract and hour from sunset since Nick's code did the opposite, but I actually would like correct sunrise/set\n",
    "    # Sunrise\n",
    "    sun_rise = assist_df_c_id.loc[c_id,'sunrise']\n",
    "    sun_rise = pd.to_datetime(sun_rise)\n",
    "    sun_rise_hour = sun_rise.hour\n",
    "    sun_rise_min = sun_rise.minute\n",
    "    if sun_rise_min <10 :\n",
    "        sun_rise_min = '0' + str(sun_rise_min)\n",
    "    else:\n",
    "        sun_rise_min = str(sun_rise_min)\n",
    "    sun_rise_for_filter = str(sun_rise_hour + 1) + ':' + sun_rise_min + ':' + str(00)\n",
    "\n",
    "    # Sunset\n",
    "    sun_set = assist_df_c_id.loc[c_id,'sunset']\n",
    "    sun_set = pd.to_datetime(sun_set)\n",
    "    sun_set_hour = sun_set.hour\n",
    "    sun_set_min = sun_set.minute\n",
    "    if sun_set_min <10 :\n",
    "        sun_set_min = '0' + str(sun_set_min)\n",
    "    else:\n",
    "        sun_set_min = str(sun_set_min)\n",
    "    sun_set_for_filter = str(sun_set_hour - 1) + ':' + sun_set_min + ':' + str(00)\n",
    "\n",
    "    data = data.between_time(sun_rise_for_filter, sun_set_for_filter)\n",
    "\n",
    "    # Calc CF\n",
    "    data['unaltered_cf'] = data['power_kW'] / data['ac']\n",
    "    # Flag approximate zeroes (cf < CF_ZERO_APPROX)\n",
    "    data['unaltered_zero_flag'] = 0\n",
    "    data.loc[data['unaltered_cf'] <= CF_ZERO_APPROX, 'unaltered_zero_flag'] = 1\n",
    "    data['non_zero_flag_count'] = data['unaltered_zero_flag']\n",
    "\n",
    "    # Remove cases where 'blip' occurs. e.g. above zero but only for a max of 2 time intervals.\n",
    "    # TODO - may be better to remove this step since we are also looking at non-clear sky days!\n",
    "    # First, count the non zeros\n",
    "    a = data['non_zero_flag_count']\n",
    "    # Now remove from data\n",
    "    data = data.drop(['non_zero_flag_count'], axis=1)\n",
    "    # Count non zeros\n",
    "    result = rcount(a)\n",
    "    # Add onto data\n",
    "    data = pd.concat([data,result], axis=1)\n",
    "\n",
    "    # Copy the unaltered zero flag - we will then remove the 'blips' from it.\n",
    "    data['zero_flag'] = data['unaltered_zero_flag']\n",
    "\n",
    "    # Case where single point of 'non zero' before returning to zero\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-1) == 0) & (data['non_zero_flag_count'] == 1),'zero_flag'] = 1\n",
    "\n",
    "    # If the non zero flag count in this row is 2 and in the next row is zero, then set zero_flag to 1 (i.e. remove 'blip')\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-1) == 0) & (data['non_zero_flag_count'] == 2),'zero_flag'] = 1\n",
    "    data.loc[(data['non_zero_flag_count'].shift(-2) == 0) & (data['non_zero_flag_count'].shift(-1) == 2),'zero_flag'] = 1\n",
    "\n",
    "    # Set CF to zero where zero flag occurs\n",
    "    data['cf'] = data['unaltered_cf']\n",
    "    data.loc[data['zero_flag'] == 1,'cf'] = 0\n",
    "\n",
    "    # Get first derivative of cf\n",
    "    data = util.calculate_first_derivative_of_variable(data, 'cf')\n",
    "\n",
    "    # --------------------------------- Reductions immediately before zero\n",
    "    # Falling dramatically before zeros\n",
    "    data['start_deriv_flag'] = 0\n",
    "    # Just get the first instance of ramp down\n",
    "    # e.g. Times where zero flag (t+1) = 1, zero flag (t) <>1 and cf_first_deriv < limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 1) & (data['zero_flag'] == 0) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    # Dealing with 'soft' disconnect\n",
    "    # Next interval is zero flagged, current value is greater than 'zero' limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 1) & (data['cf'] > CF_ZERO_APPROX),'start_deriv_flag'] = 1\n",
    "\n",
    "    # Get the next instance of ramp down (well, previous) - repeat four times. Effectively means you can capture periods in which power falls over 5 time intervals (including initial one captured above)\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "    data.loc[(data['start_deriv_flag'].shift(-1) == 1) & (data['cf_first_deriv'] < FIRST_DERIV_FALL_LIMIT),'start_deriv_flag'] = 1\n",
    "\n",
    "    # --------------------------------- Increases immediately after zero\n",
    "    # Increasing dramatically after zeros\n",
    "    data['end_deriv_flag'] = 0\n",
    "    # Just get the first instance of ramp up\n",
    "    # e.g. Times where zero flag (t) = 1, zero flag (t+1) <>1 and cf_first_deriv > limit\n",
    "    data.loc[(data['zero_flag'].shift(-1) == 0) & (data['zero_flag'] == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    # Dealing with 'soft' restarts.\n",
    "    # Previous value was zero flagged, current value is greater than the 'zero' limit\n",
    "    data.loc[(data['zero_flag'].shift(+1) == 1) & (data['cf'] > CF_ZERO_APPROX),'end_deriv_flag'] = 1\n",
    "\n",
    "    # Get next instances (x8 as slower ramp up potentially)\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['cf_first_deriv'] > FIRST_DERIV_INCREASE_LIMIT),'end_deriv_flag'] = 1\n",
    "\n",
    "    # --------------------------------- Get 'ramp' start and end points\n",
    "    # Get start points\n",
    "    data['start_pts'] = 0\n",
    "    # Case where 'start_derive_flag' is zero in previous interval (t-1), and one in current interval (t)\n",
    "    data.loc[(data['start_deriv_flag'].shift(+1) == 0) & (data['start_deriv_flag'] == 1),'start_pts'] = 1\n",
    "\n",
    "    # Get end points\n",
    "    data['end_pts'] = 0\n",
    "    # Case where 'end_deriv_flag' is 1 in previous interval (t-1), and 0 in current interval (t)\n",
    "    data.loc[(data['end_deriv_flag'].shift(+1) == 1) & (data['end_deriv_flag'] == 0),'end_pts'] = 1\n",
    "\n",
    "    # --------------------------------- Add some things to data that we need\n",
    "    # Check that the first 'start point' occurs before the first 'end point'. If not, then delete the first 'end point'\n",
    "    # Else in the early hours of the day as the generation goes from zero to non zero it looks like 'soft start'\n",
    "    # Check three times over (TODO would be better to do this as a while somehow... maybe so that it stops once the first_end_point stops changing?)\n",
    "    try:\n",
    "        for i in range(0,3):\n",
    "            first_end_point = data[data['end_pts']==1]\n",
    "            first_end_point = first_end_point['time_in_seconds'].iloc[0]\n",
    "            # Find first start point\n",
    "            first_start_point = data[data['start_pts']==1]\n",
    "            first_start_point = first_start_point['time_in_seconds'].iloc[0]\n",
    "            # Check that the first start point occurs after the first end point.\n",
    "            if first_end_point < first_start_point:\n",
    "                data.loc[data['time_in_seconds']==first_end_point, 'end_pts'] = 0\n",
    "    except:\n",
    "        x = 1\n",
    "\n",
    "    # Identify the start and end pt number (so they can be matched to each other)\n",
    "    data['start_cumsum'] = data['start_pts'].cumsum()\n",
    "    data['end_cumsum'] = data['end_pts'].cumsum()\n",
    "\n",
    "    # Get cumulative zeros between 'start' and 'end' pts\n",
    "    data['count_during_period'] = data['start_pts']\n",
    "    data.loc[data['end_pts'] == 1,'count_during_period'] =1\n",
    "    a = data['count_during_period']\n",
    "\n",
    "    # Copy as a renamed column then remove ^ name from data\n",
    "    data['start_end_combined'] = data['count_during_period']\n",
    "    # NOTE - possible issue here? Dropped column but then used later. NO - all good, it's added back on a few\n",
    "    # lines below using rcount function then merge.\n",
    "    data = data.drop(['count_during_period'], axis=1)\n",
    "\n",
    "    # Do count on df 'a' which contains the 'count_during_period' data from a few lines above.\n",
    "    result = rcount(a)\n",
    "    data = pd.concat([data,result], axis=1)\n",
    "\n",
    "    # Flag 'estimate' period (i.e. between start and end pts)\n",
    "    data['est_period'] = data['start_cumsum'] - data['end_cumsum']\n",
    "\n",
    "    # --------------------------------- get start and end dfs, then get ramp df and merge onto data\n",
    "    start_df = data[data['start_pts']==1]\n",
    "    end_df = data[data['end_pts']==1]\n",
    "\n",
    "    # In cases where there are no events, need to 'try'\n",
    "    try:\n",
    "        # Create new ramp_df.\n",
    "        # NOTE use +2 in the range in order to capture additional end points if the first end point occurs before the first start point.\n",
    "        # May make sense to even add a couple.. (i.e. +3 or +4) however this will do for now.\n",
    "        count_start_pts = start_df['start_cumsum'].max()\n",
    "        ramp_df = pd.DataFrame(data=list(range(1,int(count_start_pts+2))), columns=['event_num'])\n",
    "\n",
    "        # Get data from dfs\n",
    "        # Keep only cf, time_int and start_cumsum.\n",
    "        start_df = start_df[['cf', 'time_in_seconds', 'start_cumsum']]\n",
    "        # Then merge on start_cumsum\n",
    "        ramp_df = ramp_df.merge(start_df, left_on='event_num', right_on='start_cumsum')\n",
    "        # Rename columns\n",
    "        ramp_df = ramp_df.rename(columns = {'cf' : 'start_cf'})\n",
    "        ramp_df = ramp_df.rename(columns = {'time_in_seconds' : 'start_time_int'})\n",
    "\n",
    "        # Keep only cf, time)nt and start_cumsum.\n",
    "        end_df = end_df[['cf', 'time_in_seconds', 'end_cumsum']]\n",
    "        # Then merge on start_cumsum\n",
    "        ramp_df = ramp_df.merge(end_df, left_on='event_num', right_on='end_cumsum')\n",
    "        # Rename columns\n",
    "        ramp_df = ramp_df.rename(columns = {'cf' : 'end_cf'})\n",
    "        ramp_df = ramp_df.rename(columns = {'time_in_seconds' : 'end_time_int'})\n",
    "\n",
    "        # Check for cases where end time is BEFORE start time for an event.\n",
    "        # If this is the case, then delete that end time and shift all end times up by one.\n",
    "        # Check each event from top to bottom\n",
    "        num_events = ramp_df['event_num'].max()\n",
    "        for i in range(0, int(num_events)):\n",
    "            if ramp_df.loc[i, 'end_time_int'] < ramp_df.loc[i, 'start_time_int']:\n",
    "                ramp_df['end_time_int'] = ramp_df['end_time_int'].shift(-1)\n",
    "\n",
    "        # Calc the ramp rate\n",
    "        ramp_df['m'] = (ramp_df['end_cf'] - ramp_df['start_cf']) / (ramp_df['end_time_int'] - ramp_df['start_time_int'])\n",
    "\n",
    "        # Drop end and start cumsum, then merge onto data\n",
    "        ramp_df = ramp_df.drop(['end_cumsum', 'start_cumsum'], axis=1)\n",
    "        zero_row_for_ramp_df = pd.DataFrame(data=[0], columns=['event_num'])\n",
    "        ramp_df = pd.concat([ramp_df, zero_row_for_ramp_df])\n",
    "\n",
    "        data = data.reset_index().merge(ramp_df,  left_on='start_cumsum', right_on='event_num').set_index('t_stamp')\n",
    "        # Calc estimated CF\n",
    "        data['count_during_period_using_start_time'] = data['est_period'] * (data['time_in_seconds'] - data['start_time_int'])\n",
    "        data['est_cf'] = data['est_period'] * (data['start_cf'] + data['count_during_period_using_start_time']*data['m'])\n",
    "        # Add the 'end point'\n",
    "        data.loc[data['end_pts']==1,'est_cf'] = data['cf']\n",
    "\n",
    "        # Get est kW and est kWh\n",
    "        data['est_kW'] = data['est_cf'] * data['ac']\n",
    "        data['est_kWh'] = data['est_cf'] * data['ac'] * data['duration']/(60*60)\n",
    "\n",
    "        # Get power lost estimate\n",
    "        data['gen_loss_est_kWh'] = data['est_kWh'] - (data['power_kW']* data['duration']/(60*60))\n",
    "        # Issue is that we don't want gen lost to be less than zero!\n",
    "        data.loc[data['gen_loss_est_kWh'] <0,'gen_loss_est_kWh'] = 0\n",
    "        data['no_PV_curtail'] = 0\n",
    "    except:\n",
    "        data['no_PV_curtail'] = 1\n",
    "\n",
    "    # --------------------------------- concat onto output_df\n",
    "    output_df = pd.concat([output_df, data])\n",
    "    output_df['gen_kWh'] = output_df['power_kW'] * output_df['duration']/(60*60)\n",
    "    \n",
    "    if data['no_PV_curtail'].iloc[0] == 1:\n",
    "        estimation_method = 'None'\n",
    "        tripping_response = 'None'\n",
    "        tripping_curt_energy = 0\n",
    "    else:\n",
    "        # Clean output_df before exporting to csv\n",
    "        output_df_to_export = output_df[['ac','c_id','cf','clean','con_type','duration','energy','est_cf','est_kW',\n",
    "                                             'est_kWh','reactive_power','first_ac','gen_kWh','gen_loss_est_kWh','manufacturer',\n",
    "                                             'model','power_kW','site_id','v',\n",
    "                                             'zero_flag', 'time_in_seconds']]\n",
    "\n",
    "\n",
    "        # --------------------------------- Get summary stats\n",
    "        # Get site_id list\n",
    "        site_id_list = unaltered_data['site_id'].drop_duplicates().tolist()\n",
    "        # Create df to store results\n",
    "        sum_stats_df = pd.DataFrame(index=site_id_list)\n",
    "\n",
    "        # Get data of relevance from output_df, summarised by site_id\n",
    "        meta_df = pd.DataFrame({'power_kW': output_df.groupby('site_id')['power_kW'].sum(),\n",
    "        'gen_loss_est_kWh': output_df.groupby('site_id')['gen_loss_est_kWh'].sum(),\n",
    "        'event_num': output_df.groupby('site_id')['event_num'].max(),\n",
    "        'duration': output_df.groupby('site_id')['duration'].first(),\n",
    "        'mean_v_all_daylight_hours': output_df.groupby('site_id')['v'].mean(),\n",
    "        'first_ac': output_df.groupby('site_id')['first_ac'].first(),\n",
    "        'ac': output_df.groupby('site_id')['ac'].first(),\n",
    "        'model': output_df.groupby('site_id')['model'].first(),\n",
    "        'manufacturer': output_df.groupby('site_id')['manufacturer'].first()\n",
    "        })\n",
    "\n",
    "        # Concat onto results df and name the index\n",
    "        sum_stats_df = pd.concat([sum_stats_df, meta_df], axis=1)\n",
    "        sum_stats_df.index.name = 'site_id'\n",
    "\n",
    "        # Convert generation to kWh\n",
    "        sum_stats_df['gen_kWh'] = sum_stats_df['power_kW'] * sum_stats_df['duration']/(60*60)\n",
    "        # sum_stats_df = sum_stats_df.rename(columns = {'power_kW' : 'gen_kWh'})\n",
    "\n",
    "        # Calc percentage of gen lost\n",
    "        sum_stats_df['percentage_lost'] = sum_stats_df['gen_loss_est_kWh'].abs() / (sum_stats_df['gen_loss_est_kWh'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "\n",
    "        # Get voltage box plot statistics for both curtail times and non curtail times\n",
    "        curtail_v_df = output_df[output_df['est_period'] == 1]\n",
    "        all_other_v_df = output_df[output_df['est_period'] != 1]\n",
    "        # Filter for voltage and site it then get summary stats\n",
    "        # Curtail times\n",
    "        curtail_v_df = curtail_v_df[['v','site_id']]\n",
    "        # rename 'v' to 'curtail_v' in order to see which is which when added to sum_stats_df\n",
    "        curtail_v_df = curtail_v_df.rename(columns = {'v' : 'v_curtail'})\n",
    "        curtail_v_box_plot_stats_df = curtail_v_df.groupby('site_id').describe()\n",
    "        # Non curtail times\n",
    "        all_other_v_df = all_other_v_df[['v','site_id']]\n",
    "        # rename 'v' to 'other_v' in order to see which is which when added to sum_stats_df\n",
    "        all_other_v_df = all_other_v_df.rename(columns = {'v' : 'v_all_other'})\n",
    "        all_other_v_box_plot_stats_df = all_other_v_df.groupby('site_id').describe()\n",
    "\n",
    "        # add box plot stats onto summary stats\n",
    "        sum_stats_df = pd.concat([sum_stats_df, curtail_v_box_plot_stats_df, all_other_v_box_plot_stats_df], axis=1)\n",
    "\n",
    "        # Get penetration by postcode\n",
    "        # TODO - need to update the CER and APVI data files to match the Solar Analytics data set period being analysed!\n",
    "        # TODO - could not locate the same type of APVI file (for dwellings) so may need to use the older data.\n",
    "        # TODO - the CER data will require some attention and util will have to be updated to make it accept the updated CER data.\n",
    "        # sum_stats_df = util.get_penetration_by_postcode(PC_INSTALLS_DATA_FILE_PATH, DWELLINGS_DATA_FILE_PATH, sum_stats_df, output_df)\n",
    "\n",
    "        # Sort and get % of systems\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost', ascending =False)\n",
    "\n",
    "        # Get % of systems\n",
    "        sum_stats_df['proportion_of_sites'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites'] = (sum_stats_df['proportion_of_sites'] + 1) / len(sum_stats_df)\n",
    "\n",
    "        #BELOW IS ADAPTED FROM NAOMI'S POLYFIT METHOD\n",
    "\n",
    "        # Gets PV curtailment estimate using a polynomial fit method with an iterative step to remove 'outliers'\n",
    "        # (only really useful for clear sky days! Otherwise the straight line approximation is preferable!!)\n",
    "        # See write up of method for key limitations and next steps\n",
    "\n",
    "        #------------------------ Step 0: Import required packages ------------------------\n",
    "        # Import packages required for program\n",
    "        import matplotlib.dates as mdates\n",
    "        import seaborn as sns; sns.set()\n",
    "        # For graphing time series\n",
    "        time_fmt = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "        '''\n",
    "        # Data files are located here:\n",
    "        INPUT_DATA_FILE_PATH = 'F:/05_Solar_Analytics/2021-05-31_CANVAS_Solar_Analytics_data/02_Curtail_output/'\n",
    "        OUTPUT_FILE_PATH = \"F:/05_Solar_Analytics/2021-05-31_CANVAS_Solar_Analytics_data/03_Polyfit_output/\"\n",
    "\n",
    "        # File names are here:\n",
    "        TS_DATA_FILE_PATH = '_analysis_profiles_v4.csv'\n",
    "        SUM_STATS_DATA_FILE_PATH = \"_analysis_sum_stats_v4.csv\"\n",
    "        OUTPUT_PROFILES = \"_analysis_profiles_polyfit_v4.csv\"\n",
    "        OUTPUT_SUM_STATS = \"_analysis_sum_stats_polyfit_v4.csv\"\n",
    "\n",
    "        # File path for clear sky days csv\n",
    "        CLEAR_SKY_DAYS_FILE_PATH = 'F:/CANVAS/clear_sky_days_01-2019_07-2020_manual.csv'\n",
    "        '''\n",
    "        # This value is used to remove data points when calculating the polynomial.\n",
    "        # The first polynomial uses all non zero cf values.\n",
    "        # Then the straight line correlation between polyfit and actual cf is calculated and residuals found for each cf\n",
    "        # Data points with residuals greater than or less than the allowed residual band are removed and\n",
    "        # the polynomial fit is recalculated using this smaller subset of points: 'polyfit_iter'\n",
    "        allowed_residual_band = 0.05 # NOTE - set to 0.05 after some sensitivity testing and eye balling\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        #for data_date in data_date_list:\n",
    "        data_date = date\n",
    "        # Load PV profiles\n",
    "        data_df = output_df_to_export\n",
    "        # Load clear sky days CSV and flag NON clear sky days in data_df\n",
    "        #clear_sky_days_df = pd.read_csv(CLEAR_SKY_DAYS_FILE_PATH)\n",
    "        #clear_sky_days_list = clear_sky_days_df['clear_sky_days'].astype(str).tolist()\n",
    "        #if data_date in clear_sky_days_list:\n",
    "        if is_clear_sky_day:\n",
    "            data_df['non_clear_sky_day_flag'] = 0\n",
    "        else:\n",
    "            data_df['non_clear_sky_day_flag'] = 1\n",
    "\n",
    "        # Get list of c_ids\n",
    "        c_id_list = data_df['c_id'].drop_duplicates().tolist()\n",
    "        # Set up output_df\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df.index.name = 't_stamp'\n",
    "        counter = 0\n",
    "\n",
    "        #for c_id in c_id_list:\n",
    "        c_id = c_id\n",
    "        perc_complete = counter / len(c_id_list)\n",
    "        #print(perc_complete)\n",
    "        counter += 1\n",
    "\n",
    "        # Filter for c_id\n",
    "        pv_data = data_df[data_df['c_id'] == c_id]\n",
    "        pv_data['t_stamp_copy'] = pv_data.index\n",
    "\n",
    "        # First get time in seconds for polyfit\n",
    "        pv_data['hrs'] = pv_data.index.hour\n",
    "        pv_data['min'] = pv_data.index.minute\n",
    "        pv_data['sec'] = pv_data.index.second\n",
    "        pv_data['time_in_seconds'] = pv_data['hrs'] * 60 * 60 + pv_data['min'] * 60 + pv_data['sec']\n",
    "\n",
    "        # Try applying a 2nd order polynomial **to non-zero cf points only**\n",
    "        # Needs to be 'try' because if there are ONLY zero points then error b/c we pass an empty df to polyfit function\n",
    "        try:\n",
    "            test = pv_data[pv_data['cf']>0]\n",
    "            x = test['time_in_seconds']\n",
    "            y = test['cf']\n",
    "            z = np.polyfit(x,y,2)\n",
    "\n",
    "            # Calc the fitted line\n",
    "            test['polynomial_fit'] = z[0]*test['time_in_seconds']*test['time_in_seconds'] + \\\n",
    "                                            z[1]*test['time_in_seconds'] + z[2]\n",
    "            # This is calculated for all times (not just non zero) as well for printing / checking\n",
    "            pv_data['polynomial_fit'] = z[0]*pv_data['time_in_seconds']*pv_data['time_in_seconds'] + \\\n",
    "                                            z[1]*pv_data['time_in_seconds'] + z[2]\n",
    "\n",
    "            # Get the correlation between my polynomial and the cf data (excluding zeroes) then remove points with\n",
    "            # 'large' residuals\n",
    "            # Get line of best fit\n",
    "            test['ones'] = 1\n",
    "            A = test[['cf', 'ones']]\n",
    "            y = test['polynomial_fit']\n",
    "            m,c = np.linalg.lstsq(A,y)[0]\n",
    "            test['y_line'] = c + m*test['cf']\n",
    "\n",
    "            # Remove data points where the residual is +/- allowed_residual_band from the line of best fit\n",
    "            # (in an attempt to improve our correlation)\n",
    "            test['residuals'] = test['polynomial_fit'] - test['y_line']\n",
    "            test_filtered = test[test['residuals'].abs() <= allowed_residual_band]\n",
    "\n",
    "            # Use this filtered curve to get a new polyfit\n",
    "            x = test_filtered['time_in_seconds']\n",
    "            y = test_filtered['cf']\n",
    "            z = np.polyfit(x,y,2)\n",
    "\n",
    "            test_filtered['polynomial_fit'] = z[0]*test_filtered['time_in_seconds']*test_filtered['time_in_seconds'] + \\\n",
    "                                              z[1]*test_filtered['time_in_seconds'] + z[2]\n",
    "            pv_data['polyfit_iter'] = z[0]*pv_data['time_in_seconds']*pv_data['time_in_seconds'] + \\\n",
    "                                            z[1]*pv_data['time_in_seconds'] + z[2]\n",
    "            # Where there is est_cf (i.e. it's identified as a period of curtailment and so we have a straight line\n",
    "            # estimate) then use est_cf_polyfit_iter\n",
    "            pv_data['est_cf_polyfit_iter'] = np.nan\n",
    "            pv_data.loc[pv_data['est_cf']>0, 'est_cf_polyfit_iter'] = pv_data['polyfit_iter']\n",
    "\n",
    "            # Just keep the polyfit_iter for the periods where there was already a straight line estimate as above\n",
    "            pv_data = pv_data.drop(['polynomial_fit'], axis=1)\n",
    "\n",
    "            # Get est kW and est kWh\n",
    "            pv_data['est_kW_polyfit_iter'] = pv_data['est_cf_polyfit_iter'] * pv_data['ac']\n",
    "            pv_data['est_kWh_polyfit_iter'] = pv_data['est_cf_polyfit_iter'] * \\\n",
    "                                              pv_data['ac'] * pv_data['duration'] / (60 * 60)\n",
    "            # Get power lost estimate\n",
    "            pv_data['gen_loss_est_kWh_polyfit_iter'] = pv_data['est_kWh_polyfit_iter'] - pv_data['gen_kWh']\n",
    "            # Issue is that we don't want gen lost to be less than zero!\n",
    "            pv_data.loc[pv_data['gen_loss_est_kWh_polyfit_iter'] < 0, 'gen_loss_est_kWh_polyfit_iter'] = 0\n",
    "\n",
    "        except:\n",
    "            print('Error somewhere in the polyfit process for c_id ' + str(c_id))\n",
    "\n",
    "        # --------------------------------- concat onto output_df\n",
    "        output_df = pd.concat([output_df, pv_data])\n",
    "\n",
    "        # *********************************** CHECKS and identify 'preferred' method ***********************************\n",
    "        # Check on polyfit giving large cfs (>=1) --> allowed if the cf for that c_id is already large\n",
    "        # For each c_id get max polyfit and max cf\n",
    "        cf_max_check = pd.DataFrame({'cf_max' : output_df.groupby('c_id')['cf'].max(),\n",
    "                                     'polyfit_iter_cf_max' : output_df.groupby('c_id')['est_cf_polyfit_iter'].max(),\n",
    "                                     'site_id' : output_df.groupby('c_id')['site_id'].first()})\n",
    "        # Find cases where straight line and polyfit iter methods return cf >= 1\n",
    "        cf_max_check['straight_line_max_greater_or_equal_1'] = np.nan\n",
    "        cf_max_check.loc[cf_max_check['cf_max'] >= 1, 'straight_line_max_greater_or_equal_1'] = 1\n",
    "        cf_max_check['polyfit_iter_max_greater_or_equal_1'] = np.nan\n",
    "        cf_max_check.loc[cf_max_check['polyfit_iter_cf_max'] >= 1, 'polyfit_iter_max_greater_or_equal_1'] = 1\n",
    "        # Flag cases where straight line method must be used. i.e. the polyfit iter cf max is  >= 1, but straight line cf max is not.\n",
    "        cf_max_check = cf_max_check.fillna(0)\n",
    "        cf_max_check['must_use_straight_line_method_due_to_cf_max'] = cf_max_check['polyfit_iter_max_greater_or_equal_1'] - cf_max_check['straight_line_max_greater_or_equal_1']\n",
    "        cf_max_check.loc[cf_max_check['must_use_straight_line_method_due_to_cf_max'] < 0, 'must_use_straight_line_method_due_to_cf_max'] = 0\n",
    "        # Get new df by site_id in order to merge onto output_df\n",
    "        cf_max_check_by_site_id = pd.DataFrame({'must_use_straight_line_method_due_to_cf_max' : cf_max_check.groupby('site_id')['must_use_straight_line_method_due_to_cf_max'].max()})\n",
    "\n",
    "        # Check whether the straight line or polyfit iter gives a larger total generation lost.\n",
    "        # We want to take the larger of the two.\n",
    "        gen_loss_total_check = pd.DataFrame({'straight_line_gen_loss_total' : output_df.groupby('site_id')['gen_loss_est_kWh'].sum(),\n",
    "                                             'polyfit_iter_gen_loss_total' : output_df.groupby('site_id')['gen_loss_est_kWh_polyfit_iter'].sum()})\n",
    "        gen_loss_total_check['must_use_straight_line_method_due_to_gen_loss_total'] = np.nan\n",
    "        gen_loss_total_check.loc[gen_loss_total_check['straight_line_gen_loss_total'] > gen_loss_total_check['polyfit_iter_gen_loss_total'], 'must_use_straight_line_method_due_to_gen_loss_total'] = 1\n",
    "        gen_loss_total_check = gen_loss_total_check.fillna(0)\n",
    "        gen_loss_total_check = gen_loss_total_check[['must_use_straight_line_method_due_to_gen_loss_total']]\n",
    "\n",
    "        # Merge both checks back onto output_df and create a single column: use straight line estimate over polyfit iter? Y/N\n",
    "        output_df = output_df.merge(cf_max_check_by_site_id, left_on='site_id', right_index=True, how='left')\n",
    "        output_df = output_df.merge(gen_loss_total_check, left_on='site_id', right_index=True, how='left')\n",
    "        # Get flag if either conditions are true\n",
    "        # OR if not a clear sky day\n",
    "        output_df['use_straight_line_method_flag'] = output_df['must_use_straight_line_method_due_to_gen_loss_total'] + output_df['must_use_straight_line_method_due_to_cf_max'] + output_df['non_clear_sky_day_flag']\n",
    "        output_df.loc[output_df['use_straight_line_method_flag'] > 1, 'use_straight_line_method_flag'] = 1\n",
    "        output_df['use_polyfit_iter_method_flag'] = 1 - output_df['use_straight_line_method_flag']\n",
    "\n",
    "        # Set the preferred est_cf_preferred etc to the polyfit method, unless the straight line flag is present,\n",
    "        # in which case use the straight line method\n",
    "        output_df['est_cf_preferred'] = (output_df['est_cf_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_cf'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['est_kW_preferred'] = (output_df['est_kW_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_kW'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['est_kWh_preferred'] = (output_df['est_kWh_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['est_kWh'] * output_df['use_straight_line_method_flag'])\n",
    "        output_df['gen_loss_est_kWh_preferred'] = (output_df['gen_loss_est_kWh_polyfit_iter'] * output_df['use_polyfit_iter_method_flag']) + (output_df['gen_loss_est_kWh'] * output_df['use_straight_line_method_flag'])\n",
    "\n",
    "        # Optional save data to csv\n",
    "        #output_df.to_csv(OUTPUT_FILE_PATH + data_date + OUTPUT_PROFILES)\n",
    "\n",
    "        # --------------------------------- Summary stuff\n",
    "        # Calc the new generation lost amount by site and also get the max for checking that polyfit doesn't go above 1\n",
    "        # Also add the reason for selecting polyfit or linear estimate\n",
    "        new_gen_lost = pd.DataFrame({ 'gen_loss_est_kWh_polyfit_iter' : output_df.groupby('site_id')['gen_loss_est_kWh_polyfit_iter'].sum(),\n",
    "                                      'gen_loss_est_kWh_preferred' : output_df.groupby('site_id')['gen_loss_est_kWh_preferred'].sum(),\n",
    "                                      'linear_method_preferred' : output_df.groupby('site_id')['use_straight_line_method_flag'].max(),\n",
    "                                      'polyfit_method_preferred' : output_df.groupby('site_id')['use_polyfit_iter_method_flag'].max()})\n",
    "\n",
    "        # Open previous sum stats\n",
    "        #sum_stats_df = pd.read_csv(INPUT_DATA_FILE_PATH + data_date + SUM_STATS_DATA_FILE_PATH)\n",
    "        sum_stats_df = sum_stats_df\n",
    "\n",
    "        # Append on the new gen lost\n",
    "        sum_stats_df = sum_stats_df.merge(new_gen_lost, left_on='site_id', right_index=True)\n",
    "\n",
    "        # Calc percentage of gen lost using polyfit iter and preferred\n",
    "        sum_stats_df['percentage_lost_polyfit_iter'] = sum_stats_df['gen_loss_est_kWh_polyfit_iter'].abs() / (sum_stats_df['gen_loss_est_kWh_polyfit_iter'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "        sum_stats_df['percentage_lost_preferred'] = sum_stats_df['gen_loss_est_kWh_preferred'].abs() / (sum_stats_df['gen_loss_est_kWh_preferred'].abs() + sum_stats_df['gen_kWh'].abs())\n",
    "\n",
    "        # Get proportion of sites for graphing using polyfit iter and preferred\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost_polyfit_iter', ascending =False)\n",
    "        sum_stats_df['proportion_of_sites_polyfit_iter'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites_polyfit_iter'] = (sum_stats_df['proportion_of_sites_polyfit_iter'] + 1) / len(sum_stats_df)\n",
    "        # Preferred\n",
    "        sum_stats_df = sum_stats_df.sort_values('percentage_lost_preferred', ascending =False)\n",
    "        sum_stats_df['proportion_of_sites_preferred'] = range(len(sum_stats_df))\n",
    "        sum_stats_df['proportion_of_sites_preferred'] = (sum_stats_df['proportion_of_sites_preferred'] + 1) / len(sum_stats_df)\n",
    "\n",
    "        # Save summary statistics to  csv\n",
    "        #sum_stats_df.to_csv(OUTPUT_FILE_PATH + data_date + OUTPUT_SUM_STATS)\n",
    "\n",
    "        tripping_curt_energy = output_df['gen_loss_est_kWh_preferred'].sum()\n",
    "        generated_energy = output_df['est_kWh_preferred'].sum()\n",
    "\n",
    "        use_polyfit_iter_method_flag = output_df['use_polyfit_iter_method_flag'].iloc[0]\n",
    "        if use_polyfit_iter_method_flag == 1:\n",
    "            estimation_method = 'Polyfit'\n",
    "        else:\n",
    "            estimation_method = 'Linear'\n",
    "        if tripping_curt_energy > 0:\n",
    "            tripping_response = 'Yes'\n",
    "        else:\n",
    "            tripping_response = 'None'\n",
    "\n",
    "    return tripping_response, tripping_curt_energy, estimation_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83034b",
   "metadata": {},
   "source": [
    "# VVAr CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_organize(c_id_idx, site_details, data, unique_cids):\n",
    "    \"\"\"Get a single site data and relevant meta-data information.\n",
    "\n",
    "    Args:\n",
    "        c_id_idx (int): c_id value\n",
    "        site_details (df): site_details dataframe from unsw_20190701_site_details.csv file\n",
    "        data (df): D-PV time-series dataframe from input_monthly_file function output\n",
    "        unique_cids (df): Dataframe listing unique c_id's and their corresponding site_id\n",
    "\n",
    "    Returns:\n",
    "        data_site(df): D-PV time-series dataframe, filtered by its site_id and cleaned (polarity correction etc)\n",
    "        ac_cap (float): inverter capacity in W\n",
    "        dc_cap (float): PV array capacity in Wp\n",
    "        eff_system (float): Assumed PV array efficiency between 0 and 1\n",
    "        inverter (str): Concatenated string of inverter manufacturer and model\n",
    "        \n",
    "    Originally written by Baran for VVAr Curtailment Calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #c_id = unique_cids.loc[c_id_idx][0]\n",
    "    c_id = c_id_idx\n",
    "    \n",
    "    polarity = site_details.loc[site_details['c_id'] == c_id, 'polarity'].values[0] # get the polarity of the site\n",
    "    ac_cap = site_details.loc[site_details['c_id'] == c_id, 'ac_cap_w'].values[0]\n",
    "    dc_cap = site_details.loc[site_details['c_id'] == c_id, 'dc_cap_w'].values[0]\n",
    "    inverter = site_details.loc[site_details['c_id'] == c_id, 'inverter_manufacturer'].values[0] + ' ' + site_details.loc[site_details['c_id'] == c_id, 'inverter_model'].values[0]\n",
    "\n",
    "    # Extract single site data and organize: \n",
    "    data_site = data[data['c_id'] == c_id].sort_index() # get the monthly data of the specific c_id\n",
    "\n",
    "    data_site['power'] = data_site['power'].values * polarity # polarity correction for real power\n",
    "    data_site.loc[data_site['power'] < 0, 'power'] = 0 #replace negative power value into zero\n",
    "    data_site['reactive_power'] = data_site['reactive_power'].values * polarity # polarity correction for reactive power\n",
    "    \n",
    "    data_site['reactive_power'] = [data_site['reactive_power'].values * -1 if np.percentile(data_site.loc[(data_site.index.hour >= 7) & (data_site.index.hour <= 17), 'reactive_power'], 75) < 0 else data_site['reactive_power'].values][0]  # double check the polarity for reactive power\n",
    "    \n",
    "    if (abs(np.percentile(data_site['reactive_power'], 99))> ac_cap) | (abs(np.percentile(data_site['reactive_power'], 1))> ac_cap): #some VAr measurements in energy format and needs to be divided by duration (i.e., 60 sec)\n",
    "        # data_site['reactive_power'] =  data_site['reactive_power'].values / data_site['duration'].values # unfortunately SolA data doesn't calculate energy according to respective duration but uses a fixed 60 sec values for energy calculation\n",
    "        data_site['reactive_power'] =  data_site['reactive_power'].values / 60\n",
    "        \n",
    "    data_site.index = pd.to_datetime([str(d)[0:19] for d in data_site.index]) ## convert index to make the df plottable (by removing the UTC conversion)\n",
    "    data_site.sort_index(ascending = True, inplace = True) # sort the index in ascending form\n",
    "    # System efficiency for calculating theoretical max output later on (use conservative loss estimates for DC power)\n",
    "    eff_inv = 0.98\n",
    "    eff_vdrop = 0.98 \n",
    "    eff_derating = 0.99  # module derating losses\n",
    "    eff_system = eff_inv * eff_vdrop * eff_derating\n",
    "\n",
    "    # Apparent power of the inverter\n",
    "    data_site['va'] = np.sqrt (data_site['power'].values**2 + data_site['reactive_power'].values**2)\n",
    "    data_site['pf'] = data_site['power']/data_site['va']\n",
    "    \n",
    "    return data_site, ac_cap, dc_cap, eff_system, inverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vvar_curtailment(c_id, date, data_site,  ghi, ac_cap, dc_cap, eff_system, is_clear_sky_day):\n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    data_site_certain_date = data_site.loc[data_site.index.date == date_dt]\n",
    "    ghi = ghi.loc[ghi.index.date == date_dt]\n",
    "    data_site = data_site_certain_date\n",
    "    \n",
    "    # Manipulations on the original data_site to match the GHI\n",
    "    dummy = data_site.copy()\n",
    "    dummy.index = dummy.index.round('min')   # round the timestamp to nearest minute to match with the GHI\n",
    "    dummy = dummy.groupby(level = 0 ).mean()  # average same timestamp values that fall under the same minute category\n",
    "\n",
    "    data_site_complete = pd.DataFrame (index = ghi.index)  # create a data_site_complete with complete set of dates to match with GHI\n",
    "    data_site_complete = data_site_complete.join(dummy)\n",
    "    \n",
    "    # Required conditions for V-VAr curtailment\n",
    "    var_t = 100  # min VAr condition \n",
    "    duration = 60  # we have normalized all t-stamps to 60 second previously\n",
    "    va_criteria = data_site_complete['va'] >= (ac_cap - var_t)  # this is to ensure inverter VA is close to its rated capacity (this eliminates the instances of tripping)\n",
    "    var_criteria = abs(data_site_complete['reactive_power'].values) > var_t  # this is to ensure inverter is injecting/absorbing at least 100 vars\n",
    "    curt_criteria = va_criteria & var_criteria  # curtailment criteria that satisfies the two criteria above\n",
    "\n",
    "    data_curtailment = data_site_complete[curt_criteria]  # investigate curtailment only for the instances which satisfy above criteria \n",
    "    ghi_curtailment = ghi[curt_criteria]\n",
    "    \n",
    "    if not var_criteria.any():\n",
    "        vvar_response = 'None'\n",
    "    else:\n",
    "        vvar_response = 'Yes'\n",
    "        \n",
    "    # max_real_power refers to what the system could generate if it wasn't curtailed\n",
    "    #ISSUES FOR TROUBLESHOOTING LATER: SOMETIME MAX POWER IS LESS THAN POWER?\n",
    "    if is_clear_sky_day:\n",
    "        # POLYFIT METHOD TO CALCULATE THE MAX POWER WITHOUT CURTAILMENT, UNAPPLICABLE IN NON CLEAR SKY DAYS\n",
    "        circuit_day_data = data_site_complete.reset_index(level=0)\n",
    "        circuit_day_data.rename(columns = {'timestamp':'ts'}, inplace = True)\n",
    "        circuit_day_data['ts'] = circuit_day_data['ts'].astype(str)\n",
    "\n",
    "        df = circuit_day_data\n",
    "        df = SliceEndOffDF(df) # REMOVES LAST TAIL AND HEAD OF DATA AFTER IT CHANGES TO ZERO WATTS, BUT KEEPS ZERO WATT VALUES IN THE MIDDLE OF THE LIST\n",
    "\n",
    "        df = df.loc[df['power'] > 300]\n",
    "\n",
    "        # FILTER POWER DATA TO INCLUDE ONLY INCREASING VALUES FROM EACH SIDES (WHERE SIDES ARE DETERMINED BY EITHER SIDE OF THE MAX POWER VALUE)\n",
    "        powerArray, timeArray = FilterPowerData(df)\n",
    "\n",
    "        # FILTER DATA SO ONLY A SUBSET OF GRADIENTS BETWEEN DATAPOINTS IS PERMITTED\n",
    "        powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "        polyfit = GetPolyfit(getDateTimeList(timeArray), powerArray, 2)\n",
    "\n",
    "        polyfit_result = pd.DataFrame({\n",
    "            'timestamp' : pd.date_range(start=df['ts'].iloc[0], end=df['ts'].iloc[-1], freq='1min').astype(str)\n",
    "        })\n",
    "        polyfit_result['max_real_power'] = polyfit(getDateTimeList(polyfit_result['timestamp']))\n",
    "        polyfit_result.index = pd.to_datetime(polyfit_result['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "        polyfit_result.drop(columns = 'timestamp', inplace = True)\n",
    "\n",
    "        data_curtailment = pd.merge(data_curtailment, polyfit_result, left_index = True, right_index = True)\n",
    "        data_curtailment ['curtailment'] = data_curtailment['max_real_power'].values - data_curtailment ['power'].values\n",
    "        data_curtailment['curtailment_energy'] = data_curtailment['curtailment'].values * (duration/3600/1000) # Wmin to kWh energy: some sites have variable duration so finding curtailment in energy form (Wh)\n",
    "        \n",
    "        if not data_curtailment[data_curtailment['curtailment_energy'] > 0]['curtailment_energy'].sum() > 0:\n",
    "                data_curtailment['max_real_power'] = [min(ghi_t/1000 * dc_cap * eff_system, ac_cap) for ghi_t in ghi_curtailment['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    \n",
    "    else: #if it is not clear sky day, use ghi to estimate maximum power without curtailmentz\n",
    "        data_curtailment['max_real_power'] = [min(ghi_t/1000 * dc_cap * eff_system, ac_cap) for ghi_t in ghi_curtailment['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    # =============================================================================================\n",
    "    \n",
    "    data_curtailment ['curtailment'] = data_curtailment['max_real_power'].values - data_curtailment ['power'].values\n",
    "    data_curtailment['curtailment_energy'] = data_curtailment['curtailment'].values * (duration/3600/1000) # Wmin to kWh energy: some sites have variable duration so finding curtailment in energy form (Wh)\n",
    "    vvar_curt_energy = data_curtailment[data_curtailment['curtailment_energy'] > 0]['curtailment_energy'].sum()\n",
    "    return vvar_response, vvar_curt_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4f261",
   "metadata": {},
   "source": [
    "# VWATT CURTAILMENT PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0db49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VWATT CLASSES AND FUNCTIONS\n",
    "\n",
    "# SITE AND CIRCUIT CLASSES USED TO ORGANISE THE SITES TELEMETRY AND META DATA FOR EASE IN PROCESSING\n",
    "\n",
    "class Site:\n",
    "  def __init__(self, site_id, s_postcode, pv_install_date, ac_cap_w, dc_cap_w, inverter_manufacturer, inverter_model):\n",
    "    self.site_id = site_id\n",
    "    self.s_postcode = s_postcode\n",
    "    self.pv_install_date = pv_install_date\n",
    "    self.ac_cap_w = ac_cap_w\n",
    "    self.dc_cap_w = dc_cap_w\n",
    "    self.inverter_manufacturer = inverter_manufacturer\n",
    "    self.inverter_model = inverter_model\n",
    "    self.c_id_data = {}\n",
    "\n",
    "class Circuit:\n",
    "  def __init__(self, c_id, site_id, con_type, polarity):\n",
    "    self.c_id = c_id\n",
    "    self.con_type = con_type\n",
    "    self.polarity = polarity\n",
    "    self.day_data = {}\n",
    "    \n",
    "# ADJUST FORMATE FOR TIMESTAMP STRINGS\n",
    "def Get_timestamp_date_string(string):\n",
    "    x = string.split(\"_\")\n",
    "    return x[0] + \"-\" + x[1]\n",
    "\n",
    "# SEPARATE THE BoM GHI DATA FILES PER DAY TO SEARCH FOR CLEAR SKY DAYS\n",
    "def Separate_ghi_data(month, ghi):\n",
    "    ghi['ts'] = pd.to_datetime(pd.DataFrame({'year': ghi['Year Month Day Hours Minutes in YYYY'].values,\n",
    "                                                    'month': ghi['MM'],\n",
    "                                                    'day': ghi['DD'],\n",
    "                                                    'hour': ghi['HH24'],\n",
    "                                                    'minute': ghi['MI format in Local standard time']}))\n",
    "    ghi.rename(columns={'Mean global irradiance (over 1 minute) in W/sq m': 'mean_ghi',\n",
    "                        'Minimum 1 second global irradiance (over 1 minute) in W/sq m': 'min_ghi',\n",
    "                        'Maximum 1 second global irradiance (over 1 minute) in W/sq m': 'max_ghi',\n",
    "                        'Standard deviation of global irradiance (over 1 minute) in W/sq m': 'sd_ghi',\n",
    "                        'Uncertainty in mean global irradiance (over 1 minute) in W/sq m': 'uncertainty_ghi'},\n",
    "               inplace=True)\n",
    "    key_ghi_values = ghi[['ts', 'mean_ghi', 'min_ghi', 'max_ghi', 'sd_ghi', 'uncertainty_ghi']].copy()\n",
    "    key_ghi_values['mean_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['mean_ghi']), axis=1)\n",
    "    key_ghi_values['min_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['min_ghi']), axis=1)\n",
    "    key_ghi_values['max_ghi'] = key_ghi_values.apply(lambda row: String_to_Float(row['max_ghi']), axis=1)\n",
    "\n",
    "\n",
    "    combined_ghi_dict = {}\n",
    "    month_number = int(month.split('-')[1])\n",
    "\n",
    "    for day in range(1, Days_in_month(month_number) + 1):\n",
    "        day_string = str(day)\n",
    "        if day < 10:\n",
    "            day_string = \"0\" + day_string\n",
    "\n",
    "        date = month + \"-\" + day_string\n",
    "        df = key_ghi_values.loc[key_ghi_values['ts'] > date + \" 00:00:01\"]\n",
    "        df = df.loc[key_ghi_values['ts'] < date + \" 23:59:01\"]\n",
    "\n",
    "        combined_ghi_dict[date] = df\n",
    "\n",
    "    return combined_ghi_dict\n",
    "\n",
    "# REMOVE SPACES AND CHECK IF VALUE NULL\n",
    "def String_to_Float(string):\n",
    "    x = string.strip()\n",
    "    if not x:\n",
    "        x = 0\n",
    "    else:\n",
    "        x = float(x)\n",
    "    return x\n",
    "\n",
    "def Days_in_month(month):\n",
    "        switcher = {\n",
    "            1: 31,\n",
    "            2: 29,\n",
    "            3: 31,\n",
    "            4: 30,\n",
    "            5: 31,\n",
    "            6: 30,\n",
    "            7: 31,\n",
    "            8: 31,\n",
    "            9: 30,\n",
    "            10: 31,\n",
    "            11: 30,\n",
    "            12: 31,\n",
    "        }\n",
    "        return switcher.get(month, 0)\n",
    "    \n",
    "# LOOK FOR FOR SUDDEN VARIATIONS IN SOLAR INSOLATION DATA WHICH INDICATES LIKELY CLOUD COVER, AS OPPOSED TO CLEAR PARABOLIC SHAPE OF CLEAR SKY DAY GHI CURVES\n",
    "def Detect_clear_sky_day(ghi_df, min_max_ghi):\n",
    "    df_daytime = ghi_df.loc[ghi_df['mean_ghi'] > 0]\n",
    "\n",
    "    collective_change = 0\n",
    "    ghi_list = df_daytime.mean_ghi.tolist()\n",
    "\n",
    "    for i in range(len(ghi_list)-1):\n",
    "        collective_change += abs(ghi_list[i+1] - ghi_list[i])\n",
    "\n",
    "    if len(df_daytime.index) == 0:\n",
    "        return False, 0\n",
    "    \n",
    "    average_delta_y = collective_change/len(df_daytime.index)\n",
    "\n",
    "    if average_delta_y < 5 and max(ghi_df.mean_ghi) > min_max_ghi:\n",
    "        return True, average_delta_y\n",
    "    else:\n",
    "        return False, average_delta_y\n",
    "    \n",
    "def filter_sunrise_sunset(df):\n",
    "    \"\"\"Filter a D-PV Time series data based on its estimated sunrise and sunset time.\n",
    "\n",
    "    Args:\n",
    "    df (df): D-PV Time series data\n",
    "\n",
    "    Returns:\n",
    "    sunrise (timestamp): estimated sunrise time (when power is >10 W for the first time in a day)\n",
    "    sunset (timestamp): the opened ghi data (when power is >10 W for the last time in a day)\n",
    "    df (df): filtered D-PV Time series data\n",
    "    \"\"\"\n",
    "    if df is None or len(df.index) == 0:\n",
    "        return None\n",
    "\n",
    "    tmpDF = df.loc[df['power'] > 10]\n",
    "    if len(tmpDF.index) == 0:\n",
    "        return None\n",
    "\n",
    "    sunrise = tmpDF.index[0]\n",
    "    sunset = tmpDF.index[-1]\n",
    "\n",
    "    df = df.loc[df.index > sunrise]\n",
    "    df = df.loc[df.index < sunset]\n",
    "\n",
    "    return sunrise, sunset, df\n",
    "\n",
    "def filter_power_data(df):\n",
    "    \"\"\"Take the time and power data from D-PV time-series data & filter out curtailment. Will be used for polyfit regression.\n",
    "\n",
    "    Args:\n",
    "    df (df): Time-series D-PV data with power column and timestamp as an index\n",
    "\n",
    "    Returns:\n",
    "    powerArray (pd series): filtered power data\n",
    "    timeArray (pd datetime): filtered timestamp data\n",
    "    \"\"\"\n",
    "    \n",
    "    maxDailyPower = max(df.power)\n",
    "    if len(df.loc[df['power'] == maxDailyPower].index) > 1:\n",
    "        return None, None\n",
    "    \n",
    "    filter_first_half = []\n",
    "    filter_second_half = []\n",
    "    powerArray = df.power\n",
    "    timeArray = df.index\n",
    "    \n",
    "    halfFlag = True  # True is first half, False is second half\n",
    "    last_highest_power = 0\n",
    "    \n",
    "    for power in powerArray:\n",
    "\n",
    "        # IF power IS GREATER THAN last_highest_power THEN INCLUDE power AND INCREASE last_highest_power\n",
    "        if power > last_highest_power:\n",
    "            last_highest_power = power\n",
    "            filter_first_half.append(True)\n",
    "        else:\n",
    "            filter_first_half.append(False)\n",
    "\n",
    "        if power == maxDailyPower:\n",
    "            break\n",
    "            \n",
    "    last_highest_power = 0\n",
    "    \n",
    "    # PERFORM SAME FILTER ON SECOND SIDE OF POWER ARRAY\n",
    "    for power in powerArray.iloc[::-1]:\n",
    "\n",
    "        if power == maxDailyPower:\n",
    "            break\n",
    "\n",
    "        if power > last_highest_power:\n",
    "            last_highest_power = power\n",
    "            filter_second_half.append(True)\n",
    "        else:\n",
    "            filter_second_half.append(False)\n",
    "            \n",
    "    # COMBINE TO FILTERED SIDES\n",
    "    filter_second_half.reverse()\n",
    "    filterArray = filter_first_half + filter_second_half\n",
    "    return powerArray[filterArray], timeArray[filterArray]\n",
    "\n",
    "def Get_telemetry_string(string):\n",
    "    x = string.split(\"_\")\n",
    "    return x[0] + x[1]\n",
    "\n",
    "def Filter_data_clear_sky_days(data, clear_sky_days):\n",
    "    filtered_df = None\n",
    "    \n",
    "    for day in clear_sky_days:\n",
    "        tmp_df = data.loc[data['utc_tstamp'] > Convert_SA_time_to_UTC(day + \" 00:00:01\")]\n",
    "        tmp_df = tmp_df.loc[tmp_df['utc_tstamp'] < Convert_SA_time_to_UTC(day + \" 23:59:01\")]\n",
    "\n",
    "        if filtered_df is None:\n",
    "            filtered_df = tmp_df\n",
    "        else:\n",
    "            filtered_df = filtered_df.append(tmp_df, ignore_index=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def Convert_SA_time_to_UTC(sa_time):\n",
    "    timeFormat = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(sa_time, timeFormat)\n",
    "    sa_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    utc_time = pytz.utc\n",
    "    sa_moment = sa_local_time.localize(x, is_dst=None)\n",
    "    utc_time = sa_moment.astimezone(utc_time)\n",
    "    a = utc_time.strftime(timeFormat)\n",
    "    return a\n",
    "\n",
    "def Organise_sites(clear_sky_days, site_id_list, month, inverter_telemetry, site_details, cicuit_details):  # add gen data\n",
    "    \"\"\"organises all telemetry in the hierarchy: site->circuits->days_of_data\n",
    "\n",
    "    Args:\n",
    "    clear_sky_days (list) : List of clear sky days of a certain month\n",
    "    site_id_list (list) : List of site id with overvoltage datapoints\n",
    "    month (str) : month\n",
    "    inverter_telemetry (str) : Month in YYYY_MM format\n",
    "    site_details (df) : site_details data\n",
    "    cicuit_details (df) : circuit details data\n",
    "\n",
    "    Returns:\n",
    "    overall_site_organiser (dict) : Dict of all sites which have overvoltage datapoints. Keys are site_id,\n",
    "                                    values are site (a variable with object Site)\n",
    "    \n",
    "    Functions required:\n",
    "    Overall_site_organiser\n",
    "    \n",
    "    Side-Effects:\n",
    "    \n",
    "\n",
    "    Originally written by Tim for VWatt Curtailment Calculation.\n",
    "    \"\"\"\n",
    "    overall_site_organiser = {}\n",
    "\n",
    "    for site_id in site_id_list:\n",
    "        if site_id not in site_details.site_id.unique():\n",
    "            continue\n",
    "        overall_site_organiser[site_id] = Organise_individual_site(clear_sky_days, site_id, month, inverter_telemetry,\n",
    "                                        site_details.loc[site_details['site_id'] == site_id],\n",
    "                                        cicuit_details.loc[\n",
    "                                            cicuit_details['site_id'] == site_id])\n",
    "\n",
    "    return overall_site_organiser\n",
    "\n",
    "\n",
    "def Organise_individual_site(clear_sky_days, site_id, month, inverter_telemetry, site_details, cicuit_details):\n",
    "    \"\"\"filter D-PV data for only a certain date.\n",
    "\n",
    "    Args:\n",
    "    clear_sky_days (list) : List of clear sky days of a certain month\n",
    "    site_id (int) : site_id\n",
    "    month (str) : month in YYYY-MM format\n",
    "    inverter_telemetry (df) : D-PV time-series data\n",
    "    site_details (df) : site_details data\n",
    "    cicuit_details (df) : circuit details data\n",
    "\n",
    "    Returns:\n",
    "    site (Site class)\n",
    "    \n",
    "    Functions/Class Required:\n",
    "    Organise_individual_circuit\n",
    "    \"\"\"\n",
    "    \n",
    "    site = Site(site_id, site_details.iloc[0].s_postcode, site_details.iloc[0].pv_install_date,\n",
    "                site_details.iloc[0].ac_cap_w,\n",
    "                site_details.iloc[0].dc_cap_w, site_details.iloc[0].inverter_manufacturer,\n",
    "                site_details.iloc[0].inverter_model) #initiating an object of class Site\n",
    "\n",
    "    for row in cicuit_details.iterrows():\n",
    "        c_id = row[1].c_id #assigning the c_id\n",
    "        site.c_id_data[c_id] = Organise_individual_circuit(clear_sky_days, c_id, site_id, month,\n",
    "                                inverter_telemetry.loc[inverter_telemetry['c_id'] == c_id],\n",
    "                                row[1].con_type, row[1].polarity) #store the circuit data into variable site\n",
    "\n",
    "    return site\n",
    "\n",
    "\n",
    "def Organise_individual_circuit(clear_sky_days, c_id, site_id, month, inverter_telemetry, con_type, polarity):\n",
    "    \"\"\"filter D-PV data for only a certain date.\n",
    "\n",
    "    Args:\n",
    "    clear_sky_days (list) : List of clear sky days of a certain month\n",
    "    c_id (int) : c_id\n",
    "    site_id (int) : site_id\n",
    "    month (str) : month in YYYY-MM format\n",
    "    inverter_telemetry (df) : D-PV time-series data\n",
    "    con_type (str): Not sure about this. Seems like one of the column in circuit data\n",
    "    polarity (str): The polarity of the telemetry sensor, 1 or -1. Actual power is measured power x polarity.\n",
    "\n",
    "    Returns:\n",
    "    circuit (Circuit class): contains a D-PV data for a certain day for a certain circuit\n",
    "    \n",
    "    Functions/Class Required:\n",
    "    Circuit\n",
    "    Organise_individual_day\n",
    "    \"\"\"\n",
    "    \n",
    "    circuit = Circuit(c_id, site_id, con_type, polarity) #defining a variable circuit, having a class Circuit\n",
    "    inverter_telemetry['ts'] = inverter_telemetry.apply(lambda row: Convert_to_SA_time(row['utc_tstamp']), axis=1)\n",
    "\n",
    "\n",
    "    month_number = int(month.split('-')[1]) #not really sure what is this for..\n",
    "    for day in clear_sky_days:\n",
    "        #create a D-PV time-series data for a certain date for a certain site\n",
    "        circuit.day_data[day] = Organise_individual_day(day, inverter_telemetry) \n",
    "\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def Organise_individual_day(date, inverter_telemetry):\n",
    "    \"\"\"filter D-PV data for only a certain date.\n",
    "\n",
    "    Args:\n",
    "    date(str) : date\n",
    "    inverter_telemetry (df): D-PV time series data\n",
    "\n",
    "    Returns:\n",
    "    (df) : D-PV data filtered for a certain date, and sorted by its timestmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    inverter_telemetry = inverter_telemetry.loc[inverter_telemetry['ts'] > date + \" 00:00:01\"]\n",
    "    inverter_telemetry = inverter_telemetry.loc[inverter_telemetry['ts'] < date + \" 23:59:01\"]    \n",
    "    return inverter_telemetry.sort_values('ts', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='first', ignore_index=False, key=None)\n",
    "\n",
    "def Find_over_voltage_sites(v, clear_sky_data, cicuit_details):\n",
    "    \"\"\"Determine subsets of sites that experience over voltages to different extents for later selection\n",
    "\n",
    "    Args:\n",
    "        v (float): voltage value, but isn't what for. Seems useless.\n",
    "        clear_sky_data (df): D-PV time series data for clear sky day\n",
    "        cicuit_details (df): circuit details, consisting c_id and site_id.\n",
    "\n",
    "    Returns:\n",
    "        site_id_list_ov (dict) : dict, key is voltage limit, and value is list of overvoltage site_id\n",
    "    \"\"\"\n",
    "    site_id_list_ov = {}\n",
    "    \n",
    "    testVs = list(range(235, 256))\n",
    "    for i in testVs:\n",
    "        site_id_list_ov[i] = []\n",
    "\n",
    "    c_id_list = clear_sky_data.c_id.unique()\n",
    "\n",
    "    for c_id in c_id_list:\n",
    "        C_id_to_site_id(c_id, cicuit_details)\n",
    "\n",
    "    for c_id in c_id_list:\n",
    "\n",
    "        df = clear_sky_data.loc[clear_sky_data['c_id'] == c_id]\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "\n",
    "        df = df.loc[df['power'] > 0]\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "\n",
    "        maxV = max(df.voltage)\n",
    "\n",
    "        site_id = C_id_to_site_id(c_id, cicuit_details)\n",
    "\n",
    "        for i in testVs:\n",
    "            if maxV > i:\n",
    "                if c_id not in site_id_list_ov[i]:\n",
    "                    site_id_list_ov[i].append(site_id)\n",
    "\n",
    "    for i in testVs:\n",
    "        print(\"Length vMax > \" + str(i) + \": \" + str(len(site_id_list_ov[i])))\n",
    "\n",
    "    return site_id_list_ov\n",
    "\n",
    "# REUTRN THE SITE ID THAT CORRESPONDS TO A GIVEN CIRCUIT ID\n",
    "def C_id_to_site_id(c_id, cicuit_details):\n",
    "    return cicuit_details.loc[cicuit_details['c_id'] == c_id].iloc[0].site_id\n",
    "\n",
    "# CONVERT TIMESTAMP STRINGS FROM UTC TO LOCAL SOUTH AUSTRALIA TIME, TODO: ADJUST FOR TELEMETRY ANALYSIS IN OTHER CITIES\n",
    "def Convert_to_SA_time(utc_tstamp):\n",
    "    timeFormat1 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    timeFormat2 = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(utc_tstamp, timeFormat1)\n",
    "    adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    utc_time = pytz.utc\n",
    "    utc_moment = utc_time.localize(x, is_dst=None)\n",
    "    adelaide_local_time = utc_moment.astimezone(adelaide_local_time)\n",
    "    a = adelaide_local_time.strftime(timeFormat2)\n",
    "    return a\n",
    "\n",
    "def Assess_Volt_Watt_behaviour_site(site, clear_sky_days, overall_volt_watt_dict):\n",
    "    \"\"\"ASSESS AGGREGATED V-WATT DATA FOR A SITE\n",
    "\n",
    "    Args:\n",
    "        site (Site) : a Site object\n",
    "        clear_sky_days (list) : : list of clear sky days\n",
    "        overall_volt_watt_dict (dict) : keys are the c_id, values are dict -> keys are v, p, d, values are the values.\n",
    "\n",
    "    Returns:\n",
    "        None, but it will modify site object by appending circuit data into it.\n",
    "    \"\"\"\n",
    "    \n",
    "    for c_id in site.c_id_data.keys():\n",
    "        circuit = site.c_id_data[c_id]\n",
    "        Assess_Volt_Watt_behaviour_circuit(circuit, clear_sky_days, site.dc_cap_w, site.ac_cap_w, overall_volt_watt_dict)\n",
    "\n",
    "def Assess_Volt_Watt_behaviour_circuit(circuit, clear_sky_days, dc_cap_w, ac_cap_w, overall_volt_watt_dict):\n",
    "    \"\"\"Organize the filtered V and P data in a dictionary, used for all dates in the clear sky days.\n",
    "\n",
    "    Args:\n",
    "        circuit (Circuit) : a Circuit object\n",
    "        clear_sky_days (list) : : list of clear sky days\n",
    "        dc_cap_w (float) : tbh I thought this is the same with ac_cap_w, but seems different. Maybe it's max PV power\n",
    "        ac_cap_w (float) : inverter power capacity (watt)\n",
    "        overall_volt_watt_dict (dict) : keys are the c_id, values are dict -> keys are v, p, d, values are the values.\n",
    "\n",
    "    Returns:\n",
    "        None, but it will modify overall_volt_watt_dict by appending its values. v and p are points in the VWatt curve buffer.\n",
    "    \"\"\"\n",
    "    \n",
    "    for date in clear_sky_days:\n",
    "        voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray = Append_Volt_Watt_behaviour_data(circuit.day_data[date], circuit.c_id, date, ac_cap_w)\n",
    "        if voltArray is not None:\n",
    "            \n",
    "            Display_day(circuit.c_id, date, circuit.day_data[date], ac_cap_w, voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray)\n",
    "            if circuit.c_id not in overall_volt_watt_dict.keys():\n",
    "                overall_volt_watt_dict[circuit.c_id] = {\"v\": [], 'p': [], 'd': 0}\n",
    "\n",
    "            overall_volt_watt_dict[circuit.c_id]['v'] += voltArray\n",
    "            overall_volt_watt_dict[circuit.c_id]['p'] += relativeWattArray\n",
    "            overall_volt_watt_dict[circuit.c_id]['d'] += 1\n",
    "    print(\"Length of sites determined to be assessable: \" + str(len(overall_volt_watt_dict.keys())))\n",
    "\n",
    "def Append_Volt_Watt_behaviour_data(df, c_id, date, dc_cap_w):    \n",
    "    \"\"\"ORGANISE DATA FOR DETERMINING COMPLIANCE FUNCTION BELOW\n",
    "\n",
    "    Args:\n",
    "        df (df) : D-PV time series data\n",
    "        c_id (int) : c_id\n",
    "        date (str) : date\n",
    "        dc_cap_w (float) : inverter power capacity in watt\n",
    "\n",
    "    Returns:\n",
    "        voltArrayCompliance (list) : list of voltArray in the buffer range\n",
    "        relativeWattArrayCompliance (list) : list of relativeWattArray in the buffer range\n",
    "        filteredTimeArray (list) : list of time filtered by removing outliers\n",
    "        filteredPowerArray (list) : list of power filtered by removing outliers\n",
    "    \"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if len(df.index) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    if max(df.power) < 0.3:\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = SliceEndOffDF(df)\n",
    "\n",
    "    df = df.loc[df['power'] > 300]\n",
    "\n",
    "    if len(df.index) < 20:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Filter power data for only 'uncurtailed instances' (estimation as it is unknown when inverter is actively curtailing output)\n",
    "    powerArray, timeArray = FilterPowerData(df)\n",
    "\n",
    "    # Filter data for limited gradients, useful in creating more accurate polyfits, as determined by visual verification\n",
    "    powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "    if powerArray is None or len(powerArray) < 20:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Get polyfit estimation\n",
    "    polyfit = GetPolyfit(getDateTimeList(timeArray), powerArray, 2)\n",
    "\n",
    "    # Simple filter for very high and low values to aid in displaying data in figures\n",
    "    filteredPowerArray, filteredTimeArray = FilterArray(polyfit(getDateTime(df)), getDateTime(df), 100000, 0)\n",
    "    \n",
    "    filteredPowerArray = Change_W_to_kW(filteredPowerArray)\n",
    "    \n",
    "    maxPower = max(filteredPowerArray)\n",
    "    \n",
    "    # I have no idea what is this for\n",
    "    maxCompliance = 0\n",
    "    bestVWLimit = 248\n",
    "    bestTotalPoints = 1\n",
    "\n",
    "    # Determine which data points are of interest for compliance by comparing actual output vs polyfit predicted output, and voltage conditions\n",
    "    # Ie. W-Watt curtailment can only occur when P_modelled > P_max_allowed.\n",
    "    complianceCount, voltArrayCompliance, timeArrayCompliance, absoluteWattArrayCompliance, relativeWattArrayCompliance, successfulRelativeWattArray, successfulVoltArray = DetermineCompliance(\n",
    "        polyfit, df, dc_cap_w, 248)\n",
    "    maxVoltWattTimeArray, maxVoltWattPowerArray = getMaxVoltWattCurve(dc_cap_w, df, 249)\n",
    "\n",
    "\n",
    "    if len(voltArrayCompliance) > 0:\n",
    "        return voltArrayCompliance, relativeWattArrayCompliance, filteredTimeArray, filteredPowerArray\n",
    "        # I have no idea what is the use of \n",
    "        # complianceCount, voltArrayCompliance, timeArrayCompliance, absoluteWattArrayCompliance, relativeWattArrayCompliance, successfulRelativeWattArray, successfulVoltArray\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "def SliceEndOffDF(df):\n",
    "    if df is None or len(df.index) == 0:\n",
    "        return None\n",
    "\n",
    "    tmpDF = df.loc[df['power'] > 0]\n",
    "    if len(tmpDF.index) == 0:\n",
    "        return None\n",
    "\n",
    "    startTime = tmpDF.iloc[0].ts\n",
    "    endTime = tmpDF.iloc[len(tmpDF.index) - 1].ts\n",
    "\n",
    "    df = df.loc[df['ts'] > startTime]\n",
    "    df = df.loc[df['ts'] < endTime]\n",
    "\n",
    "    return df\n",
    "\n",
    "# FILTER POWER DATA TO INCLUDE ONLY INCREASING VALUES FROM EACH SIDES (WHERE SIDES ARE DETERMINED BY EITHER SIDE OF THE MAX POWER VALUE)\n",
    "def FilterPowerData(graphDF):\n",
    "    if len(graphDF.index) == 0:\n",
    "        return None, None\n",
    "\n",
    "    maxDailyPower = max(graphDF.power)\n",
    "\n",
    "    if len(graphDF.loc[graphDF['power'] == maxDailyPower].index) > 1:\n",
    "        return None, None\n",
    "\n",
    "    filterArray1 = []\n",
    "    filterArray2 = []\n",
    "    powerArray = graphDF.power\n",
    "    timeArray = graphDF.ts\n",
    "\n",
    "    halfFlag = True  # True is first half, False is second half\n",
    "    waterMark = 0\n",
    "\n",
    "    for currPower in powerArray:\n",
    "\n",
    "        # IF currPower IS GREATER THAN waterMark (LAST HIGHEST VALUE) THEN INCLUDE currPower AND INCREASE waterMark\n",
    "        if currPower > waterMark:\n",
    "            waterMark = currPower\n",
    "            filterArray1.append(True)\n",
    "        else:\n",
    "            filterArray1.append(False)\n",
    "\n",
    "        if currPower == maxDailyPower:\n",
    "            break\n",
    "\n",
    "    waterMark = 0\n",
    "\n",
    "    # PERFORM SAME FILTER ON SECOND SIDE OF POWER ARRAY\n",
    "    for currPower in powerArray.iloc[::-1]:\n",
    "\n",
    "        if currPower == maxDailyPower:\n",
    "            break\n",
    "\n",
    "        if currPower > waterMark:\n",
    "            waterMark = currPower\n",
    "            filterArray2.append(True)\n",
    "        else:\n",
    "            filterArray2.append(False)\n",
    "\n",
    "    # COMBINE TO FILTERED SIDES\n",
    "    filterArray2.reverse()\n",
    "    filterArray = filterArray1 + filterArray2\n",
    "    return powerArray[filterArray], timeArray[filterArray]\n",
    "\n",
    "def FilterDataLimitedGradients(powerArray, timeArray):\n",
    "    \"\"\"Filter the powerArray data so it includes only decreasing gradient (so the shape is parabolic)\n",
    "\n",
    "    Args:\n",
    "    powerArray (pd series): non curtailment filtered power data\n",
    "    timeArray (pd datetime): non curtailment filtered timestamp data\n",
    "\n",
    "    Returns:\n",
    "    powerArray (pd series): gradient filtered power data\n",
    "    timeArray (pd datetime): gradient filtered timestamp data\n",
    "    \n",
    "    Written by Tim\n",
    "    \"\"\"\n",
    "\n",
    "    if powerArray is None:\n",
    "        return None, None\n",
    "\n",
    "    # IN GENERAL ANLGE MUST BE BETWEEN THESE VALUES\n",
    "    angleLowerLimit = 80\n",
    "    angleUpperLimit = 90\n",
    "\n",
    "    # BUT AFTER 'continuanceLimit' CONTINUOUS VALUES HAVE BEEN ACCEPTED, THE LOWER ANGLE LIMIT IS RELAXED TO THIS VALUE BELOW\n",
    "    widerAngleLowerLimit = 70\n",
    "    continuanceLimit = 2\n",
    "\n",
    "    gradients = []\n",
    "    timeGradients = []\n",
    "    powerArray = powerArray.tolist()\n",
    "    timeArray = timeArray.tolist()\n",
    "    filterArray = []\n",
    "\n",
    "    n = len(powerArray)\n",
    "    gradientsCompliance = [0] * n\n",
    "\n",
    "    runningCount = 0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        g = abs(math.degrees(math.atan((powerArray[i] - powerArray[i - 1]) / (\n",
    "                    getSingleDateTime(timeArray[i]) - getSingleDateTime(timeArray[i - 1])))))\n",
    "\n",
    "        addFlag = False\n",
    "\n",
    "        if g > angleLowerLimit and g < angleUpperLimit:\n",
    "            addFlag = True\n",
    "            runningCount += 1\n",
    "\n",
    "        elif runningCount > continuanceLimit and g > widerAngleLowerLimit:\n",
    "            addFlag = True\n",
    "\n",
    "        else:\n",
    "            runningCount = 0\n",
    "\n",
    "        if addFlag:\n",
    "            gradientsCompliance[i - 1] += 1\n",
    "            gradientsCompliance[i] += 1\n",
    "\n",
    "        if g > 85:\n",
    "            gradients.append(g)\n",
    "            timeGradients.append(timeArray[i])\n",
    "\n",
    "    if gradientsCompliance[0] == 1 and gradientsCompliance[1] == 2:\n",
    "        filterArray.append(True)\n",
    "    else:\n",
    "        filterArray.append(False)\n",
    "\n",
    "    for i in range(1, n - 1):\n",
    "        if gradientsCompliance[i] == 2:\n",
    "            filterArray.append(True)\n",
    "        elif gradientsCompliance[i] == 1 and (gradientsCompliance[i - 1] == 2 or gradientsCompliance[i + 1] == 2):\n",
    "            filterArray.append(True)\n",
    "        else:\n",
    "            filterArray.append(False)\n",
    "\n",
    "    if gradientsCompliance[n - 1] == 1 and gradientsCompliance[n - 2] == 2:\n",
    "        filterArray.append(True)\n",
    "    else:\n",
    "        filterArray.append(False)\n",
    "    \n",
    "\n",
    "    powerArray = pd.Series(powerArray)\n",
    "    timeArray = pd.Series(timeArray)\n",
    "\n",
    "    powerArray = powerArray[filterArray]\n",
    "    timeArray = timeArray[filterArray]\n",
    "\n",
    "    return powerArray, timeArray\n",
    "\n",
    "# INTEGRATE POWER OUTPUT DATA OVER EACH DAY FOR COMPARISON WITH CURTAILMENT CALCUALTIONS\n",
    "def determine_total_energy_yields(month, monthly_data, site_organiser):\n",
    "    count = 0\n",
    "    for site in site_organiser.values():\n",
    "        \n",
    "        for c in site.c_id_data.values():\n",
    "            if c.c_id not in total_energy_yield_dict.keys():\n",
    "                total_energy_yield_dict[c.c_id] = {}\n",
    "            count += 1\n",
    "            print(\"count: \" + str(count))\n",
    "            total_energy_yield_dict[c.c_id][month] = calculate_months_energy_yield(c.c_id, monthly_data)\n",
    "            \n",
    "def calculate_months_energy_yield(c_id, monthly_data):\n",
    "    \"\"\"Itegrate power output data over each day for comparison with curtailment calcualtions.\n",
    "\n",
    "    Args:\n",
    "    c_id (int): c_id\n",
    "    monthly_data (df) : D-PV Time Series Data of a certain site, must contain power and time in utc with certain format\n",
    "\n",
    "    Returns:\n",
    "    MeasuredEnergy (float) : Amount of energy generation in that month in kWh\n",
    "\n",
    "    Originally written by Tim for VWatt Curtailment. May be applicable for daily application if the monthly data\n",
    "    is already filtered into only certain date.\n",
    "    \"\"\"\n",
    "    c_data = monthly_data.loc[monthly_data['c_id'] == c_id]\n",
    "\n",
    "    c_data['utc_tstamp'] = c_data.apply(lambda row: remove_tstamp_ms(row['utc_tstamp']), axis=1)\n",
    "    \n",
    "    c_data = c_data.sort_values('utc_tstamp', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='first', ignore_index=False, key=None)\n",
    "\n",
    "    powerData = c_data.power.tolist()\n",
    "    timeData = c_data.utc_tstamp.tolist()\n",
    "    MeasuredEnergy = AreaUnderCurve(timeData, powerData)/1000\n",
    "    return MeasuredEnergy  \n",
    "\n",
    "# REMOVING MILISECOND VALUE IN TIMESTAMP STRINGS\n",
    "def remove_tstamp_ms(tstamp_string):\n",
    "    timeFormat1 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    timeFormat2 = \"%Y-%m-%d %H:%M:%S\"\n",
    "    x = datetime.strptime(tstamp_string, timeFormat1)\n",
    "    return x.strftime(timeFormat2)\n",
    "\n",
    "def AreaUnderCurve(timeData, powerData):\n",
    "    \n",
    "    energy = 0\n",
    "    \n",
    "    for i in range(0, len(timeData) - 1):\n",
    "        t2 = ChangeToTimestamp(timeData[i+1])\n",
    "        t1 = ChangeToTimestamp(timeData[i])\n",
    "        \n",
    "        dt = t2-t1\n",
    "        \n",
    "        trapArea = (dt / 3600) * 0.5 * (powerData[i] + powerData[i+1])\n",
    "        energy += trapArea\n",
    "        \n",
    "    return energy\n",
    "\n",
    "def ChangeToTimestamp(timeString):\n",
    "    element = datetime.strptime(timeString,'%Y-%m-%d %H:%M:%S')\n",
    "    return datetime.timestamp(element)\n",
    "\n",
    "def getSingleDateTime(d):\n",
    "    \"\"\"CONVERT A SINGLE STRING TIMESTAMP TO DATETIME OBJECTS\n",
    "\n",
    "    Args:\n",
    "    d (str): string timestamp\n",
    "\n",
    "    Returns:\n",
    "    daetimeobject\n",
    "    \n",
    "    Written by Tim\n",
    "    \"\"\"\n",
    "    return md.date2num(datetime.strptime(d, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "def GetPolyfit(xArray, yArray, functionDegree):\n",
    "    \"\"\"GET POLYFIT OF DESIRED DEGREE, NEED xArray as float, not dt object\n",
    "\n",
    "    Args:\n",
    "    xArray (ndarray) : List of float unix timestamp\n",
    "    yArray (pd Series): List of power value corresponding to xArray time\n",
    "    functionDegree (int): Degree of polynomial. Quadratic functions means functionDegree = 2\n",
    "\n",
    "    Returns:\n",
    "    polyfit (np poly1d): polyfit model result, containing list of the coefficients and the constant.\n",
    "                        The first, second, and third values are coef of x^2, x, and the constant.\n",
    "    \n",
    "    Written by Tim\n",
    "    \"\"\"\n",
    "     \n",
    "\n",
    "    timestamps = xArray\n",
    "    xp = np.linspace(timestamps[0], timestamps[len(timestamps) - 1], 1000) #IDK what is this for. Seems redudant.\n",
    "    z = np.polyfit(timestamps, yArray, functionDegree)\n",
    "    polyfit = np.poly1d(z)\n",
    "\n",
    "    return polyfit\n",
    "\n",
    "def FilterArray(xArray, yArray, maxVal, minVal):\n",
    "    \"\"\"FILTER ARRAY TO INCLUDE VALUES WITHIN A CERTAIN RANGE\n",
    "\n",
    "    Args:\n",
    "        xArray (list) : list of the x values\n",
    "        yArray (list) : list of the y values\n",
    "        maxVal (float) : maximum x value for the filter\n",
    "        minVal (float) : minimum x value for the filter\n",
    "\n",
    "    Returns:\n",
    "        (pd series) : list of filtered x values\n",
    "        (pd series) : list of filtered y values\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_arr = []\n",
    "    for val in xArray:\n",
    "        if val > maxVal or val < minVal:\n",
    "            filter_arr.append(False)\n",
    "        else:\n",
    "            filter_arr.append(True)\n",
    "    # NOTE: conversion between series and lists was for conveniences of used filter operator, but could be adjusted for better time performance\n",
    "    xSeries = pd.Series(xArray)\n",
    "    ySeries = pd.Series(yArray)\n",
    "\n",
    "    return xSeries[filter_arr].tolist(), ySeries[filter_arr].tolist()\n",
    "\n",
    "def getDateTimeList(List):\n",
    "    \"\"\"CONVERT A LIST STRING TIMESTAMP TO DATETIME OBJECTS, THEN CONVERT IT TO FLOAT OF UNIX TIMESTAMPS.\n",
    "    \n",
    "    Args:\n",
    "    List (pd Series) : List of time in str. Example can be timeArray\n",
    "\n",
    "    Returns:\n",
    "    datenums (ndarray) : List of float unix timestamp\n",
    "    \n",
    "    This is used for polyfit preparation. Written by Tim\n",
    "    \"\"\"\n",
    "    # \n",
    "    dates = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in List]\n",
    "    datenums = md.date2num(dates)\n",
    "    return datenums\n",
    "\n",
    "# TRANSFORM A TIMESTAMP STRING INTO A TIMESTAMP INT VALUE (SECONDS SINCE 1970)\n",
    "def getDateTime(df):\n",
    "    dates = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in df.ts]\n",
    "    datenums = md.date2num(dates)\n",
    "    return datenums\n",
    "\n",
    "def Change_W_to_kW(filteredPowerArray):\n",
    "    l = []\n",
    "    for x in filteredPowerArray:\n",
    "        l.append(x/1000)\n",
    "        \n",
    "    return l\n",
    "\n",
    "# INDIVIDUAL DAY/SITE ANALYSIS \n",
    "def DetermineCompliance(polyfit, graphDF, maxPower, vwLimit):\n",
    "    \"\"\"Count how many datapoints are in the buffer range of the referenced VW curve.\n",
    "\n",
    "    Args:\n",
    "        polyfit (Polyfit) : a function to map timeSeries value to power vlaue\n",
    "        graphDF (df): time series D-PV data\n",
    "        maxPOwer (float) : maximum power\n",
    "        vwLimit (float) : a single value of the vwLimit that we want to investigate. Could be 235-255 V. This\n",
    "                            is the value where the maximum allowed power starts decreasing.\n",
    "\n",
    "    Returns:\n",
    "        complianceCount (int) : number of datapoints in the buffer range of the referenced VW curve\n",
    "        voltArray (list) : list of voltage value which experience VWatt curtailment \n",
    "                            (max expected power > max allowed power)\n",
    "        timeArray (list) : list of time value which experience VWatt curtailment\n",
    "        absoluteWattArray (list) : list of power value in watt which experience VWatt curtailment\n",
    "        relativeWattArray (list) : list of cf value (power / inverter power capacity), 0-1 \n",
    "                                    which experience VWatt curtailment\n",
    "        successfulRelativeWattArray (list) : list of relativeWattArray in the buffer range\n",
    "        successfulVoltArray (list) : list of voltArray in the buffer range\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    voltArray = []\n",
    "    timeArray = []\n",
    "    absoluteWattArray = []\n",
    "    relativeWattArray = []\n",
    "\n",
    "    successfulRelativeWattArray = []\n",
    "    successfulVoltArray = []\n",
    "\n",
    "    complianceArray = [] #not sure what is this for\n",
    "\n",
    "    # TODO: Changing to list aided with how analysis functions were created, should be kept as pd series and adjust analysis functions for better time performance\n",
    "    dfPower = graphDF.power.tolist()\n",
    "    dfTime = graphDF.ts.tolist()\n",
    "    dfVoltage = graphDF.voltage.tolist()\n",
    "\n",
    "    for i in range(len(dfPower)):\n",
    "\n",
    "        actualPower = dfPower[i]\n",
    "        voltage = dfVoltage[i]\n",
    "        timestamp = getSingleDateTime(dfTime[i])\n",
    "\n",
    "        # Expected power for the time of day\n",
    "        expectedPower = polyfit(timestamp)\n",
    "\n",
    "        # Expected max power based on volt-watt\n",
    "        maxVWPower = voltWattCurve(voltage, vwLimit) * maxPower\n",
    "\n",
    "        # CALCULATING THE AMOUNT OF OBSERVED CURTAILMENT\n",
    "        if maxVWPower < expectedPower:\n",
    "            voltArray.append(voltage)\n",
    "            timeArray.append(timestamp)\n",
    "\n",
    "            absoluteWattArray.append(actualPower)\n",
    "            relativeWattArray.append(actualPower / maxPower)\n",
    "\n",
    "    # Perform compliance count\n",
    "    complianceCount = 0\n",
    "    #I am not really sure about this, bcs in Tim's thesis the buffer is simply 0.07 kW for both\n",
    "    #the upper and the lower buffer\n",
    "    bufferHighVals = 0.03 * 1000 \n",
    "    bufferLowVals = 0.09 * 1000\n",
    "\n",
    "    # I have no ide why Tim marks below's code as a comment. \n",
    "    # for i in range(len(relativeWattArray)):\n",
    "    #\n",
    "    #     relativeWatt = relativeWattArray[i]\n",
    "    #     expectedWatt = voltWattCurve(voltArray[i], vwLimit)\n",
    "    #\n",
    "    #     if relativeWatt > 0.9:\n",
    "    #         if expectedWatt - bufferHighVals < relativeWatt < expectedWatt + bufferHighVals:\n",
    "    #             complianceCount += 1\n",
    "    #             successfulRelativeWattArray.append(relativeWatt)\n",
    "    #             successfulVoltArray.append(voltArray[i])\n",
    "    #\n",
    "    #     else:\n",
    "    #         if expectedWatt - bufferLowVals < relativeWatt < expectedWatt + bufferLowVals:\n",
    "    #             complianceCount += 1\n",
    "\n",
    "    return complianceCount, voltArray, timeArray, absoluteWattArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray\n",
    "\n",
    "def voltWattCurve(v, limit):\n",
    "    \"\"\"VOLT-WATT LIST BASED ON V3 INVERTER SETTING AND VOLTAGE INPUT\n",
    "\n",
    "    Args:\n",
    "        v (float): voltage value\n",
    "        limit (float): voltage value where the maximum allowed power starts decreasing. Could be 235-255 V.\n",
    "\n",
    "    Returns:\n",
    "        (float) : the maximum allowed cf (power/inverter capacity)\n",
    "    \"\"\"\n",
    "    \n",
    "    if v < limit:\n",
    "        return 1\n",
    "    if v < 265:\n",
    "        return (1 - 0.8 * (v - limit) / (265 - limit))\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def getMaxVoltWattCurve(maxPower, graphDF, vwLimit):\n",
    "    \"\"\"RETURNS THE MAXIMUM ALLOWED W/VA AND TIME LIST BASED ON AN INVERTER'S VOLTAGE DATA\n",
    "\n",
    "    Args:\n",
    "        maxPower (float) : maximum power value\n",
    "        graphDF (df) : D-PV time series data containing voltage, power, and time col\n",
    "        vwLimit (value) : voltage value when the maximum allowed power starts decreasing\n",
    "\n",
    "    Returns:\n",
    "        maxVoltWattTimeArray (list) : list of time\n",
    "        maxVoltWattPowerArray (list) : list of maximum allowed power (in kW) for time in maxVoltWattTimeArray\n",
    "    \"\"\"\n",
    "    maxVoltWattTimeArray = []\n",
    "    maxVoltWattPowerArray = []\n",
    "\n",
    "    # TODO: SHOULD BE CHANGED TO A COLUMN WISE FUNCTION FOR BETTER TIME PERFORMANCE\n",
    "    for row in graphDF.iterrows():\n",
    "        voltage = row[1].voltage\n",
    "\n",
    "        maxVoltWattTimeArray.append(getSingleDateTime(row[1].ts)) #convert to datetime object\n",
    "\n",
    "        maxVoltWattPowerArray.append(voltWattCurve(voltage, vwLimit) * maxPower / 1000) #obtain the max allowed voltage value\n",
    "\n",
    "    return maxVoltWattTimeArray, maxVoltWattPowerArray\n",
    "\n",
    "# GO THROUGH THE COMBINED VW BEHAVIOUR DATA FOR ALL SITES \n",
    "def Overall_volt_watt_assessment(overall_volt_watt_dict, complaincePercentageLimit, bufferHighVals, bufferLowVals): #buf \n",
    "    \"\"\"Assess the whole site for VWatt response and count VWatt, Non VWatt, and inconclusive.\n",
    "\n",
    "    Args:\n",
    "        overall_volt_watt_dict (dict) : a dict containing all sites in a clear sky day\n",
    "        complaincePercentageLimit (float) : threshold limit for VWatt response determination\n",
    "        bufferHighVals (float) : the amount of upper & lower buffer for the VW curve, used in low W/VA\n",
    "        bufferLowVals (float) : the amount of upper & lower buffer for the VW curve, used in high W/VA\n",
    "        \n",
    "    Returns:\n",
    "        None, but summarize the VWatt sites, Non VWatt sites, and inconclusive sites count.\n",
    "    \"\"\"\n",
    "    \n",
    "    bestVWLimit = 248\n",
    "    \n",
    "    countVW = 0\n",
    "    countNVW = 0\n",
    "    countNA = 0\n",
    "    \n",
    "    global site_id_dict\n",
    "    \n",
    "    # AGGREGATE RESULTS FOR STATISTICAL ANALYSIS\n",
    "    for c_id in overall_volt_watt_dict.keys():\n",
    "        if c_id not in overall_volt_watt_dict.keys():\n",
    "            continue\n",
    "        res = Site_volt_watt_assessment(c_id, overall_volt_watt_dict[c_id], complaincePercentageLimit, bufferHighVals, bufferLowVals)\n",
    "        '''\n",
    "        if res is None:\n",
    "            countNA += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"NA\"].append(c_id)\n",
    "            print(\"\\n!!! NOT ENOUGH POINTS !!!\\n\")\n",
    "            \n",
    "        elif res == True:\n",
    "            countVW += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"VW\"].append(c_id)\n",
    "            print(\"\\n!!! VOLT-WATT !!!\\n\")\n",
    "            \n",
    "        elif res == False:\n",
    "            countNVW += 1\n",
    "            buffers_site_id_dict[bufferLowVals][\"NVW\"].append(c_id)\n",
    "            print(\"\\n!!! NON-VOLT-WATT !!!\\n\")'''\n",
    "\n",
    "        if res is None:\n",
    "            countNA += 1\n",
    "            site_id_dict[\"NA\"].append(c_id)\n",
    "            print(\"\\n!!! NOT ENOUGH POINTS !!!\\n\")\n",
    "\n",
    "        elif res == True:\n",
    "            countVW += 1\n",
    "            site_id_dict[\"VW\"].append(c_id)\n",
    "            print(\"\\n!!! VOLT-WATT !!!\\n\")\n",
    "\n",
    "        elif res == False:\n",
    "            countNVW += 1\n",
    "            site_id_dict[\"NVW\"].append(c_id)\n",
    "            print(\"\\n!!! NON-VOLT-WATT !!!\\n\")\n",
    "    \n",
    "    totalSites = countVW + countNVW \n",
    "    \n",
    "    \n",
    "    if totalSites == 0: totalSites = 1\n",
    "    print(\"FOR4 buffer: \" + str(bufferLowVals))\n",
    "    print(\"\\n\\nVolt-Watt sites: \" + str(countVW) + \" = \" + str2(countVW/totalSites*100) + \"%\")\n",
    "    print(\"NON Volt-Watt sites: \" + str(countNVW) + \" = \" + str2(countNVW/totalSites*100) + \"%\")\n",
    "    print(\"Not enough points to assess: \" + str(countNA))\n",
    "    print(\"Total sites: \" + str(countNA + totalSites))\n",
    "    \n",
    "# ROUND TO 2DP AND STRINGIFY A FLOAT\n",
    "def str2(num):\n",
    "    return str(round(num, 2))\n",
    "\n",
    "# DISPLAY BOTH THE POWER/VOLTAGE vs TIME PLOT, AS WELL AS W/VA vs VOLTAGE\n",
    "def Display_day(c_id, date, df, dc_cap_w, voltArray, relativeWattArray, filteredTimeArray, filteredPowerArray):\n",
    "    \n",
    "    # Returns the maxmimum permitted real power output based on the inverter's voltage conditions\n",
    "    maxVoltWattTimeArray, maxVoltWattPowerArray = getMaxVoltWattCurve(dc_cap_w, df, 250)\n",
    "    \n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    plt.subplots_adjust(bottom=0.1)\n",
    "    plt.xticks(rotation=25)\n",
    "    ax = plt.gca()\n",
    "    xfmt = md.DateFormatter('%H:%M:%S')\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax.tick_params(axis='y', labelcolor='red')\n",
    "    lns1 = ax.plot(getDateTime(df), df.voltage, 'tomato', label='Local voltage')\n",
    "    plt.ylabel(\"Voltage (V)\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    plt.plot(maxVoltWattTimeArray,maxVoltWattPowerArray, 'limegreen')\n",
    "    plt.plot(filteredTimeArray, filteredPowerArray, 'blue')\n",
    "    \n",
    "    lns4 = ax2.plot(getDateTime(df), df.power/1000, 'skyblue', label='Real power')\n",
    "    plt.ylabel(\"Power (kW)\")\n",
    "    plt.title(\"c_id: \" + str(c_id) + \"   Date: \" + date + \"   DC cap: \" + str(dc_cap_w))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.title(\"c_id: \" + str(c_id) + \"   Date: \" + date + \"   DC cap: \" + str(dc_cap_w))\n",
    "\n",
    "    z = np.polyfit(voltArray, relativeWattArray, 1)\n",
    "\n",
    "    slope, intercept = np.polyfit(voltArray, relativeWattArray, 1)\n",
    "    \n",
    "    p = np.poly1d(z)\n",
    "    xRange = list(range(248,260))\n",
    "    \n",
    "    plt.plot(xRange,p(xRange),\"r--\")\n",
    "    \n",
    "    plt.plot(getSampleVoltages(230, 266), getWattsCurve(250), label='Best VW limit fit')\n",
    "    plt.plot(getSampleVoltages(250, 266), getWattsCurveBuffer(250, 0.05), label='Upper buffer')\n",
    "    plt.plot(getSampleVoltages(250, 266), getWattsCurveBuffer(250, -0.05), label='Lower buffer')\n",
    "\n",
    "    plt.scatter(voltArray, relativeWattArray, c=\"purple\", marker='.', label='Inverter data')\n",
    "    plt.xlabel(\"Voltage (V)\")\n",
    "    plt.ylabel(\"Power (p.u.)\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# RETURN A LIST OF NUMBERS WITHIN SPECIFIED RANGE\n",
    "def getSampleVoltages(a, b):\n",
    "    return list(range(a, b))\n",
    "\n",
    "# PRODUCES V-WATT REDUCTION CURVE FOR A SPECIFIC V-WATT LIMIT\n",
    "def getWattsCurve(vwLimit):\n",
    "    curve = []\n",
    "    vs = getSampleVoltages(230, 266)\n",
    "    for v in vs:\n",
    "        curve.append(voltWattCurve(v, vwLimit))\n",
    "    return curve\n",
    "\n",
    "# PRODUCES V-WATT REDUCTION CURVE FOR A SPECIFIC V-WATT LIMIT WITH A SPECIFIED BUFFER\n",
    "def getWattsCurveBuffer(vwLimit, buffer):\n",
    "    curve = []\n",
    "    vs = list(range(vwLimit, 266))\n",
    "    for v in vs:\n",
    "        curve.append(min([voltWattCurve(v, vwLimit) + buffer, 1]))\n",
    "    return curve\n",
    "\n",
    "def Site_volt_watt_assessment(c_id, site_volt_watt_dict, complaincePercentageLimit, bufferHighVals, bufferLowVals): #buf\n",
    "    \"\"\"Check VWatt behaviour of a certain site from with a certain threshold of number of points in the buffer range.\n",
    "\n",
    "    Args:\n",
    "        c_id (int) : c_id\n",
    "        site_volt_watt_dict (dict) : the key is site_id, the value is a dict with v, p, and d.\n",
    "        complaincePercentageLimit (float) : threshold limit for VWatt response determination\n",
    "        bufferHighVals (float) : the amount of upper & lower buffer for the VW curve, used in low W/VA\n",
    "        bufferLowVals (float) : the amount of upper & lower buffer for the VW curve, used in high W/VA\n",
    "  \n",
    "    Returns:\n",
    "        (bool) : True if VWatt, False if not VWatt, None if inconclusive due to \n",
    "                either not enough point or not enough overvoltage data.\n",
    "    \"\"\"\n",
    "    \n",
    "    bestComplianceCount = 0\n",
    "    bestCompliancePercentage = 0\n",
    "    bestVWLimit = None\n",
    "    bestVoltArray = None\n",
    "    bestRelativeWattArray = None\n",
    "    bestSuccessfulRelativeWattArray = None\n",
    "    bestSuccessfulVoltArray = None\n",
    "        \n",
    "    # BUFFER AND ANGLE SETTINGS FOR THE ANALYSIS\n",
    "    complainceCountLimit = 150\n",
    "    totalPointsCountLimit = 150\n",
    "    upperAngleLimit = -0.03 #I have no idea what is this for\n",
    "    lowerAngleLimit = -0.06 #I have no idea what is this for\n",
    "    \n",
    "    # VARIABLE TO CHECK IF THE ANALYSIS RAN OUT OF POINTS AT 256V OR BEFORE. \n",
    "    # IF AT 256V AND NO VW BEHAVIOUR IDENTIFIED THEN INCONCLUSIVE RESULT\n",
    "    notEnoughPointsV = 256\n",
    "    \n",
    "    print(\"\\n\\nc_id: \" + str(c_id))\n",
    "    for vwLimit in list(range(246,258)): #I am not sure why this is 246 until 257. V3 should be 235 to 255.\n",
    "        complianceCount, compliancePercentage, voltArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray = Determine_volt_watt_scatter_compliance(vwLimit, site_volt_watt_dict['v'], site_volt_watt_dict['p'], bufferHighVals, bufferLowVals)\n",
    "        if len(voltArray) == 0:\n",
    "            print(\"Ran out of points at VWLimit \" + str(vwLimit))\n",
    "            notEnoughPointsV = vwLimit\n",
    "            break\n",
    "        \n",
    "        # IF THE RESULT HAS HIGHER COMPLIANCE THAN PREVIOUS V THRESHOLD MEASURE, USE IT INSTEAD\n",
    "        if bestComplianceCount < complianceCount:\n",
    "            bestComplianceCount = complianceCount\n",
    "            bestVWLimit = vwLimit\n",
    "            bestTotalPoints = len(voltArray)\n",
    "            bestVoltArray = voltArray\n",
    "            bestRelativeWattArray = relativeWattArray\n",
    "            bestSuccessfulRelativeWattArray = successfulRelativeWattArray\n",
    "            bestSuccessfulVoltArray = successfulVoltArray\n",
    "            bestCompliancePercentage = compliancePercentage\n",
    "           \n",
    "    \n",
    "    if bestComplianceCount > 0:        \n",
    "        print(\"Best VWLimit: \" + str(bestVWLimit)) \n",
    "        \n",
    "    else:\n",
    "        print(\"No VWLimit results in any compliance\")\n",
    "    \n",
    "    if bestComplianceCount > 0 and bestTotalPoints > totalPointsCountLimit:\n",
    "        slope, intercept = np.polyfit(bestVoltArray, bestRelativeWattArray, 1)\n",
    "        print(\"Slope: \" + str(slope))\n",
    "        \n",
    "        \n",
    "        if bestComplianceCount > complainceCountLimit and bestCompliancePercentage > complaincePercentageLimit and lowerAngleLimit < slope and slope < upperAngleLimit:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            if notEnoughPointsV < 256:\n",
    "                return None\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def Determine_volt_watt_scatter_compliance(vwLimit, originalVoltArray, originalRelativeWattArray, bufferHighVals, bufferLowVals):\n",
    "    \"\"\"CHECKS EACH DATA POINT TO SEE IF IT FITS WITHIN THE NECESSARY BUFFER TO BE ADDED TO THE SUCCESSFUL DATAPOINT LIST.\n",
    "\n",
    "    Args:\n",
    "        vwLimit (float) : Voltage value where the maximum allowed power starts decreasing \n",
    "        originalVoltArray (list) : List of all voltage which is curtailed, ie expected power is higher than maximum allowed power\n",
    "        originalRelativeWattArray (list) : List of all relative power which is curtailed\n",
    "        bufferHighVals (float) : the amount of upper & lower buffer for the VW curve, used in low W/VA\n",
    "        bufferLowVals (float) : the amount of upper & lower buffer for the VW curve, used in high W/VA\n",
    "\n",
    "    Returns:\n",
    "        complianceCount (int) : number of points falling in the buffer range\n",
    "        compliancePercentage (int) : percentage of points falling in the buffer range\n",
    "        voltArray (list) : filtered voltage value throwing away outlier\n",
    "        relativeWattArray (list) : filtered relative power\n",
    "        successfulRelativeWattArray (list) : relative power in the VW curve buffer range\n",
    "        successfulVoltArray (list) : voltage in the VW curve buffer range\n",
    "    \"\"\"\n",
    "    \n",
    "    complianceCount = 0\n",
    "    successfulRelativeWattArray = []\n",
    "    successfulVoltArray = []\n",
    "\n",
    "    # FILTER DATA TO ONLY EXAMINE VALUES HIGHER THAN THE VW LIMIT (AND LOWER THAN 1000, USED AS FilterArray FUNCTION IS USED ELSEWHERE)\n",
    "    voltArray, relativeWattArray = FilterArray(originalVoltArray, originalRelativeWattArray, 1000, vwLimit)\n",
    "\n",
    "    for i in range(len(relativeWattArray)):\n",
    "\n",
    "        relativeWatt = relativeWattArray[i]\n",
    "        expectedWatt = voltWattCurve(voltArray[i], vwLimit)\n",
    "\n",
    "        # FOR HIGHER W/VA USE A SMALLER BUFFER, AS THESE VALUES ARE MORE LIKELY TO SUFFER RANDOM VARIATIONS\n",
    "        if relativeWatt > 0.9:\n",
    "            if expectedWatt - bufferHighVals < relativeWatt < expectedWatt + bufferHighVals:\n",
    "                complianceCount += 1\n",
    "                successfulRelativeWattArray.append(relativeWatt)\n",
    "                successfulVoltArray.append(voltArray[i])\n",
    "\n",
    "        # FOR LOWER W/VA USE A LARGER BUFFER, AS THESE VALUES ARE LESS LIKELY TO SUFFER RANDOM VARIATIONS\n",
    "        else:\n",
    "            if expectedWatt - bufferLowVals < relativeWatt < expectedWatt + bufferLowVals:\n",
    "                complianceCount += 1\n",
    "                successfulRelativeWattArray.append(relativeWatt)\n",
    "                successfulVoltArray.append(voltArray[i])\n",
    "\n",
    "    compliancePercentage = 0\n",
    "    if len(voltArray) > 0:\n",
    "        compliancePercentage = complianceCount/len(voltArray)\n",
    "    return complianceCount, compliancePercentage, voltArray, relativeWattArray, successfulRelativeWattArray, successfulVoltArray\n",
    "\n",
    "# QUANTIFY CURTAILMENT FUR TO V-WATT FOR SITES THAT HAVE ALREADY BEEN IDENTIFIED AS HAVING V-WATT ENABLED: FOR EACH DAY OF A CIRCUIT'S DATA\n",
    "def Assess_curtailment_day(df, c_id, date, dc_cap_w):    \n",
    "\n",
    "    if df is None:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    if len(df.index) == 0:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    if max(df.power) < 0.3:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    df = SliceEndOffDF(df)\n",
    "\n",
    "    df = df.loc[df['power'] > 300]\n",
    "\n",
    "    if len(df.index) < 20:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    powerArray, timeArray = FilterPowerData(df)\n",
    "\n",
    "    powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "    if powerArray is None or len(powerArray) < 20:\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    polyfit = GetPolyfit(getDateTimeList(timeArray), powerArray, 2)\n",
    "\n",
    "    filteredPowerArray, filteredTimeArray = FilterArray(polyfit(getDateTime(df)), getDateTime(df), 100000, 0)\n",
    "\n",
    "    filteredPowerArray = Change_W_to_kW(filteredPowerArray)\n",
    "    \n",
    "    maxPower = max(filteredPowerArray)\n",
    "\n",
    "    graphDF = df.loc[df['power'] > 0.1 * maxPower]\n",
    "    powerData = graphDF.power.tolist()\n",
    "    timeData = graphDF.ts.tolist()\n",
    "    \n",
    "    powerExpected = GetExpectedPower(timeData, polyfit)\n",
    "    \n",
    "    MeasuredEnergy = AreaUnderCurve(timeData, powerData)\n",
    "    ExpectedEnergy = AreaUnderCurve(timeData, powerExpected)\n",
    "    \n",
    "\n",
    "    Curtailment = ExpectedEnergy - MeasuredEnergy\n",
    "    if Curtailment < 0.01:\n",
    "        Curtailment = 0\n",
    "    \n",
    "    maxCompliance = 0\n",
    "    bestVWLimit = 248\n",
    "    bestTotalPoints = 1\n",
    "    complianceCount, voltArrayCompliance, timeArrayCompliance, absoluteWattArrayCompliance, relativeWattArrayCompliance, successfulRelativeWattArray, successfulVoltArray = DetermineCompliance(polyfit, df, dc_cap_w, 249)\n",
    "    maxVoltWattTimeArray, maxVoltWattPowerArray = getMaxVoltWattCurve(dc_cap_w, df, 249)\n",
    "\n",
    "    \n",
    "    if len(voltArrayCompliance) > 0:\n",
    "        return voltArrayCompliance, relativeWattArrayCompliance, filteredTimeArray, filteredPowerArray, Curtailment/1000, ExpectedEnergy/1000\n",
    "\n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "# RETURN THE EXPECTED POWER DATA A SPECIFIC TIMESTAMP ACCORDING TO A GIVEN POLYFIT\n",
    "def GetExpectedPower(timeData, polyfit):\n",
    "    \n",
    "    expectedPower = []\n",
    "    \n",
    "    for t in timeData:\n",
    "        expectedPower.append(polyfit(getSingleDateTime(t)))\n",
    "        \n",
    "    return expectedPower\n",
    "\n",
    "def check_polyfit(data_site):\n",
    "    \"\"\"Filter the power data, do polyfit, check its quality, and calculate expected energy generated.\n",
    "\n",
    "    Args:\n",
    "        data_site (df): Cleaned D-PV time-series data\n",
    "\n",
    "    Returns:\n",
    "        polyfit (polyfit) : function to transform map timestamp into expected power without curtailment\n",
    "        is_good_polyfit_quality (bool) : True only if more than 50 actual points are near to polyfit result\n",
    "        energy_generated (float) : calculated energy generated\n",
    "        energy_generated_expected (float): calculated expected generated energy from the polyfit \n",
    "        data_site (df): data_site with expected power column\n",
    "    \"\"\"\n",
    "    data_site.index.rename('ts', inplace = True)\n",
    "\n",
    "    sunrise, sunset, data_site = filter_sunrise_sunset(data_site)\n",
    "    data_site['power_relative'] = data_site['power'] / ac_cap\n",
    "    timestamp_complete = data_site.index\n",
    "    data_site_more_300 = data_site.loc[data_site['power'] > 300]\n",
    "\n",
    "    powerArray, timeArray = filter_power_data(data_site_more_300)\n",
    "    timeArray = timeArray.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    timeArray = timeArray.to_series(index=None, name='None')\n",
    "    powerArray, timeArray = FilterDataLimitedGradients(powerArray, timeArray)\n",
    "\n",
    "    timeArrayFloat = getDateTimeList(timeArray)\n",
    "\n",
    "    polyfit = GetPolyfit(timeArrayFloat, powerArray, 2)\n",
    "\n",
    "    polyfitPowerArray = polyfit(timeArrayFloat)\n",
    "\n",
    "    timestamp = timestamp_complete\n",
    "    timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    timestamp = getDateTimeList(timestamp)\n",
    "    data_site['power_expected'] = polyfit(timestamp)\n",
    "    data_site.loc[data_site['power_expected']<0, 'power_expected'] = 0\n",
    "\n",
    "    #plt.plot(data_site.index, data_site['power'])\n",
    "    #plt.plot(data_site.index, data_site['power_expected'])\n",
    "    #plt.show()\n",
    "\n",
    "    error = abs(data_site['power_expected'] - data_site['power'])\n",
    "    points_near_polyfit_count = error[error<50].count()\n",
    "\n",
    "    if points_near_polyfit_count > 50: #the initial value is 50\n",
    "        is_good_polyfit_quality = True\n",
    "    else:\n",
    "        is_good_polyfit_quality = False\n",
    "    \n",
    "    return data_site, polyfit, is_good_polyfit_quality\n",
    "\n",
    "def check_overvoltage_avail(data_site):\n",
    "    '''Check whether the maximum voltage of the data is higher than the minimum Vlimit stated in AS/NZS 4777.2\n",
    "    \n",
    "    Args:\n",
    "        data_site (df): Cleaned D-PV time-series data\n",
    "\n",
    "    Returns:\n",
    "        is_overvoltage_avail (bool) : True only if the maximum voltage of the data is higher \n",
    "                                        than the minimum Vlimit stated in AS/NZS 4777.2\n",
    "    '''\n",
    "    \n",
    "    max_voltage = data_site['voltage'].max()\n",
    "    min_Vlimit = 235\n",
    "    if max_voltage > min_Vlimit:\n",
    "        is_overvoltage_avail = True\n",
    "    else:\n",
    "        is_overvoltage_avail = False\n",
    "    return is_overvoltage_avail\n",
    "\n",
    "def check_energy_curtailed(vwatt_data):\n",
    "    \"\"\"Calculation of the amount of energy curtailed only in the VWatt curtailment period (expected power > max allowed power from VWatt curve).\n",
    "\n",
    "    Args:\n",
    "        vwatt_data (df): a time series D-PV data with power and power expected columns, only in curtailment period.\n",
    "\n",
    "    Returns:\n",
    "        vwatt_curt_energy (float): the curtailed energy because of VWatt, in kWh.\n",
    "    \"\"\"\n",
    "    \n",
    "    energy_generated_expected = vwatt_data['power_expected'].resample('h').mean().sum()/1000\n",
    "    energy_generated = vwatt_data['power'].resample('h').mean().sum()/1000\n",
    "    vwatt_curt_energy = energy_generated_expected - energy_generated\n",
    "    return vwatt_curt_energy\n",
    "    \n",
    "    \n",
    "def check_vwatt_response(data_site):\n",
    "    \"\"\"Check whether the inverter shows vwatt response or not.\n",
    "    \n",
    "    This function will be done in a loop over Vlimit 235 - 255 V.\n",
    "    Steps:\n",
    "    1. Make a power limit value based on VW curve\n",
    "    2. Filter voltage and power, which is curtailed (expected power from polyfit is higher than allowed voltage)\n",
    "    3. Count the percentage of datapoints from previous step in the buffer range of VWatt curve\n",
    "    4. If the percentage from the previous step is higher than certain limit, we say it shows VWatt response.\n",
    "\n",
    "    Args:\n",
    "        data_site (df) : D-PV time series data\n",
    "        polyfit(polyfit): a funciton to map timestamp value to expected power without curtailment\n",
    "\n",
    "    Returns:\n",
    "        vwatt_response (str) : Yes, None, or Inconclusive due to insufficient overvoltage datapoint.\n",
    "        \n",
    "    TODO: \n",
    "    1. Reassess whether it is necessary to determine VWatt using count and gradient threshold\n",
    "    2. Test for non VWatt sample & inconclusive sample\n",
    "    \"\"\"\n",
    "    \n",
    "    #for Vlimit in list(range (246, 258)): #This is from Tim. Tim's range is different, which IDK why.\n",
    "    best_percentage = 0 #initiation\n",
    "    for Vlimit in list(range (235, 256)):\n",
    "        #step 1. Make a power limit value based on VW curve\n",
    "        data_site['power_limit_vw'] = data_site['voltage'].apply(voltWattCurve, limit = Vlimit) * ac_cap\n",
    "\n",
    "        #step 2. Filter voltage and power, which is curtailed (expected power from polyfit is higher than allowed voltage)\n",
    "        suspect_data_filter = data_site['power_limit_vw'] < data_site['power_expected'] \n",
    "        suspect_data = pd.DataFrame()\n",
    "        suspect_data = data_site[suspect_data_filter].copy()\n",
    "\n",
    "        #step 3. Count the percentage of datapoints from previous step in the buffer range of VWatt curve\n",
    "        \n",
    "        #create the buffer range\n",
    "        BUFFER_HIGH_VAL =  150 #This is from Tim's thesis. In Tim's program the used value is 0.035 * ac_cap but IDK it doesn't work well.\n",
    "        BUFFER_LOW_VAL = 150  #This is from Tim's thesis. In Tim's program the used value is 0.08 * ac_cap but IDK it doesn't work well.\n",
    "        buffer_high_filter = suspect_data['power_relative'] > 0.9\n",
    "        buffer_low_filter = ~buffer_high_filter\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None  # default='warn'\n",
    "        suspect_data.loc[buffer_high_filter, 'power_limit_upper'] = suspect_data['power_limit_vw'] + BUFFER_HIGH_VAL\n",
    "        suspect_data.loc[buffer_high_filter, 'power_limit_lower'] = suspect_data['power_limit_vw'] - BUFFER_HIGH_VAL\n",
    "    \n",
    "        suspect_data.loc[buffer_low_filter, 'power_limit_upper'] = suspect_data['power_limit_vw'] + BUFFER_LOW_VAL\n",
    "        suspect_data.loc[buffer_low_filter, 'power_limit_lower'] = suspect_data['power_limit_vw'] - BUFFER_LOW_VAL\n",
    "        \n",
    "        #count points in buffer\n",
    "        is_low_ok = suspect_data['power_limit_lower'] < suspect_data['power']\n",
    "        is_upp_ok = suspect_data['power'] < suspect_data['power_limit_upper']\n",
    "        suspect_data['is_in_buffer_range'] = is_low_ok & is_upp_ok\n",
    "        count_in_buffer_range = suspect_data['is_in_buffer_range'].values.sum() #count true in a col\n",
    "        percentage_in_buffer_range = count_in_buffer_range / len(suspect_data.index) * 100\n",
    "        \n",
    "        #put the best VWLimit stats\n",
    "        if percentage_in_buffer_range > best_percentage or best_percentage == 0:\n",
    "            best_percentage = percentage_in_buffer_range\n",
    "            best_Vlimit = Vlimit\n",
    "            \n",
    "    #step 4. If the percentage from the previous step is higher than certain limit, we say it shows VWatt response.\n",
    "    PERCENTAGE_THRESHOLD = 84\n",
    "    #print(best_percentage)\n",
    "    #print (best_Vlimit)\n",
    "    if best_percentage > PERCENTAGE_THRESHOLD: #Tim uses count threshold and gradient threshold. I am not sure whether it is necessary.\n",
    "        vwatt_response = 'Yes'\n",
    "        vwatt_curt_energy = check_energy_curtailed(suspect_data)\n",
    "    elif suspect_data['voltage'].max() < 255:\n",
    "        vwatt_response = 'Inconclusive due to insufficient data points'\n",
    "        vwatt_curt_energy = float('nan')\n",
    "    else: #no Vlimit results a good fit in all possible Vlimit value\n",
    "        vwatt_response = 'None'\n",
    "        vwatt_curt_energy = 0\n",
    "            \n",
    "    return vwatt_response, vwatt_curt_energy\n",
    "\n",
    "def check_vwatt_curtailment(data_site, date):\n",
    "    #check if clear sky day. This contains redundant steps like making ghi dict for all days etc, can still be improved.\n",
    "    is_clear_sky_day = check_clear_sky_day(date) \n",
    "\n",
    "    if not is_clear_sky_day:\n",
    "        vwatt_response = 'Inconclusive due to non clear sky day.'\n",
    "        vwatt_curt_energy = float('nan')\n",
    "        print('Not clear sky day')\n",
    "        return vwatt_response, vwatt_curt_energy\n",
    "\n",
    "    #check the polyfit quality\n",
    "    data_site, polyfit, is_good_polyfit_quality = check_polyfit(data_site)\n",
    "\n",
    "    if not is_good_polyfit_quality:\n",
    "        vwatt_response = 'Inconclusive due to poor power data'\n",
    "        vwatt_curt_energy = float('nan')\n",
    "        print('Polyfit quality is not good enough')\n",
    "        return vwatt_response, vwatt_curt_energy\n",
    "\n",
    "    #check overvoltage sufficiency\n",
    "    is_overvoltage_avail = check_overvoltage_avail(data_site)\n",
    "\n",
    "    if not is_overvoltage_avail:\n",
    "        vwatt_response = 'Inconclusive due to insufficient overvoltage datapoint.'\n",
    "        vwatt_curt_energy = float('nan')\n",
    "        print('No voltage point over 235 V')\n",
    "        return vwatt_response, vwatt_curt_energy\n",
    "\n",
    "    #check vwatt-response here\n",
    "    vwatt_response, vwatt_curt_energy = check_vwatt_response(data_site)\n",
    "    \n",
    "    return vwatt_response, vwatt_curt_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a812d60",
   "metadata": {},
   "source": [
    "# MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e9fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_general_files(file_path):\n",
    "    circuit_details = pd.read_csv(file_path + r\"/unsw_20190701_circuit_details.csv\")\n",
    "    site_details = pd.read_csv (file_path + r\"/unsw_20190701_site_details.csv\")\n",
    "    site_details = site_details.merge(circuit_details, left_on = 'site_id', right_on = 'site_id')\n",
    "    unique_cids = pd.read_csv(file_path + r\"/UniqueCids.csv\", index_col = 0)\n",
    "    return site_details, unique_cids\n",
    "\n",
    "def input_monthly_files(file_path, data_date_idx):\n",
    "    \"\"\"Open time-series D-PV data and ghi data of a certain month. Only compatible for SoLA data format.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file location of the data\n",
    "        data_date_idx (str): The month of the files in format 'YYYYMM' eg '201907'\n",
    "\n",
    "    Returns:\n",
    "        data (df): the opened & cleaned time-series D-PV data\n",
    "        ghi (df): the opened & cleaned ghi data\n",
    "        data_ori (df): the opened & unmodified time-series D-PV data\n",
    "        ghi_ori (df): the opened & unmodified ghi data\n",
    "        \n",
    "    Originally written by Baran for VVAr Curtailment Calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = file_path + r\"/processed_unsw_\" + data_date_idx + '_data_raw.csv'\n",
    "    data_ori = pd.read_csv(data_path)\n",
    "    data = data_ori.set_index('utc_tstamp')\n",
    "\n",
    "    # Convert timestamp to local Adelaide time\n",
    "    data.index = pd.to_datetime(data.index) # convert index from object type to datetime\n",
    "    Adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "    data.index = data.index.tz_localize(pytz.utc).tz_convert(Adelaide_local_time) # convert utc to local adelaide time\n",
    "    data.index.rename('Timestamp', inplace = True)\n",
    "\n",
    "    # Load GHI data\n",
    "    ghi_date_idx = data_date_idx[0:4] + '_' + data_date_idx[4:]\n",
    "    ghi_path = file_path + r\"/sl_023034_\" + ghi_date_idx +'.txt'\n",
    "    ghi = pd.read_csv (ghi_path) \n",
    "    ghi_ori = ghi.copy()\n",
    "\n",
    "    ghi['timestamp'] = pd.to_datetime(pd.DataFrame ({'year' : ghi['Year Month Day Hours Minutes in YYYY'].values, \n",
    "                                                    'month' : ghi['MM'], \n",
    "                                                    'day' : ghi['DD'], \n",
    "                                                   'hour' : ghi['HH24'], \n",
    "                                                   'minute' : ghi['MI format in Local standard time']}))\n",
    "    ghi.set_index('timestamp', inplace = True)\n",
    "    # Deal with the space characters (ghi is in object/string form at the moment)\n",
    "    ghi['Mean global irradiance (over 1 minute) in W/sq m'] = [float(ghi_t) if ghi_t.count(' ')<= 3 else np.nan for ghi_t in ghi['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    return data, ghi, data_ori, ghi_ori\n",
    "\n",
    "def filter_date(data, date):\n",
    "    date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    data = data[data.index.date == date_dt] #focus only on the date\n",
    "    return data\n",
    "\n",
    "def summarize_result_into_dataframe(c_id, date, energy_generated, is_clear_sky_day, tripping_response, tripping_curt_energy, vvar_response, vvar_curt_energy, vwatt_response, vwatt_curt_energy):\n",
    "    summary = pd.DataFrame({\n",
    "        'c_id' : [c_id],\n",
    "        'date' : [date],\n",
    "        'energy generated (kWh)' : [energy_generated],\n",
    "        'clear sky day': [is_clear_sky_day],\n",
    "        'tripping response' : [tripping_response],\n",
    "        'tripping curtailment (kWh)' : [tripping_curt_energy],\n",
    "        'V-VAr response' : [vvar_response],\n",
    "        'V-VAr curtailment (kWh)' : [vvar_curt_energy],\n",
    "        'V-Watt response' : [vwatt_response],\n",
    "        'V-Watt curtailment (kWh)' : [vwatt_curt_energy]\n",
    "    })\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13c88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GHI data\n",
    "def read_ghi(file_path, ghi_filename):\n",
    "    ghi_path = file_path + ghi_filename\n",
    "    ghi = pd.read_csv (ghi_path) \n",
    "    ghi_ori = ghi.copy()\n",
    "\n",
    "    ghi['timestamp'] = pd.to_datetime(pd.DataFrame ({'year' : ghi['Year Month Day Hours Minutes in YYYY'].values, \n",
    "                                                    'month' : ghi['MM'], \n",
    "                                                    'day' : ghi['DD'], \n",
    "                                                   'hour' : ghi['HH24'], \n",
    "                                                   'minute' : ghi['MI format in Local standard time']}))\n",
    "    ghi.set_index('timestamp', inplace = True)\n",
    "    # Deal with the space characters (ghi is in object/string form at the moment)\n",
    "    ghi['Mean global irradiance (over 1 minute) in W/sq m'] = [float(ghi_t) if ghi_t.count(' ')<= 3 else np.nan for ghi_t in ghi['Mean global irradiance (over 1 minute) in W/sq m']]\n",
    "    return ghi, ghi_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbac8f3",
   "metadata": {},
   "source": [
    "# SAMPLE DETAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b732cf",
   "metadata": {},
   "source": [
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\" #for running in CEEM 09 computer at TETB <br>\n",
    "#file_path = r\"/Users/samhan/Downloads/data\" #for running in Samhan's personal laptop <br>\n",
    "\n",
    "sample 1 <br>\n",
    "date = '2019-09-03' #non clear sky day, good sample for tripping <br>\n",
    "c_id = 1317822057 #this is site with tripping response on 2019-09-03 <br>\n",
    "\n",
    "sample 2 <br>\n",
    "date = '2019-07-20' #clear sky day, good sample for vvar with c_id = 1018350709 <br>\n",
    "c_id = 1018350709 #this is row 436 in the site id, and having the biggest vvar curtailment on 2019-07-20 <br>\n",
    "\n",
    "sample 3 <br>\n",
    "date = '2019-08-14' #clear sky day with vwatt response <br>\n",
    "c_id = 466930914 #this is c_id having vwatt enabled. c_id: 466930914 / site_id 1641159822, date: 2019-08-14 <br>\n",
    "\n",
    "sample 4 <br>\n",
    "date = '2020-04-13' #clear sky day <br>\n",
    "c_id = 1165442853 #VWatt enabled, very clear. Works well even for the current polyfit algorithm. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10826e15",
   "metadata": {},
   "source": [
    "# TRIAL FOR ONLY 1 SITE AND # 1 DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb098fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = r\"/Users/samhan/Downloads/data\" #this is for running in Samhan's laptop\n",
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\" #for running in TETB CEEM09 computer\n",
    "site_details, unique_cids= input_general_files(file_path)\n",
    "sample_number = 4 #available is 1 - 4 for now.\n",
    "sample_filename = '/data_sample_{}.csv'.format(sample_number)\n",
    "ghi_filename = '/ghi_sample_{}.csv'.format(sample_number)\n",
    "\n",
    "data = pd.read_csv(file_path + sample_filename)\n",
    "# ghi, ghi_ori = read_ghi(file_path, ghi_filename)\n",
    "ghi = pd.read_csv(file_path + ghi_filename, index_col = 0)\n",
    "ghi.index = pd.to_datetime(ghi.index)\n",
    "pd.to_datetime(data['Timestamp'].str.slice(0, 19, 1))\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'].str.slice(0, 19, 1))\n",
    "data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "c_id = data['c_id'][0]\n",
    "date = str(data.index[0])[:10]\n",
    "\n",
    "data_site, ac_cap, dc_cap, eff_system, inverter = site_organize(c_id, site_details, data, unique_cids)\n",
    "\n",
    "is_clear_sky_day = check_clear_sky_day(date)\n",
    "energy_generated = check_energy_generated(data_site, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e237e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIPPING\n",
    "tripping_response, tripping_curt_energy, estimation_method = check_tripping_curtailment(is_clear_sky_day, c_id, data_site, unique_cids, ac_cap, site_details, date)\n",
    "\n",
    "#VVAR, VWATT, SUMMARY\n",
    "vvar_response, vvar_curt_energy = check_vvar_curtailment(c_id, date, data_site, ghi, ac_cap, dc_cap, eff_system, is_clear_sky_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbdaf329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vwatt_response, vwatt_curt_energy  = check_vwatt_curtailment(data_site, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b868966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_id</th>\n",
       "      <th>date</th>\n",
       "      <th>energy generated (kWh)</th>\n",
       "      <th>clear sky day</th>\n",
       "      <th>tripping response</th>\n",
       "      <th>tripping curtailment (kWh)</th>\n",
       "      <th>V-VAr response</th>\n",
       "      <th>V-VAr curtailment (kWh)</th>\n",
       "      <th>V-Watt response</th>\n",
       "      <th>V-Watt curtailment (kWh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1165442853</td>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>31.842794</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.678549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         c_id        date  energy generated (kWh)  clear sky day  \\\n",
       "0  1165442853  2020-04-13               31.842794           True   \n",
       "\n",
       "  tripping response  tripping curtailment (kWh) V-VAr response  \\\n",
       "0              None                           0            Yes   \n",
       "\n",
       "   V-VAr curtailment (kWh) V-Watt response  V-Watt curtailment (kWh)  \n",
       "0                      0.0             Yes                  3.678549  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarize_result_into_dataframe(c_id, date, energy_generated, is_clear_sky_day, tripping_response, tripping_curt_energy, vvar_response, vvar_curt_energy, vwatt_response, vwatt_curt_energy)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32d44a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_site['power'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc6d89d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25af64d43d0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_site['reactive_power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f91c63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polyfit_sample = pd.DataFrame(data_site['power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a776af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\"\n",
    "polyfit_sample.to_csv(file_path + '/sample_4_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a700f",
   "metadata": {},
   "source": [
    "# BUGS LIST\n",
    "## IMPORTANT\n",
    "1. Naomi's calculation of generated energy (kWh) daily is different from Baran's (probably dataset issue? cleaning issue?)\n",
    "2. Tim's polyfit sometime underestimate the expected generation without curtailment, so the curtailed energy is always negative\n",
    "3. No sunrise and sunset data from Baran's data, while it is used in Naomi's code\n",
    "4. Trippping curtailment calculated using Baran's dataset (processed_unsw_201909_data_raw) is different from using Naomi's dataset (which I don't have) so I can't test\n",
    "5. How to make sure that estimated tripping_curt_energy doesn't include VVAr and VWatt?\n",
    "6. Incomplete dataset? Only midnight data, eg on 2020-03-01, c_id = 1317822057\n",
    "7. Tripping case not detected by Naomi's script? eg on 2020-03-02, c_id = 1317822057. Also on '2019-07-21' c_id 1317822057\n",
    "8. Adding bug no. 1, Tim's calculation of generated energy is also different from Baran's and Naomi's!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059be14",
   "metadata": {},
   "source": [
    "# BUG 6. INCOMPLETE DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9656f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_id</th>\n",
       "      <th>energy</th>\n",
       "      <th>power</th>\n",
       "      <th>reactive_power</th>\n",
       "      <th>voltage</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:33:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6148</td>\n",
       "      <td>245.3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:36:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6100</td>\n",
       "      <td>243.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:37:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6013</td>\n",
       "      <td>242.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:38:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5996</td>\n",
       "      <td>242.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:39:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5976</td>\n",
       "      <td>242.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:41:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>5990</td>\n",
       "      <td>242.3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:42:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-99</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>5986</td>\n",
       "      <td>242.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:43:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>5975</td>\n",
       "      <td>241.5</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:44:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-99</td>\n",
       "      <td>-1.650000</td>\n",
       "      <td>5965</td>\n",
       "      <td>241.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:46:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6083</td>\n",
       "      <td>244.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:47:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6078</td>\n",
       "      <td>244.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:30:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6143</td>\n",
       "      <td>245.6</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:31:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-103</td>\n",
       "      <td>-1.716667</td>\n",
       "      <td>6141</td>\n",
       "      <td>245.7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:32:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-102</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>6145</td>\n",
       "      <td>245.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:48:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6095</td>\n",
       "      <td>244.4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:49:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6091</td>\n",
       "      <td>243.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:51:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-101</td>\n",
       "      <td>-1.683333</td>\n",
       "      <td>6068</td>\n",
       "      <td>243.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:52:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>6061</td>\n",
       "      <td>243.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:53:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>6028</td>\n",
       "      <td>243.7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:55:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-98</td>\n",
       "      <td>-1.633333</td>\n",
       "      <td>5875</td>\n",
       "      <td>240.2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:56:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-97</td>\n",
       "      <td>-1.616667</td>\n",
       "      <td>5863</td>\n",
       "      <td>239.8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 23:58:55+10:30</th>\n",
       "      <td>1317822057</td>\n",
       "      <td>-98</td>\n",
       "      <td>-1.633333</td>\n",
       "      <td>5853</td>\n",
       "      <td>238.9</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 c_id  energy     power  reactive_power  \\\n",
       "Timestamp                                                                 \n",
       "2020-03-01 23:33:55+10:30  1317822057    -102 -1.700000            6148   \n",
       "2020-03-01 23:36:55+10:30  1317822057    -102 -1.700000            6100   \n",
       "2020-03-01 23:37:55+10:30  1317822057    -101 -1.683333            6013   \n",
       "2020-03-01 23:38:55+10:30  1317822057    -101 -1.683333            5996   \n",
       "2020-03-01 23:39:55+10:30  1317822057    -101 -1.683333            5976   \n",
       "2020-03-01 23:41:55+10:30  1317822057    -100 -1.666667            5990   \n",
       "2020-03-01 23:42:55+10:30  1317822057     -99 -1.650000            5986   \n",
       "2020-03-01 23:43:55+10:30  1317822057    -101 -1.683333            5975   \n",
       "2020-03-01 23:44:55+10:30  1317822057     -99 -1.650000            5965   \n",
       "2020-03-01 23:46:55+10:30  1317822057    -101 -1.683333            6083   \n",
       "2020-03-01 23:47:55+10:30  1317822057    -101 -1.683333            6078   \n",
       "2020-03-01 23:30:55+10:30  1317822057    -102 -1.700000            6143   \n",
       "2020-03-01 23:31:55+10:30  1317822057    -103 -1.716667            6141   \n",
       "2020-03-01 23:32:55+10:30  1317822057    -102 -1.700000            6145   \n",
       "2020-03-01 23:48:55+10:30  1317822057    -101 -1.683333            6095   \n",
       "2020-03-01 23:49:55+10:30  1317822057    -101 -1.683333            6091   \n",
       "2020-03-01 23:51:55+10:30  1317822057    -101 -1.683333            6068   \n",
       "2020-03-01 23:52:55+10:30  1317822057    -100 -1.666667            6061   \n",
       "2020-03-01 23:53:55+10:30  1317822057    -100 -1.666667            6028   \n",
       "2020-03-01 23:55:55+10:30  1317822057     -98 -1.633333            5875   \n",
       "2020-03-01 23:56:55+10:30  1317822057     -97 -1.616667            5863   \n",
       "2020-03-01 23:58:55+10:30  1317822057     -98 -1.633333            5853   \n",
       "\n",
       "                           voltage  duration  \n",
       "Timestamp                                     \n",
       "2020-03-01 23:33:55+10:30    245.3        60  \n",
       "2020-03-01 23:36:55+10:30    243.0        60  \n",
       "2020-03-01 23:37:55+10:30    242.8        60  \n",
       "2020-03-01 23:38:55+10:30    242.0        60  \n",
       "2020-03-01 23:39:55+10:30    242.2        60  \n",
       "2020-03-01 23:41:55+10:30    242.3        60  \n",
       "2020-03-01 23:42:55+10:30    242.2        60  \n",
       "2020-03-01 23:43:55+10:30    241.5        60  \n",
       "2020-03-01 23:44:55+10:30    241.9        60  \n",
       "2020-03-01 23:46:55+10:30    244.2        60  \n",
       "2020-03-01 23:47:55+10:30    244.4        60  \n",
       "2020-03-01 23:30:55+10:30    245.6        60  \n",
       "2020-03-01 23:31:55+10:30    245.7        60  \n",
       "2020-03-01 23:32:55+10:30    245.4        60  \n",
       "2020-03-01 23:48:55+10:30    244.4        60  \n",
       "2020-03-01 23:49:55+10:30    243.8        60  \n",
       "2020-03-01 23:51:55+10:30    243.9        60  \n",
       "2020-03-01 23:52:55+10:30    243.8        60  \n",
       "2020-03-01 23:53:55+10:30    243.7        60  \n",
       "2020-03-01 23:55:55+10:30    240.2        60  \n",
       "2020-03-01 23:56:55+10:30    239.8        60  \n",
       "2020-03-01 23:58:55+10:30    238.9        60  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_date_idx = '202003'\n",
    "file_path = r\"C:\\Users\\samha\\Documents\\CANVAS\\data\"\n",
    "data_path = file_path + r\"\\processed_unsw_\" + data_date_idx + '_data_raw.csv'\n",
    "data = pd.read_csv(data_path, index_col = 1)\n",
    "\n",
    "# Convert timestamp to local Adelaide time\n",
    "data.index = pd.to_datetime(data.index) # convert index from object type to datetime\n",
    "Adelaide_local_time = pytz.timezone('Australia/Adelaide')\n",
    "data.index = data.index.tz_localize(pytz.utc).tz_convert(Adelaide_local_time) # convert utc to local adelaide time\n",
    "data.index.rename('Timestamp', inplace = True)\n",
    "\n",
    "date = '2020-03-01'\n",
    "date_dt = dt.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "data = data[data.index.date == date_dt] #focus only on the date\n",
    "\n",
    "data [ data['c_id'] == 1317822057]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee82a27",
   "metadata": {},
   "source": [
    "# OTHER SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c38716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96c17be8",
   "metadata": {},
   "source": [
    "# MESSY BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad5469",
   "metadata": {},
   "source": [
    "# POLYFIT OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0505f4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "n_buyers = 10\n",
    "n_shares = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98447b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "prices = np.random.random(n_buyers)\n",
    "money_available = np.random.randint(1, 4, n_buyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7803ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701 0.22479665\n",
      " 0.19806286 0.76053071 0.16911084 0.08833981]\n",
      "[1 1 1 3 1 3 3 2 1 1]\n",
      "[ 1.29647768 48.18824404  1.57816269  4.00638948  2.00598984 13.34539487\n",
      " 15.14670609  2.62974258  5.91328161 11.3199242 ]\n"
     ]
    }
   ],
   "source": [
    "n_shares_per_buyer = money_available / prices\n",
    "print(prices, money_available, n_shares_per_buyer, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "155db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = LinearConstraint(np.ones(n_buyers), lb=n_shares, ub=n_shares)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
